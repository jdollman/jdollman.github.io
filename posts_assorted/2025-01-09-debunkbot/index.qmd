---
title: "Debunkbot Analysis"
description: "Replicating Costell, Rand, Pennycook (2024)"
author: "Justin Dollman"
date: 01-09-2025
format:
  html:
    grid:
    margin-width: 350px
    echo: false
    fig-align: "center"
    css: styles.css
    embed-resources: true
    meta:
      robots: "noindex, nofollow"
reference-location: margin
image: debunkbotte.webp
draft: false
---

```{r message=FALSE}
library(scales)
library(tokenizers)
library(tidytext)
library(tinytable)
library(viridis)
library(ggridges)
library(ggrepel)
library(ggtext)
library(tidyverse)

theme_set(
  theme_minimal(base_family = "IBM Plex Sans") +
    theme(
      text = element_text(family = "IBM Plex Sans"),
      strip.text = element_text(face = 'bold', size = rel(1.1)),
      axis.text = element_text(face = "bold", size = 11),
      axis.title = element_text(face = "bold", size = 12)
    )
)

setwd('/Users/jdollman/Dropbox/git/data/2025/debunkbot')

# Ingest the Cleaned Data
chats <- readRDS('chats_provisional.rds')
vars <- readRDS('response_id_bkg_supplementary_vars.rds')
# At one point I had had isConspiracy != 'YES'. WHOOPS.
v <- filter(vars, finished, !do_not_use_data, isConspiracy == 'YES')

chats_v <- 
  left_join(chats, v, by = join_by(ResponseId)) %>% 
  filter(!do_not_use_data)

# This *might* not be correct. 
# For now you're leaving in `isConspiracy != 'YES'`,
# but perhaps it should be the opposite!
conspiracy_tbl <-
  readRDS('CLASSIFICATION-CONSPIRACIES-02/conspiracy_classification_tbl.rds') %>% 
  anti_join(select(filter(vars, do_not_use_data, isConspiracy != 'YES'), ResponseId))

dwe_tbl <- 
  readRDS('DISAGREE-WITH-EXPERTS-CLASSIFICATION/disagree_with_experts_tbl.rds') %>% 
  anti_join(select(filter(vars, do_not_use_data), ResponseId))

vars_mna <- readRDS('data_for_attrition.rds')
```

Last year, researchers Costello, Rand, and Pennycook (CRP, hereafter) [published](https://www.science.org/doi/10.1126/science.adq1814) "Durably reducing conspiracy beliefs through dialogues with AI," where they found that interactions with ChatGPT^[The model they used was `gpt-4-turbo`.] substantially reduced participants' conspiracy belief both immediately post-intervention *and* two weeks post-intervention. Not only did it reduce belief in participants' *focal* conspiracy, defined as whatever conspiracy the participant endorsed at the beginning of the study, it also reduced credence given to other, "non-focal" conspiracies.^[The non-focal conspiracies were the 15 popular conspiracy theories from the Belief in Conspiracy Theories Inventory ([here](https://onlinelibrary.wiley.com/doi/10.1002/acp.1583)). These 15 are the "greatest hits" of conspiracy theories: Area 51, JFK assassination, Princess Diana, (Fake) Moon Landing, etc.] Their paper is a very important methodological advance in the 'corrections' literature which had been hampered by necessarily mostly static materials that could only hamfistedly react to participants' unique profiles and priorities.

For their paper, they relied on a [CloudResearch Connect](https://www.cloudresearch.com/products/connect-for-participants/) sample quota-matched to the US Census along various demographic dimensions to make it demographically representative. But knowing that their study would make a splash, they set up a [website](https://www.debunkbot.com/) to which the inevitable media coverage could direct traffic to and generate additional participants. Sure enough, the website has had over 30,000 visitors since its inauguration in September of last year. This post will be an analysis of the data created by those participants. 

Two notes before beginning. First, I had the pleasure of meeting the study's lead author, Tom Costello, when he came to Stony Brook to participate on a [panel](https://library.stonybrook.edu/library-events/democracy-in-the-digital-age-ais-influence-on-2024-elections/) and he kindly granted me access to the data. Second, though this post is (or will be) self-contained, I would encourage anyone who hasn't read the CRP paper to do so, as it's the "real thing." This post, at least as in its current state, is more 'exploratory vibes' and likely only of interest as inside baseball. If you're pressed for time you might want to skip the first two sections, **The Sample** and **The Survey Experience**, and go straight to [**Results**](#sec-results).^[If the paper as a whole is inside baseball, the first two sections might be 'dugout-ology'.]

# The Sample

As I mentioned, it was a stroke of farsightedness to create the website in advance of news coverage of the Science article.

```{r}
vars %>% 
  mutate(week = floor_date(start_time, 'week')) %>%
  filter(week > '2024-09-01') %>% 
  ggplot(aes(week)) +
  geom_bar() +
  scale_y_continuous(labels = scales::label_comma()) +
  labs(y = 'Weekly Visitor Volume', x = '')
```

Lest you be seduced by those towering five-digit weekly volumes, the vast majority of those who landed on [debunkbot.com](debunkbot.com) just came to check out the site and did **not** complete the interaction with a large language model. If you filter out the people who didn't finish, the plot above turns into the one below:

```{r}
vars %>% 
  mutate(week = floor_date(start_time, 'week'),
         do_not_use_data = ifelse(do_not_use_data, 'Verboten', 'Permission Granted')) %>%
  filter(week > '2024-09-01', finished) %>%
  ggplot(aes(week, fill = do_not_use_data)) +
  geom_bar() +
  scale_y_continuous(labels = scales::label_comma()) +
  labs(y = 'Weekly Survey Completion Volume', x = '') +
  theme(legend.title = element_blank(),
        legend.position = 'inside',
        legend.position.inside = c(0.80, 0.27),
        panel.grid.major.x = element_blank()) +
  scale_fill_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  scale_color_viridis_d(option = "mako", begin = 0.2, end = 0.8)
```

There are two things to note, First, the tenfold reduction of the $y$-axis. Second, among those who did finish, around one third of them chose to not make their data public. The saddest part of that is that those who are 'paranoid' enough to want to prevent researchers from using their data are exactly the people that you'd most want to study. Still, considering the remaining number of participants each week as their own samples, almost every one of them is larger than the average social psychology study published from the 1950s to the 1990s.

Whereas CRP's sample was demographically representative of the US, these survey takers are far from that. They're young(ish), very white, preposterously educated, and very Democratic.

```{r}
white_pct <- 
  v %>%
  filter(str_count(Ethnicity, ',') == 0) %>%
  count(Ethnicity) %>%
  mutate(pct = round(n / sum(n) * 100, 1)) %>% 
  filter(Ethnicity == 'White/Caucasian') %>% 
  pull(pct)

pct_repub <- round(mean(str_detect(v$partisanship, 'Republican'), na.rm = TRUE) * 100, 1)
pct_dem <- round(mean(str_detect(v$partisanship, 'Democratic'), na.rm = TRUE) * 100, 1)
pct_indep <- round(mean(str_detect(v$partisanship, 'Independent'), na.rm = TRUE) * 100, 1)
partisan_count <- 
  v %>%
  filter(!is.na(partisanship)) %>% 
  count(partisanship) %>% 
  mutate(prop = n / sum(n))

partisan_mode_value <- 
  slice_max(partisan_count, order_by = prop, n = 1) %>% 
  pull(partisanship) %>% 
  as.character()

partisan_mode_pct <- round(max(partisan_count$prop) * 100, 1)
```

-   The average survey taker reports being `r round(mean(v$age_int, na.rm = TRUE))` years old.[^1]
-   Respondents are `r white_pct`% white.
-   Whereas the majority of Americans do not have a bachelor's degree, the modal respondent has a bachelor's with an almost equal amount having a master's degree! In fact, more people report having a doctorate than (only) a high school diploma!
-   Given the ability to choose among seven partisanship categories from strong Democrat to strong Republican, the most common response was `r partisan_mode_value` with `r partisan_mode_pct`%! A full `r pct_dem`% of respondents identify as some kind of Democrat ('plain' Democrats as well as leaners). Only `r pct_repub`% report being some kind of Republican. The remaining `r pct_indep`% of those who gave their partisanship said they were 'Strictly Independent'.

[^1]: This is calculated after trimming off implausibly (i.e., *trollishly*) low and high ages.

While these aren't the distributions you'd hope for if you wanted to make inferences about the US population, it is unique to have data on conspiracy theorizing from precisely the population *least* associated with conspiratorial ideation.

# The Survey Experience

An underrated aspect of understanding an experiment's results is having a sense of subjects' phenomenology as they go through it. To that end, I'd recommend that anyone reading this just go to [debunkbot.com](https://www.debunkbot.com/) and try it out for themselves. But if you lack either the ability, time, or desire to go there, you can read my description with restrained editorializing below.

Participants navigate to debunkbot.com, where they are informed that, should they choose, they will participate in a survey where they interact with an AI ("a conspiracy debunking bot") and get personalized feedback.^[The landing page of [debunkbot.com](https://www.debunkbot.com/) frames site visitors' upcoming experience in perhaps a slightly antagonistic way. First, the thing is called *Debunk*Bot, which I'd say implies that you're about to give it some [bunk](https://en.wiktionary.org/wiki/bunk#:~:text=(slang)%20Bunkum%3B%20senseless%20talk%2C%20nonsense.). Also, the landing page encourages you to "Test your beliefs against an AI. Will you change your mind? Try our conspiracy debunking bot and get personalized feedback." which *I* feel has vibes of, "You 'win' if you don't change your mind and lose if you do". I asked my good friend Claude what they thought and got the reply, "its framing is notably antagonistic in several ways." This isn't necessarily bad. It just means that users' responses are going to be even more skewed toward the outgrowths of antagonistic political discourse culture: trolling, obstinancy, and extremity. In the empirical social sciences we often say, "Alright, this is going to be an even tougher test of your hypothesis." Tom Costello remarked that maybe there could be an "EagleChat" skin for the site which would be more amenable to a certain crowd.] Then, across two pages, people describe a conspiracy they "find particularly credible or compelling" and, on the second page, "share more about what led you to find this theory compelling."^[This text is sent to ChatGPT, which confirms (or not) that the visitor has in fact described a conspiracy theory. If ChatGPT determines that the visitor did *not* describe a conspiracy theory, the visitor is offered the possibility of participating in a different arm of the survey, [the 'Disagreement with Experts' arm](#sec-disagreement).] Visitors' responses in these two open-ended text boxes represent one set of data -- a set of natural language descriptions conspiracy theories which spontaneously come to people's minds. 

Before visitors proceed to their conversation with the LLM, they see an LLM-generated short summary of their conspiracy theory and are prompted, "On a scale of 0% to 100%, please indicate your level of confidence that this statement is true." Lastly, visitors respond to the question "How important is this theory to your personal beliefs or understanding of the world?" on a 1-8 scale (where 8 is anchored with "Extremely important to my beliefs and worldview"). At that point comes the core of the experiment, the four-turn interaction with the LLM (more on that in the next paragraph). Afterward, participants rate their post-interaction certainty in the conspiracy using the following scale.

![](DEBUNKBOT-POST-IMAGES/CERTAINTY_POST.png){fig-align="center" width="75%"}

Lastly, participants can fill out an optional demographics page and/or a user feedback page (a sugggestions box, if you will).

Now for the editorializing!

What's the DebunkBot conversation like? From the perspective of a `debunkbot.com` site visitor, the interaction starts with the AI responding to the specific conspiracy theory previously endorsed by the visitor. Behind the scenes, the conversation starts when the LLM first 'sees' the following [system message](https://www.reddit.com/r/ChatGPT/comments/138b1qt/what_is_the_system_message_on_chatopenaicom/):

> "Your goal is to very effectively persuade users to stop believing in the epistemically fraught belief that {{conspiracyTheory}} You will be having a conversation with a person who, on a psychometric survey, endorsed this belief as {{userBeliefLevel}} out of 100 (where 0 is Definitely False, 50 is Uncertain, and 100 is Definitely True). Further, we asked the user to provide an open-ended response about their perspective on this matter, which is piped in as the first user response. Please generate a response that will persuade the user that this belief is not supported, based on their reasoning. Again, your goal is to create a conversation that allows individuals to reflect on, and change, their beliefs (toward a less epistemically unwarranted view of the world). Use simple language that an average person will be able to understand."

After that `system message`, it sees its first `user message`, which is the reasoning the visitor supplied when prompted earlier. The LLM then generates the aforementioned message that, from the visitor's perspective, opens the dialogue with the LLM. After that, the user has three chances to engage with the LLM. If you would like to see an example of how such a conversation plays out, I've put some screenshots of a conversation I had with it in the [appendix](#sec-conspiracy). If you want my description of the messages, the first thing I would say is that they're lengthy *for a conversation*. If you think about this from the perspective of the researchers, they had to navigate a tradeoff between giving short, more conversational messages that contained less information and longer messages that really tried to, you know, *debunk* a conspiracy. Personally, I would have gone for shorter messages to make the interaction seem more conversational and place less of a burden on participants, but a) it's ultimately an empirical question which message length is best and b) shorter messages would mean more rapid pinging of OpenAI's API and so invite glitchiness, costs, and other undesirable things. The tone of the messages is definitely friendly, but a little too faux-friendly.[^2] For example, here are the first sentence(s) of its first three messages to me:

[^2]: In case you think it's a given an LLM would be faux-friendly, I mean that it's generic, customer service friendly. For an example of a language model that seems genuinely friendly, I'd recommend the Sonnet 3.5 version of Claude.

1.  It's great to hear that you're open to exploring different perspectives on this topic. I understand your point about the proximity of the Wuhan Virology Institute to the initial outbreak, and it's natural to question these coincidences.
2.  You bring up important points about trust in large organizations and the potential for bias in the scientific community.
3.  I appreciate your candor and critical view-it's important to have open discussions about the realities of how institutions work, including scientific ones.

There's nothing *wrong* with these messages, but I do feel like I'm being kid-gloved a little. I imagine the people whom misinformation experts most want to reach would find this tone to be patronizing -- though I'm very unsure about that.

I'd also note that I found the language model to be a little evasive when I pressed it for particulars, but this is perhaps a function of its responses being quite long and general.[^3] According to me, the best conversational turn I had with it happened 3/4^ths^ of the way though the conversation and I thought we weren't getting anywhere. I asked, "If you could give one piece of evidence that you think is the most damaging to the lab leak hypothesis, what would it be?" to which it then did give the most convincing piece of information it had so far given. I think that's telling. If my experience is representative, the LLM is probably too reactive. As in, it's not 'agenda-setting' enough -- it has the debate "on the conspiracy theorist's terms." It's probably a familiar experience to those of you who have talked to people who believe what call conspiracies, and it is like playing the world's least fun version of whack-a-mole. Hopefully future research will look into the value of taking more initiative in debunking conspiracy theories/misinformation.

[^3]: Again, check out the conversation -- or have one yourself! -- to get a feel for what I mean.

# Results {#sec-results}

## On Certainty

As indicated by the title of the paper, "Durably *reducing conspiracy beliefs* through dialogues with AI" (emphasis added), the key result was the reduction in conspiracy belief from pre- to post-interaction. Did that also happen in the online sample?

```{r}
# KEEP THIS AS AN EXAMPLE OF WHAT NOT TO DO
# FACETED DENSITY PLOTS ARE REALLY A MUCH BETTER WAY TO GO
# HERE YOU REALLY CANT SEE THE MAGNITUDE OF THE SPIKES AT 0, 50, 100
# vars %>% 
#   filter(!is.na(certitude_post),
#          !is.na(arm)) %>%
#   pivot_longer(cols = c(certitude_pre, certitude_post), names_to = 'pre_post', values_to = 'value') %>% 
#   select(arm, pre_post, value) %>% 
#   mutate(pre_post = str_remove(pre_post, 'certitude_'),
#          pre_post = str_to_title(pre_post),
#          pre_post = paste0('(', pre_post, ')')) %>% 
#   unite(condition, arm, pre_post, sep = ' ') %>% 
#   ggplot(aes(x = condition, y = value)) +
#   geom_jitter(height = 0, width = 0.3, alpha = 0.1) +
#   labs(caption = 'INSERT QUESTION TEXT',
#        y = 'Belief Certainty (0-100)',
#        x = 'Condition')


v_long_summary <- 
  v %>% 
  filter(!is.na(certitude_post),
         !is.na(arm)) %>%
  pivot_longer(cols = c(certitude_pre, certitude_post), names_to = 'pre_post', values_to = 'value') %>% 
  select(arm, pre_post, value) %>% 
  mutate(pre_post = str_remove(pre_post, 'certitude_'),
         pre_post = str_to_title(pre_post),
         pre_post = paste0('(', pre_post, ')')) %>% 
  unite(condition, arm, pre_post, sep = ' ', remove = FALSE)

v_100s <- 
  summarise(v_long_summary, value = mean(value > 50), .by = condition) %>% 
  mutate(value = (value * 100) %>% round %>% paste0(., '%'))

```

```{r}
ggplot(v_long_summary, aes(x = value, y = condition, color = arm, fill = arm)) +
  # geom_density_ridges(alpha = 0.5, jittered_points = TRUE,
  #                    position = position_points_jitter(width = 0, height = 0),
  #                    point_alpha = 0.1) +
  geom_density_ridges(
    stat = "binline",
    bins = 25,
    scale = 0.95,
    alpha = 0.7,
    draw_baseline = FALSE,
    height = 0.95
  )+
  labs(caption = '"On a scale of 0% to 100%, please indicate your\nlevel of confidence that this statement is true."',
       x = 'Belief Certainty (0-100)',
       y = '') +
  annotate(geom = 'label', y = v_100s$condition, x = 75, vjust = -0.2, hjust = 0,
           label = v_100s$value,
           fontface = 'bold',
           size = 3) +
  theme_ridges(font_size = 12, font_family = "IBM Plex Sans") +
  theme(legend.position = "none", 
        axis.text = element_text(face = 'bold'),
        axis.title.x = element_text(face = 'bold')) +
  scale_x_continuous(breaks = c(0, 50, 100), expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  scale_fill_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  scale_color_viridis_d(option = "mako", begin = 0.2, end = 0.8)
  
```

There you see that there was this mass of respondents between 50 and 100 that flattened. To see that more clearly we can go from looking at the pre- and post- certainty distributions and instead visualize the distribution of certainty's _change_:

```{r}
v %>% 
  filter(!is.na(certitude_delta),
         arm %in% c('Conspiracy', 'Experts')) %>%
  mutate(arm = paste(arm, 'Arm')) %>% 
  ggplot(aes(certitude_delta)) +
  geom_histogram(aes(y = after_stat(count/sum(count)), fill = arm), bins = 25) +
  theme(legend.position = 'none') +
  labs(x = 'Change in Certainty\n(Post - Pre)', y = '') +
  scale_y_continuous(labels = percent_format()) +
  facet_wrap(~arm, scales = 'free') +
  scale_fill_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  scale_color_viridis_d(option = "mako", begin = 0.2, end = 0.8)
```

```{r}
# IF JUST FOCUSING ON CONSPIRACIES, USE OTHER FILTER CONDITION
delta_cert <- 
  v %>% 
  filter(!is.na(certitude_delta),
         arm == 'Conspiracy'
         # !is.na(arm)
         ) %>% 
  summarise(increase = mean(certitude_delta > 0),
            nochange = mean(certitude_delta == 0),
            decrease = mean(certitude_delta < 0)
            ) %>% 
  mutate(across(everything(), ~ round(.x * 100), .names = "pct_{.col}"))
```

You can see from the histogram above that individuals' delta scores run the entire gamut of possibility, from -100 (a respondent starts at complete certainty and ends at 0) to +100 (where a respondent initially reports 0% credence and ends up with 100% credence). `r delta_cert$pct_nochange`% have a certainty delta of 0, *i.e.* no change. `r delta_cert$pct_increase`% of respondents report being *more* certain of their conspiracy after the debunking intervention, and thin plurality of respondents, `r delta_cert$pct_decrease`%, report that they are _less_ certain about the conspiracy they described.

Looking at the raw "certainty trajectories" gives an indication of how people use the certainty scale:

```{r}
v %>%
  filter(!is.na(certitude_delta), 
         arm == 'Conspiracy'
         # !is.na(arm))
  ) %>% 
  unite(certitude_traj, certitude_pre, certitude_post, sep = ' --> ') %>% 
  count(certitude_traj, certitude_delta) %>% 
  arrange(-n) %>%
  mutate(percent = round(n / sum(n) * 100),
         percent = paste0(percent, '%')) %>% 
  filter(percent > '1%') %>% 
  select(`Start --> End` = certitude_traj, Delta = certitude_delta, `Pct.` = percent) %>% 
  tt()
  
```

Before analyzing any data, I would not have predicted the most common response being 100% both pre- and post-treatment and I *definitely* would not have predicted it being several times larger than the next most common certainty trajectory.^[This is one of the cases where it would be nice to have some conditional logic in one's survey that detects an individual's 100%-to-100% trajectory and asks them *why*. In our era of online surveys with massively modififiable survey flow, it's criminal how little we prompt users to explain their sometimes bizarre response combinations.] The 0%-to-0% group is likely one that you just want to exclude because they simply do not believe in the conspiracy that the LLM is purportedly debunking.^[For those who understand how credence works (or, is supposed to work), you'll also notice that these people who report 100% (or 0%) certainty in their conspiracy either do *not* understand what 100% (or 0%) certainty means or are trolling.]

There are two explanations for why the 50%-to-50% group is as prominent as it is. The substantive explanation is that when people are uncertain, they default to saying 50% (or "it's a coin toss"). That is, they map any certainty level between 10% and 90% to 50%. The 'methods' (or artifact) explanation for the prominence of the 50%-to-50% group is that the certainty scale is initialized with the dial(?) at 50%. So, if the participant does *nothing* and just clicks through on both occasions, they will appear as having been 50% certain both times. And participants love nothing more than just clicking through.

This pattern of responses – particularly the clustering at 0%, 50%, and 100% – reminded me of my brief prior life when I was really into forecasting (i.e., trying to predict the future), a key subarea of which is probability elicitation. I suspect that no researcher would say that Joe Sixpack is going to faithfully map their subjective probability onto a 0-100 scale. So, looking at the English-language anchors on the scale, I discretized it according to the following mapping:

| Category               | Range  |
|------------------------|--------|
| Def. False             | 0-15   |
| Prob. False            | 15-35  |
| Uncertain (Lean False) | 35-45  |
| Uncertain              | 45-55  |
| Uncertain (Lean True)  | 55-65  |
| Prob. True             | 65-85  |
| Def. True              | 85-100 |

This allows me to visualize the trajectories thus:

```{r}
colors_diverging <- viridisLite::mako(7)
colors_diverging[[4]] <- 'grey90'

library(ggsankey)
unsankey <- function(x){
    labels <- c(
    'Def. False',
    'Prob. False',
    'Uncertain (Lean False)',
    'Uncertain',
    'Uncertain (Lean True)',
    'Prob. True',
    'Def. True'
  )
    
    factor(x, levels = labels, labels = labels, ordered = TRUE)
}

v_sankey <- 
  v %>%
  rename(Pre = certitude_pre_fct2, Post = certitude_post_fct2) %>% 
  make_long(Pre, Post)
  

ggplot(v_sankey, aes(x = x, 
               next_x = next_x, 
               node = node, 
               next_node = next_node,
               fill = factor(node),
               label = node)) +
  geom_sankey(flow.alpha = 0.8) +
  theme_sankey(base_size = 16, base_family = "IBM Plex Sans") +
  theme(legend.position = 'none') +
  scale_fill_manual(values = colors_diverging) +
  scale_color_manual(values = colors_diverging) +
  geom_sankey_label(size = 3.5, fill = "white", family = "IBM Plex Sans") +
  labs(x = '')

```

Or, if you just want to see the distributions side by side:

```{r}

# v %>%
#   filter(!is.na(certitude_delta), 
#          arm == 'Conspiracy'
#          # !is.na(arm))
#   ) %>% 
#   select(certitude_pre_fct, certitude_post_fct) %>% 
#   pivot_longer(everything(), names_to = 'var', values_to = 'val') %>%
#   mutate(val = fct_rev(val)) %>%
#   ggplot(aes(x = var, fill = val)) +
#   geom_bar() +
#   scale_fill_manual(values = colors_diverging) +
#   # scale_color_manual(values = colors_diverging) +
#   scale_x_discrete(limits = c("certitude_pre_fct", "certitude_post_fct"),
#                    labels = c("Pre", "Post")) +
#     # scale_y_continuous(labels = scales::percent_format()) +
#   theme(legend.title = element_blank(),
#         panel.grid = element_blank()) +
#   labs(x = '', y = '')

#| fig-align: "center"
v %>%
  filter(!is.na(certitude_delta), 
         arm == 'Conspiracy') %>% 
  select(certitude_pre_fct, certitude_post_fct) %>% 
  pivot_longer(everything(), names_to = 'var', values_to = 'val') %>%
  ggplot(aes(x = var, fill = val, color = val)) +
  geom_hline(yintercept = .25 * 1:3, linetype = 2) +
  geom_bar(position = position_fill(reverse = TRUE),
           alpha = 0.92) +
  scale_fill_manual(values = colors_diverging) +
  scale_color_manual(values = colors_diverging) +
  scale_x_discrete(
    limits = c("certitude_pre_fct", "certitude_post_fct"),
    labels = c("Pre", "Post"),
    expand = c(0, 0)
  ) +
  scale_y_continuous(labels = scales::percent_format()) +
  theme(legend.title = element_blank(), 
        panel.grid = element_blank()) +
  guides(fill = guide_legend(reverse = TRUE),
         color = guide_legend(reverse = TRUE)) +
  labs(x = '', y = '')
```


## Conspiracy Descriptions (Forthcoming)

In addition to replicating the certainty reduction result of the original paper, the much greater sample size of the online sample allows for a text analysis of people's conspiracy beliefs. For now, I had ChatGPT categorize the user-generated conspiracy text into both narrow and broader categories (species and genus, if you will). As a preview, what are these (mostly) highly educated Democrats conspiratorial about? The top 10 conspiracies mentioned were:

```{r}
#| fig-align: "center"
conspiracy_tbl %>%
  count(species, sort = TRUE) %>% 
  filter(!is.na(species), species != 'NA') %>% 
  slice_head(n = 10) %>% 
  rename(Conspiracy = species, Mentions = n) %>% 
  tt()
```

Quite a few classics. The people who (claim to) believe that "bIrDS AreNT rEaL" also showed up:

```{r}

# conspiracy_tbl %>% 
#   filter(str_detect(text, '(B|b)irds')) %>%
#   mutate(text = str_remove_all(text, ' NA'),
#          text = str_squish(text)) %>% 
#   select(`User Text` = text)

# library(reactable)
# 
# conspiracy_tbl %>% 
#   filter(str_detect(text, '(B|b)irds')) %>%
#   mutate(text = str_remove_all(text, ' NA'),
#          text = str_squish(text)) %>% 
#   select(`User Text` = text) %>%
#   reactable(
#     pagination = TRUE,
#     defaultPageSize = 10,
#     pageSizeOptions = c(10, 25, 50, 100),
#     searchable = TRUE
#   )

library(DT)

conspiracy_tbl %>% 
  filter(str_detect(text, '(B|b)irds')) %>%
  mutate(text = str_remove_all(text, ' NA'),
         text = str_squish(text)) %>% 
  select(`User Text` = text) %>%
  datatable(options = list(
    pageLength = 10,  # number of rows per page
    searching = TRUE, # enable search box
    lengthMenu = c(10, 25, 50, 100), # options for number of rows per page
    scrollX = TRUE    # enable horizontal scrolling if needed
  ))
```

## A Typology of Users and Their Conversations (Forthcoming)

This will be the most interesting part of the post (or perhaps *publication* further down the road). What aspects of conversations are associated with persuasion? Did `gpt-4-1106-preview` (or `gpt-4o-2024-08-06`) deploy different strategies with different users to more or less effect? Beyond persuasion, do different LLM conversational moves lead to more or less engagement? Investigating those interesting dyadic elements will have to come later.

For now, let's look at the most basic measure of conversational quality: message length. The plot below breaks down message length by who is generating the words (LLM/`assistant` or visitor/`user`) and what turn in the conversation it is (where `1` is the each party's opening message, `2` the first reply, etc.):

```{r}
modal_points <- chats %>%
    mutate(
    words_count = count_words(content),
    quantile = rank(words_count),
    quantile = quantile / max(quantile),
    .by = role
  ) %>%
  filter(quantile < 0.99,
         role != 'system',
         turn %in% c(1, 4)) %>%
  mutate(words_count = tokenizers::count_words(content),
         turn = as.character(turn),
         role = str_to_title(role)) %>%
  summarize(
    density_obj = list(density(words_count)),
    modal_words = density_obj[[1]]$x[which.max(density_obj[[1]]$y)],
    density_at_mode = max(density_obj[[1]]$y),
    .by = c(role, turn)
  ) %>%
  mutate(
    label = sprintf("%d words", round(modal_words))
  )
  
chats %>%
  filter(role != 'system',
         turn < 5) %>%
  mutate(words_count = tokenizers::count_words(content),
         turn = as.character(turn),
         role = str_to_title(role)) %>%
  mutate(quantile = rank(words_count),
         quantile = quantile / max(quantile),
         .by = role) %>%
  filter(quantile < 0.99) %>% 
  ggplot(aes(x = words_count, fill = turn, color = turn)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~role, scales = 'free') +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    legend.position = 'inside',
    legend.position.inside = c(0.93, 0.3),
    legend.direction = "vertical") +
  geom_text_repel(data = modal_points,
                aes(x = modal_words, y = density_at_mode, 
                    label = label),
                nudge_x = c(25, -120, 35, -150),
                nudge_y = c(0.001, -0.0001, 0, 0.0001),
                segment.curvature = 0,
                color = 'black',
                family = "IBM Plex Sans") +
    geom_point(data = modal_points,
             aes(x = modal_words, y = density_at_mode),
             shape = 21,
             color = 'white',
             size = 3) +
  labs(x = 'Distribution of Number of Words/Message',
       caption = 'Broken down by role {user, assistant} and turn in the conversation {1, 2, 3, 4}',
       fill = 'Turn') +
    guides(color = "none", fill = guide_legend(override.aes = list(shape = NA))) +
  scale_fill_viridis_d(option = "mako") +
  scale_color_viridis_d(option = "mako")
```

First, the fact that the left and right panes have different x-axis scales makes it less obvious that the 'Assistant' (the LLM) is writing *a lot* more than the user. Whereas the lion's share of assistant messages are > 400 words, the distribution of user message lengths is extremely right-skewed, with most messages fewer than 50 words in length. I've also highlighted the modal number of words used in the first and fourth (last) turns, and you see a big drop-off in engagement. Many of the users' extremely short last messages are either valedictions (*bye*) or expressions of gratitude (*thanks*, *thank you*). As you can see, it's not just the last messages that are extremely short, though. Users' extremely short messages are usually simple affirmations (*yes*, *ok*, *sure*, *i agree*) or unelaborated disagreement (*no*, *i dont believe you*, *im not convinced*).

The savvy consumers of survey research among you will wonder about [mode](https://en.wikipedia.org/wiki/Mode_effect#:~:text=Mode%20effect%20is%20a%20broad,different%20data%20to%20be%20collected.) [effects](https://academic.oup.com/poq/article/81/S1/280/3091905). These mode effects are potentially relevant all over the survey, but nowhere are they as obviously relevant as in users' interaction with the language model. It's simply more costly to type responses when you're working with two thumbs compared to a full set of 10 fingers. The plots below, which disaggregate by mobil vs. desktop use (and re-aggregate over all four turns) were surprising to me:^[Transparency note: I filtered out the 1% longest messages both among the users and assistants to prevent tails from overtaking both plots.]


::: {.panel-tabset}
## Histogram
```{r mobile_v_web_histo}
words_count <- 
  chats_v %>%
  filter(role == 'user',
         turn < 5) %>%
  mutate(words_count = count_words(content),
         turn = as.character(turn),
         role = str_to_title(role),
         Mobile = ifelse(mobile, 'Mobile Device', 'Desktop')) %>%
  mutate(quantile = rank(words_count),
         quantile = quantile / max(quantile),
         .by = role) %>%
  filter(quantile < 0.99)

ggplot(words_count, aes(x = words_count, fill = Mobile, color = Mobile)) +
  geom_histogram(position = position_dodge(width = 3), alpha = 0.8, binwidth = 10) +
  # facet_wrap(~role, scales = 'free') +
  scale_fill_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  scale_color_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  # scale_x_log10()
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    legend.position = 'inside',
    legend.position.inside = c(0.82, 0.18),
    legend.direction = "vertical",
    legend.title = element_blank()
  ) +
  labs(x = 'No. Words Per Message')

```
## Boxplot
```{r mobile_v_web_boxplot}
ggplot(words_count, aes(x = Mobile, y = words_count, fill = Mobile)) +
  geom_boxplot(
    width = 0.5,
    color = viridis(1, option = 'mako', begin = .5),
               outlier.alpha = 0.2) +
  # facet_wrap(~role, scales = 'free') +
  scale_fill_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  scale_color_viridis_d(option = "mako", begin = 0.2, end = 0.8) +
  theme(
    legend.position = 'none'
  ) +
  labs(y = 'No. Words Per Message',
       x = '')
```
:::

```{r}
quantile_df <- function(x, probs = c(0.25, 0.5, 0.75)) {
  tibble(
    val = quantile(x, probs, na.rm = TRUE),
    quant = probs
  )
}

words_iqr <- 
  words_count %>% 
  reframe(quantile_df(words_count), .by = mobile) %>% 
  pivot_wider(names_from = mobile, values_from = val) %>% 
  mutate(DELTA = `FALSE` - `TRUE`)

q25 <- words_iqr[[which(words_iqr$quant == 0.25), 'DELTA']]
q50 <- words_iqr[[which(words_iqr$quant == 0.5), 'DELTA']]
q75 <- words_iqr[[which(words_iqr$quant == 0.75), 'DELTA']]
```

I would have suspected that the distributions would be much more different than they in fact are. With the boxplot, you can eyeball that the 25th, 50th, and 75th percentile desktop responders are only writing about `r paste(q25,q50,q75, sep = ', ')` more words, respectively, than their mobile counterparts.

### A Typology of Users (Forthcoming)

From a theoretical perspective, it's interesting to think about what [moderates](https://en.wikipedia.org/wiki/Moderation_(statistics)) treatment effects. Are Republicans more resistant to corrections? What about strong partisans versus leaners? Which type of conspiracy theories are most 'debunk-resistant'? The list goes on and on. 

From a practical perspective, a researcher would want to classify respondents' "engagement type." Some unengaged respondents do the open-text version of [straight-lining](https://www.qualtrics.com/blog/straightlining-what-is-it-how-can-it-hurt-you-and-how-to-protect-against-it/), while others are more engaged, but "in the wrong way" (*i.e.*, trolling).^[Of course, trolling as a result of treatment *is* itself interesting. For example, if you had two randomly assigned treatments and found that one led to half as much trolling as the other, that's a really important outcome!] Ideally, one would have measured these people's demographics and political background information at a prior time so that you could model engagment quality (including propensity to troll), but the way information is collected here there's simply no way to do that. In future analyses I will attempt to classify people into the buckets of zero effort, pure trolling, pure hostility, and genuine engagement. Problematically, it's only among the latter set that we can really trust their use of the 0-100 belief certainty scale.

# Appendix

## In-Survey Attrition

The initiated among you will know about the [scourge of missing data](https://www.degruyter.com/document/doi/10.7208/9780226891293-008). Missing data can turn nice unbiased estimates into not-so-nice biased estimates. It turns random samples into [SLOP](https://en.wikipedia.org/wiki/Self-selection_bias) and, depending on when nonresponse happens, it turns random assignment into non-random assignment, degrading causal quantities to mere correlations. It's vexatious, *en fin*.

That said, this online survey has no pretention to be a random sample, nor is there random assignment to treatment. This makes it unclear how much of an _additional_ problem missing data is. Nevertheless, here I walk the reader through where visitors attrit in the survey. 

Visitors can be lost in two different ways. The first is that they simply close the survey (whereafter the rest of the values are simply `NA`). The second happens when the visitor makes makes a choice that means their data will not be useful. Let's start with the latter class of cases.

### Attrition via Respondent Choices

```{r}
percent <-
  function(x, meann = TRUE) {
    if (meann) {x <- mean(x, na.rm = TRUE)}
    paste0(round(x * 100), '%')
  }

dnumd_pct <- percent(vars$do_not_use_data)
pct_not_conspiracy <- percent(vars$isConspiracy == 'NO')
pct_continue_anyway <- percent(filter(vars, isConspiracy == 'NO')$RerouteOption == 'Continue') 
```

First, the visitor could click the `(Optional) - Do not use my data` button between assenting to the consent paragraph and cliking the Captcha. `r dnumd_pct` of respondents do this.

![](DEBUNKBOT-POST-IMAGES/DNUMD.png){fig-align="center" width="75%"}

All respondents' conspiracy theory descriptions are sent to ChatGPT to check that what the visitor described is, in fact, a conspiracy. If ChatGPT determines that the participant has *not* described a conspiracy, the participant sees the following page:

![](DEBUNKBOT-POST-IMAGES/CONTINUE.png){fig-align="center" width="75%"}

Those who click `Try other version` are shuttled into the `Disagree with experts` arm, but the majority choose `Continue`. The AI determined that, among those who wrote at least 30 characers describing a purported conspiracy theory, `r pct_not_conspiracy` of them failed to describe a conspiracy theory. Of those `r pct_not_conspiracy`, `r pct_continue_anyway` chose to continue in the conspiracy arm anyway. The data from those in the latter group is not straightforwardly useful in analyses _about conspiracy theories_ since these people have not described a conspiracy. As a reminder, this failure to describe a conspiracy is prior to a failure to *believe* in the conspiracy. Which brings me to an interesting wrinkle.

Below, you see the scale the subjects used to indicate the extent to which they believe in their conspiracy theory. 

![](DEBUNKBOT-POST-IMAGES/CERTAINTY-SCALE.png){fig-align="center" width="75%"}

What should we do if a person rates their conspiracy on the false side of uncertain? On the one hand, it might seem strange to talk about _belief_ in conspiracies among people who don't believe in conspiracies (where lack of belief in $p$ is defined as giving less than 50% probability to $p$). On the other hand, and I believe it's the more sophisticated hand, it makes just as much sense to say that an intervention reduced conspiracy belief from 40% to 20% as it does to talk about an 80% to 40% reduction. In both cases you reduced conspiracy belief by half, and there's nothing magical about the fact that the 80-to-40 reduction crosses 50. So, in all the analyses here, I allow for initial belief to be less than 50%.^[Much as the distinction in hypothesis testing between 'significant' and 'insignificant' is [not itself significant](http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf), the distinction between >50% credence and <50% credence is likewise not significant. Imagine you're working at a nuclear reactor where most people believe the probability it will "go Chernobyl" in the next month is 0.0000001%, but there are two weirdos who believe that probability is 45% and 55%, respectively. It would be extremely bizarre to group the 45% fella with the 0.0000001% group, but that's what treating 50% as a magical credence line does. In fact, *not* reifying 50% as a magical belief line seems especially relevant when talking about conspiracy belief. For example, imagine a person who gives "only" a 40% probability to the proposition that the earth is flat. Though they might not "believe" the earth is flat, they are wildly miscalibrated. If this all seems like pedantic necrotic horse over-flagellation, the Forecasting Research Institute released a [study](https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/66ba37a144f1d6095de467df/1723479995772/AIConditionalTrees.pdf) last year where they pitted a group **concerned** about AI safety against a group of  [superforecasters](https://en.wikipedia.org/wiki/Superforecaster) **unconcerned** about AI as an [X-risk](https://www.lesswrong.com/tag/existential-risk). If we let $\textbf A$ mean **AI causes existential catastrophe by year 2100**, the **concerned** group came into the study with $\Pr(\textbf A) = 28.4\%$, while the **unconcerned** group came in at $\Pr(\textbf A) =0.5\%$. If they had to binarized belief at 50%, the entire debate would have evaporated since no one would have 'believed' in the existential risk posed by AI. It would also result in strange descriptions of reality, such as "There is a group of extremely concerned people dedicating their lives to reducing AI risk, even though they don't believe in it."]

### Attrition via Exit

Now, what about those who simply click out of the study? In the interactive plot below, you can hover your mouse over each point to see how many subjects were present at each point.^[This plot is currently malfunctioning. The points are in the correct places and the descriptions that appear when hovered over are correct, but the % still in study and % reduction information is not.]

```{r message=FALSE}
vars_mna <- vars_mna %>%
  mutate(
    Description = case_when(
      variable == "IPAddress" ~ "Subjects land on the page!",
      variable == "Consent" ~ "Many subjects don't get past the consent page.",
      variable == "ConspiracyQuestion1" ~ "Some subjects fail to respond to prompt to describe a conspiracy theory.",
      variable == "baseSystemMessage" ~ "A user clicks out while waiting for the LLM to check the conspiracy plausibility.",
      variable == "Sureness_1" ~ "These participants drop out instead of giving their degree belief in the conspiracy.",
      variable == "Post_1" ~ "These ones do not make it to the post-interaction certainty rating, likely dropped during the conversation.",
      variable == "Q39" ~ "This is the final page of the experiment. Subjects may give final feedback.",
      variable == "Ethnicity_6_TEXT" ~ "These subjects filled in the optional demographic survey.",
      TRUE ~ NA_character_
    )
  )

p <- ggplot(vars_mna, aes(
  x = reorder(variable, -prop_present),
  y = prop_present,
  # Include all the info we want in the tooltip
  text = paste0(
    "Description: ", Description, "<br>",
    "% Still in Study: ", percent(prop_present), "<br>",
    "Reduction from Previous Step: ", ifelse(reduction == 1, "NA", percent(reduction))
  )
)) +
  geom_line(aes(group = 1)) +
  geom_point(aes(fill = prop_present), shape = 21, size = 2.5) +
  scale_y_log10(labels = percent_format()) +
  labs(x = "",
       y = "Subjects in Study") +
  theme(legend.position = "none",
        axis.text.x = element_blank()) +
  scale_fill_viridis(option = "mako", begin = 0.4, end = 1)

library(plotly)
ggplotly(p, tooltip = "text")
```

## Disagreement with Experts Description and Descriptives

As mentioned above, not everyone who completes the survey completes the conspiracy reduction arm. After the visitor fills in the two text boxes available to describe a conspiracy and the evidence for it, those responses are sent to a language model to determine if the visitor has in fact described a conspiracy. If it determines that the user has not described a conspiracy, the user will see the following screen:


![](DEBUNKBOT-POST-IMAGES/TRY-ANOTHER-VERSION.png){fig-align="center" width="75%"}

If they fill in the `Try other version` radio button (as pictured), they will flow into the `Disagreement with experts` stream. This results in a very self-selected group.

See [the screenshots below](#sec-disagreement) if you'd like, but basically visitors are asked to describe a belief that most people (possibly including experts) would disagree with and to give the reasons why they find it compelling. If you want to see what an entire conversation looks like, [those screenshots](#sec-keto-konvo) are below as well.

The counts below are very provisional, but I had ChatGPT code what groups were the implied groups the visitor disagreed with, and the 8 most common were:

```{r}
#| fig-align: "center"
dwe_tbl %>%
  filter(group != 'NA') %>% 
  count(group, sort = TRUE) %>% 
  slice_head(n = 8) %>% 
  rename(Group = group, `No. Disagreements` = n) %>% 
  tt()
```

I also had ChatGPT code both what the implied consensus was and what alternative take the subject had on. Here are just a few of the more entertaining (users' text followed by three columns of ChatGPT's coding):

```{r}
#| fig-align: "center"
dwe_tbl %>% 
  filter(ResponseId %in% c('R_3sZwWgIXW41lev9', 'R_273y4DxRCBXL4w1', 'R_8V902mo5UJIFXoA', 'R_8hKGcPyPvk8EUrg', 'R_7f7qetobzr8lpfD')) %>% 
  select(text = Text, group, group_view, user_view) %>% 
  mutate(text = as.character(text),
         text = str_remove(text, '\\d+\\. ')) %>% 
    datatable(options = list(
    pageLength = 10,  # number of rows per page
    scrollX = TRUE    # enable horizontal scrolling if needed
  ))
```


## Disagreement with Experts Screens {#sec-disagreement}

For completeness, here's what the visitor sees once they are in the Disagreement with Experts arm.

![](DEBUNKBOT-POST-IMAGES/DISAGREE-WITH-EXPERTS-1.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/DISAGREE-WITH-EXPERTS-2.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/DISAGREE-WITH-EXPERTS-3.png){fig-align="center" width="90%"}

## Sample Conspiracy Conversation: Lab Leak {#sec-conspiracy}

For transparency, I've posted the conversation as screenshots.^["Receipts," as the kids say] If the text is too wee for ye eyes, you can right-click any given image and then normal-click `Open Image in New Tab` on the menu that magically appears.

![](DEBUNKBOT-POST-IMAGES/WUHAN-CONVO-01.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/WUHAN-CONVO-02.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/WUHAN-CONVO-03.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/WUHAN-CONVO-04.png){fig-align="center" width="90%"}

## Sample Experts Conversation: Keto Diet {#sec-keto-konvo}

Comments below.

![](DEBUNKBOT-POST-IMAGES/KETO-CONVO-01.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/KETO-CONVO-02.png){fig-align="center" width="90%"}
![](DEBUNKBOT-POST-IMAGES/KETO-CONVO-03.png){fig-align="center" width="90%"}
![The formatting got funky here.](DEBUNKBOT-POST-IMAGES/KETO-CONVO-04.png){fig-align="center" width="90%"}

In the conversation above, I stumbled into a conversational 'failure mode.' My disagreement with experts was, "I think that I think that the health benefits of ketogenic diets are more than just via reduction in calories consumed. Sugars, particularly simple sugars, might be uniquely bad for health (where weight is just one facet of overall health)."^[And my elaboration was, "I've known people who have lost a lot of weight on the keto diet, more so than any other diet. Also, I have a very vague but positive impression of Gary Taubes, a researcher associated with these ideas."] Now, I only gave this proposition a 55% chance of being true.^[In retrospect, 55% is pretty high. Were I to do it again I think I'd put it at 40%. I think I had some sort of glitch in my 'credence to number' mapping where I started (anchored) on 50% and then inched my probability upward to indicate that I thought the view has some merit. But a less dumb approach would have been to anchor on $\Pr(\text{non-obvious health claim is true)}$ and then increment *that* upward to the extent the keto claim is more likely than a random non-obvious health claim.] And though I gave the LLM a claim that is at least plausible and I expressed almost maximal uncertainty about it, the LLM seemed forced to argue against my claim as if I had given it a ludricrous claim and said I was certain (e.g. smoking doesn't actually cause lung cancer, 95%!). That is, neither the direction nor the degree to which user is miscalibrated is taken into account. It would make the behind-the-scenes design a bit more complicated, but ideally you would let the model change its argumentative tack based on the (lack of) discrepancy between the plausibility of the visitor's view (conspiracy, disagreement with experts) and the probability the user assigns to that view. In that case, instead of "DebunkBot" it would be more "CalibrationBot" that reasons with the user in such a way to get to that probability.^[The survey actually already calculates the plausibility of the claim, but as far as I can make out that information is only used to subset respondents' change scores to generate personalized feedback like, "When compared to others who hold beliefs in *conspiracies with similar plausibility* and personal importance levels, your change in belief ranks at the 61st percentile. This means your level of belief change was greater than that of approximately 61% of similar respondents."]