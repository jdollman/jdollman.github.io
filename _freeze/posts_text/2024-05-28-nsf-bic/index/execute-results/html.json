{
  "hash": "fb339a4d58df7ab0b9938f1ea9e02eb3",
  "result": {
    "markdown": "---\ntitle: \"Using a Certain Generative Pre-trained Transformer to Code NSF Grants\"\n# description: \"\"\nauthor: Justin Dollman\ndate: 05-28-2024\ncategories: [ChatGPT, coding, NSF]\nimage: grant_writing.webp\ndraft: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringr)\n\n# Sample Data\ndf <- data.frame(\n  id = 1:3,\n  text = c(\"This is line1. And here is line2!\", \n           \"Some. Text. To. Split.\",\n           \"No splits here.\")\n)\n\n# Regex Pattern (customize this!)\npattern <- \"[.!?]\"  # Split on periods, exclamation points, or question marks\n\n# Splitting & Unnesting\ndf_split <- df %>%\n  mutate(text = str_split(text, pattern)) %>%  # Split into lists\n  unnest(text) %>%  # Unnest the lists into rows\n  mutate(text = trimws(text))   # Remove extra whitespace\n\n# (Optional) Filter out empty rows if needed\n# df_split <- df_split %>%\n#   filter(text != \"\") \n```\n:::\n\n\nThe original version of this post was going to demonstrate a fully scaled coding endeavor of tens of thousands of documents. Unfortunately for the world (mostly me, though) OpenAI's batch processing is still working out some glitches that prevent (link to forthcoming document)\n\nOutline for forthcoming document (which you should post!) -- Basically, the plan is as follows \n\n1. Carve up each NSF grant abstract into smaller parts (✅, easy regex solution)\n  * Should increase performance of downstream coding (FN. Though this is an empirical question. Part of the reason I'm going on this circuitous route is to see if it's true!)\n  * Saves tokens. Immediately it costs more time and tokens, but \n  * \n2. Send a sample of those to ChatGPT to code for the *presence* of broader impacts (❌), this is where it would be nice if I could batch\n3. Using the now labelled data, build a classifier (✅, using a subsample...)\n4. Generate predicted probabilities for which segment of the NSF abstract contains a broader impact.\n  4a. The goal is that one of its n segments has a much higher probability than the rest\n  4b. \n5. Send each grant's (probably) BI-containing segment to be coded\n\nSo, [separating my concerns](), this post will concern only the prompt-engineering considerations for getting ChatGPT to accurately code unstructured text along two somewhat subtle dimensions. The next post will implement \n\nWhat are Broader Impacts?\nhttps://new.nsf.gov/funding/learn/broader-impacts\n\n'Coding' unstructured text into nice quantitative features is a time-consuming, mentally-draining, and eventually hellish task. Recently, tried this for a task that human^[It's wild that 'human' is going to become used more and more as a prepositive adjective in front of nouns we .... C'est le siècle le plus important] research assistants had done a poor job at. Sure enough, both ChatGPT (4) and Claude (Opus?) did pretty bad jobs. Google's Gemini didn't even try.\n\nA while back I was part of a project in which the principal investigator was looking at the broader impacts of NSF grants. A prior research assistant had manually coded 400 **project outcome reports**^[] along two dimensions: are the project's beneficiaries `{advantaged, universal, disadvantaged}` and is the relationship between the core activity of the grant and the beneficiaries `{inherent, direct, or indirect}`. I refer you [the resulting publication]() for elaboration on the whys and wherefores of this coding. By the time I was brought onto the project, the task was to figure out if certain types of broader impact strategies were associated with more or fewer publications. Given the complexities of the models I wanted to run (zero-inflated this-and-that, controlling for various institutional factors), the N = 400 sample was pretty meager. \n\nMy first foray into working with the API.\n\n* How similar are API responses to 'manual' responses?\n\n* Structure of a POR\n\nDownload grants here: https://www.nsf.gov/awardsearch/download.jsp\n\nKeyword extraction\nUseful for (at least) three reasons\nLet the PI check the behavior of the classifier quickly\nGive the model 'time to think'\nHave information to do more in-depth analyses later\n\nkey values to get out\ndescription of broader impact (one or two English sentences)\nbeneficiaries\nrelationship between project and beneficiaries (training, etc.)\nimmediacy_code\ninclusion_code\n\nWhat about first using it to build a classifier\n\n# Appendices\n\n## A1: Broader Impact Classifier\n\n## A2: Results of Text-Length Experiment\n\nRQ: How much does irrelevant text degrade the (coding/keyword) capabilities of of ChatGPT?\n\nIf you could separate the abstract or project out report 'at the joints, you could send each part to ChatGPT to label it as involving a broader impact or not. Then, with that labelled data you could build a classifier. In a second go through, you'd only send the part about the broader impact to ChatGPT for the coding. This would ultimately save you a lot of tokens (because you only send a relatively small sample to ChatGPT) and likely increase performance (recency/first-thing bias is even worse for LLMs than for humans)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}