{
  "hash": "8a82d50fb71e53a7c0b064a6346a5ab5",
  "result": {
    "markdown": "---\ntitle: \"Prompt Experiment\"\nauthor: \"JD\"\ndate: \"2024-05-23\"\noutput: pdf_document\n---\n\n\n\n\nThis markdown file creates the prompt parts, permutes them, packages the entire prompt together with an abstract, and batches them off. It's the entire thing. It even includes the function to read in OpenAI's returned output!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glue)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n```\n:::\n\n\n# Prompt Permuter\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Opening sentences\nopen_a <- \"You are a sharp, conscientious, and unflagging researcher who skilled at evaluating scientific grants across all areas of study\"\nopen_b <- \"You are an intelligent, meticulous, and tireless researcher with expertise in reviewing scientific grants across various fields\"\nopen_c <- \"You are a bright, punctilious, and indefatigable researcher who is an expert at reading scientific grants across all disciplines\"\n\n# Definitions/descriptions of inclusion categories\nincl_uni <- \"A broader impact is classified as `universal` if anyone could at least in theory benefit from it. Public goods such as improving primary school teaching practices or developing a better municipal water filter are examples of universal impacts.\"\nincl_adv <- \"A broader impact is classified as `advantaged` if the primary benefit will be experienced by advantaged groups and/or maintain status hierarchies. Scientists, as well as wealthy people and institutions count as advantaged.\"\nincl_inc <- \"Along the inclusion dimension, a broader impact is `inclusive` if its main beneficiaries are marginalized or underrepresented people. Common examples of inclusive broader impacts are programs that help women, people of color, and people with disabilities advance in STEM fields.\"\n\n# Definitions/descriptions of **immediacy** categories\nimmed_int <- \"A broader impact is `intrinsic` if the broader impact is inherent to and inseparable from the main principal purpose of the grant. For example, if a project is developing carbon capture and sequestration technology, the research and societal benefits of reducing greenhouse gases overlap and thus the broader impact is intrinsic to the project.\"\nimmed_dir <- \"Along the immediacy dimension, a broader impact is classified as `direct` if its impact flows directly from the research but is not the specific goal of the research. Training graduate students is a quintessential direct broader impact. For most research grants, training a graduate student is not the purpose of the research. Rather, researchers train graduate students in order to complete a research project. The training is directly related to the research, but it is not the point or purpose of the research.\"\nimmed_ext <- \"Some broader impacts are `extrinsic`. These broader impacts are separate from the main intellectual merit of the research project, and often the project is only tenuously related to it. For example, if a cell biologist studying proteins creates a presentation for a local high school about STEM careers, the broader impact is extrinsic. The outreach to high school students is a separate endeavor that takes place outside, or is extrinsic to, the research.\"\n```\n:::\n\n\nThe following function generates three prompts, the first with opening A, second with opening B, third with opening C. Underneath that deterministic design, the orders or inclusion and immediacy are randomly permuted. One hacky feature here is using a `while` loop to generate rows one-by-one, making sure that no two prompts are identical -- this ensures each abstract receives labels from three distinct prompts. If I hadn't done this, there would be decent probability that any given project outcome report would have at least two identical prompt permutations beneath the unique opening.\n\nI've also hard-coded generation of *three* prompts. This is simply because there are three opening sentences.\n\nOutput is a `tibble`, because why not.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_three_prompts <- function() {\n  \n  prompts_tbl <- tibble(condition = character(), prompt = character())\n  not_enough_rows <- TRUE\n  \n  while (not_enough_rows){\n  \n  incl_randomization <- sample(c('incl_uni', 'incl_adv', 'incl_inc'))\n  incl_condition <- paste(str_remove(incl_randomization, 'incl_'), collapse = '_')\n  \n  immed_randomization <- sample(c('immed_int', 'immed_dir', 'immed_ext'))\n  immed_condition <- paste(str_remove(immed_randomization, 'immed_'), collapse = '_')\n  \n  condition <- paste(incl_condition, immed_condition, sep = '-')\n  \n  if(condition %in% prompts_tbl$condition) next\n  \n  incl_prompt <-\n    paste(sapply(incl_randomization, get), collapse = ' ')\n  immed_prompt <-\n    paste(sapply(immed_randomization, get), collapse = ' ')\n  \n  prompt <- paste0(\n    '. You will be presented with an NSF grant project outcome report. NSF grant project outcome reports contain information on both the intellectual merit of a project (how the project will advance science) and its broader impact (how the project will benefit society). Your task is to find and describe the broader impact, then code it along two dimensions: inclusion and immediacy. Pay close attention to the meanings of words in backticks below. The first dimension, inclusion, refers to who will primarily receive the benefits of the broader impact. ',\n    incl_prompt,\n    ' The second dimension, immediacy, refers to the centrality of the broader impact to the main part of research project. ',\n    immed_prompt,\n    ' Please code the following NSF grant along the two dimensions of inclusion and immediacy. Take a deep breath, reason step by step, and respond in machine-readable json output. Use the following format, writing your responses where the triple backticked json values are (but you should use double quotes since that is correct json formatting): [{\"broader_impact_description\": ```Write a one or two sentence description of broader impact staying as close as possible to the text.```, \"principal_beneficiaries\": ```Who are the primary beneficiaries of the broader impact. Are they mentioned explicitly or are you making an inference?```, \"reasoning\": ```In a sentence, relate the broader impact to the coding rubric```, \"inclusion_code\": ```Choose from {universal, advantaged, inclusive}```, \"immediacy_code\": ```Choose from {intrinsic, direct, extrinsic}```}] Note: if a grant has more than one explicitly mentioned broader impact, reason about each one separately and give each its own json response, separating them with a comma. Do not use any formatting such as newlines, backticks, or escaped characters.'\n  )\n  \n  prompts_tbl <- bind_rows(prompts_tbl, tibble(condition = condition, prompt = prompt))\n  if (nrow(prompts_tbl) == 3) {\n    prompts_tbl$prompt <- paste0(c(open_a, open_b, open_c), prompts_tbl$prompt)\n    return(prompts_tbl)\n  }\n  }\n}\n```\n:::\n\n\n# Experiment Data Frame\n\nNow I create 1,200 prompts, three for each project outcome report.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nprompts <- \n  replicate(400, generate_three_prompts(), simplify = FALSE) %>% \n  purrr::list_rbind()\n```\n:::\n\n\nNow I read in the 400 project outcome reports (first line), then repeat each one three times, then bind them together with the prompts and create a unique id for each prompt-POR combination.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npors <- readRDS('por_tbl.rds')\n\nexperiment_tbl <- \n  slice(pors, rep(1:n(), each = 3)) %>% \n  bind_cols(prompts) %>% \n  mutate(batch_id = paste(award_id, condition, sep = ';'),\n         .before = 1)\n\nexperiment_tbl$batch_id <- paste0(experiment_tbl$batch_id, '-', rep(letters[1:3], 400))\n```\n:::\n\n\n# Batching and Dispatching\n\nThis code gets everything read to send to OpenAI. The actual part where I interact with the API should eventually include also programmatically downloading the batches. When I wrote this for the post I had few enough that I could manually download them all in about a minute and I was in a time crunch, so I just did that.\n\nThe function immediately below formats a request to be send to OpenAI's `chat-completions` API. It defaults to using a temperature of 0.2 and returning just one system response. As of writing, the default temperature if the client doesn't specify otherwise is 1. The default number of response choices is 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nendpoint_url <- \"https://api.openai.com/v1/chat/completions\"\n\n## YOUR API KEY HERE\n## OR, DO IT PROGRAMMATICALLY IF YOUR PROFESSIONAL\n## api_key <- \n\ncreate_request <- function(custom_id, system_message, user_message, \n                           temp = 0.2, n_choices = 1) {\n  list(\n    custom_id = custom_id,\n    method = \"POST\",\n    url = \"/v1/chat/completions\",\n    body = list(\n      model = \"gpt-4o\",\n      temperature = temp,\n      n = n_choices,\n      messages = list(\n        list(role = \"system\", content = system_message),\n        list(role = \"user\", content = user_message)\n      )\n    )\n  )\n}\n```\n:::\n\n\nThe function below takes in three arguments that any batching function would have to take: ids, system messages (instructions, context), and user messages (i.e., prompts, here, project outcome reports). Hopefully this is fixed by the time you're reading this, but at the time of doing this experiment, the Batch API allowed for uses to have very few \"enqueued\" messages. This is why the function creates subbatches. You specify the first and the last indices to give the beginning and end of the subbatch. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsend_subbatch <- function(batch_ids, system_messages, user_messages, \n                          start_idx, end_idx, temp = 0.2, n_choices = 1) {\n  \n  requests <- vector(mode = 'list', length = end_idx - start_idx + 1)\n  \n  j <- 1L\n  for (i in start_idx:end_idx){\n    requests[[j]] <- create_request(batch_ids[[i]], system_messages[[i]], \n                                    user_messages[[i]], temp = temp, \n                                    n_choices = n_choices)\n    j <- j + 1L\n  }\n  \n  jsonl_fn <- glue(\"requests_{start_idx}_{end_idx}.jsonl\")\n  fileConn <- file(jsonl_fn, open = \"w\")\n  for (request in requests) {\n    writeLines(jsonlite::toJSON(request, auto_unbox = TRUE), fileConn)\n  }\n  \n  close(fileConn)\n  \n  files_response <- POST(\n    'https://api.openai.com/v1/files',\n    add_headers(Authorization = paste(\"Bearer\", api_key)),\n    body = list(\n      purpose = \"batch\",\n      file = upload_file(jsonl_fn)\n    ),\n    encode = \"multipart\"\n  )\n  # Define the JSON body\n  body <- list(\n    input_file_id = content(files_response)$id,\n    endpoint = \"/v1/chat/completions\",\n    completion_window = \"24h\"\n  )\n  \n  # Make the POST request\n  batch_response <- POST(\n    \"https://api.openai.com/v1/batches\",\n    add_headers(\n      Authorization = paste(\"Bearer\", api_key),\n      `Content-Type` = \"application/json\"\n    ),\n    body = body,\n    encode = \"json\"\n  )\n  \n  batch_response\n}\n\n## And a function that takes in the batch id and gets its status so that\n## you can move on to the next subbatch when it's done\n\nbatch_status_parsed <- function(batch_response_id) {\n  response_status <- GET(\n    paste0(\"https://api.openai.com/v1/batches/\",\n           batch_response_id), \n    add_headers(\n      Authorization = paste(\"Bearer\", api_key),\n      `Content-Type` = \"application/json\"\n    )\n  )\n  \n  content(response_status, as = 'parsed', encoding = 'UTF-8')\n}\n```\n:::\n\n\nBelow are two `while` loops that send a subbatch, check on them, and move on to the next subbatch\n\nOn thing this code could do is keep a log of the number of successes you've had at a given number of samples. So, roughly, if you've had success with enough of them but need to drop for a particular batch, you can go back up as soon as the unusually large one is done. This is definitely not robust code. It worked well-enough for me, though.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If you've already done some, you'll set this to something other than 1\nstarting_idx <- 1\n# The acceptable increment (i.e., how many requests you're sending) depends\n# on how many tokens you're sending. You'll have to play around with it\nincrement <- 46\n# Don't touch this!\nlast_run <- FALSE\ntemp <- 0.2\nn_choices <- 2\n## How many samples are there?\nn_samples <- 1200\n\nwhile (TRUE) {\n  \n  if (starting_idx >= n_samples - increment){\n    increment <- n_samples - starting_idx\n    last_run <- TRUE\n  }\n  \n  batch_response <-\n    send_subbatch(\n      batch_ids = experiment_tbl$batch_id,\n      system_messages = experiment_tbl$prompt,\n      user_messages = experiment_tbl$por,\n      start_idx = starting_idx,\n      end_idx = starting_idx + increment,\n      temp = temp,\n      n_choices = n_choices\n    )\n  \n  if (last_run == TRUE) break\n  \n  Sys.sleep(30)\n  \n  status <- batch_status_parsed(content(batch_response)$id)$status\n  \n  while (status != 'completed') {\n    Sys.sleep(30)\n    \n    status <- batch_status_parsed(content(batch_response)$id)$status\n    \n    if (status == 'failed') {\n      print('Oops!')\n      break\n    }\n  } \n  \n  # there are other reasons a batch might fail,\n  # but, since\n  # A. i've already got status and\n  # B. so far in my case it's always been \"Enqueued token limit reached\"\n  # ((which means the batch was too large))\n  if (status == 'failed') increment <- increment - 5; Sys.sleep(30)\n  \n  if (status == 'completed'){\n    starting_idx <- starting_idx + increment + 1\n    print('We had a success!')\n  }\n  \n}\n```\n:::\n\n\nI did once get an `Error in while (status != \"completed\") { : argument is of length zero` because `NULL` had been returned by `batch_status_parsed`. Weird stuff was going on at that point even on their website.\n\n# Reading in Processed Batches\n\nThis was hell.\n\nThe code below, which should probably eventually be moved to a different document, reads in the `jsonl` files returned by `OpenAI`'s `chat-completions` API and returns a dataframe.\n\nThe first step is to convert the `.jsonl` files with their unlovable malformatting into nice `.txt` files\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd('temp_02_n_2_openai_responses')\njsonls_fn <- list.files(pattern = 'jsonl$')\n  \nunfuck_text <- function(fucked_text) {\n  fucked_text %>%\n    str_remove_all('\\\\\\\\n') %>%\n    str_replace_all('\\\\\\\\\"', '\"') %>%\n    str_remove_all('```') %>%\n    str_remove_all('\\\\s{2,}') %>%\n    str_remove_all('json[l]*')\n}\n\nfor (jsonl_fn in jsonls_fn){\n  t <- readLines(jsonl_fn)\n  writeLines(\n    unfuck_text(t),\n    con = str_replace(jsonl_fn, 'jsonl$', 'txt'))\n}\n```\n:::\n\n\nNow with the 'nice' `.txt` files we can relatively easily excise the `json` output returned by `chat-completions`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#\n# helper functions\n#\n\njson2df <- function(match_matrix_entry){\n  data.frame(t(unlist(fromJSON(match_matrix_entry))))\n}\n\ntodf <- function(choice_text) {\n  n_impacts <- str_count(choice_text, 'broader_impact_description')\n  # the control flow isn't actually necessary, \n  # but i suspect it makes things faster\n  # should what's if and what's else be flipped, though?\n  if (n_impacts == 1) {\n    json2df(choice_text) %>% mutate(n_bis = n_impacts)\n  } else {\n    choice_text <- str_replace_all(choice_text, ',\\\\s*\\\\{\\\"broader_', '&&&{\\\"broader_')\n    choice_text <- str_split_1(choice_text, '&&&')\n    # this is an experiment trailing thing\n    map(choice_text, json2df) %>% list_rbind() %>% mutate(n_bis = n_impacts)\n  }\n}\n\n#\n# the big function!\n#\n\ntxtfn2df <- function(fn) {\n  json_readlines <- readLines(fn)\n  json_readlines <- str_replace_all(json_readlines, '\\\\[([A-Z]+)\\\\]', '(\\\\1)')\n  for (i in seq_along(json_readlines)) {\n    # you'll want to get the condition\n    \n    condition <- str_match(json_readlines[[i]], '\\\\\"custom_id\\\\\": \\\\\"(\\\\d+;[-a-z_]+)')[1, 2]\n    \n    match_matrix <-\n      str_match_all(json_readlines[[i]], '\\\"content\\\": \\\"\\\\[(.*?)\\\\]') %>% .[[1]]\n    \n    # verify that this is always 2\n    # correct, this has been verified\n    # for future you, it's 2 because you set the n parameter (number of choices) to 2 in the API call\n    # two_coders_question_mark[[i]] <- length(matches) -- that's how you checked\n    matches <- match_matrix[, 2]\n    \n    if (i > 1) {\n      \n      coded_tbl <-\n        bind_rows(\n          coded_tbl,\n          map2(matches, c(1, 2), ~ todf(.x) %>% mutate(ra = .y)) %>%\n            list_rbind() %>%\n            mutate(condition = condition)\n        )\n        \n    }\n    else {\n      coded_tbl <-\n        map2(matches, c(1, 2), ~ todf(.x) %>% mutate(ra = .y)) %>% \n        list_rbind() %>% \n        mutate(condition = condition)\n    }\n  }\n  \n  coded_tbl\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd('temp_02_n_2_openai_responses')\ntxts_fn <- list.files(pattern = 'txt$')\nsecond_coding_tbl <- map(txts_fn, txtfn2df) %>% list_rbind()\n\nsecond_coding_tbl <- \n  select(second_coding_tbl, condition, ra, n_bis, inclusion_code, immediacy_code, \n         bi_desc = broader_impact_description, principal_beneficiaries, reasoning) %>% \n  arrange(condition)\n\n# write_csv(second_coding_tbl, 'llm_coded400.csv')\n```\n:::\n\n\n# Appendix\n\n## Other Batching Code\n\nI didn't end up using for anything in the post, but I had developed this code semi-independently on a different computer to send subbatches. Next time I work with the `chat-completions` API I'll combine the subbatching and dispatcing above with what's below (assuming both attempts have unique merits).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrant_sample14_16 <- readRDS('grant_sample14_16.rds')\ngrant_sample14_16_abs <- grant_sample14_16[['grant_sample14_16_abs']]\ngrant_sample14_16_fn <-  grant_sample14_16[['grant_sample14_16_fn']]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nendpoint_url <- \"https://api.openai.com/v1/chat/completions\"\n\ninstructions <- \"You will be given a fragment of an abstract of an NSF grant application. Your job is to classify the fragment as either containing the project's broader impact (a statement the project's societal benefits) or is completely about the project's intellectual merit. Broader impacts include inclusion, STEM education and workforce, public engagement, societal well-being, national security, partnerships, economic competitiveness, infrastructure. Respond with a two-item list object. The first item is a dummy variable, 1 to indicate the presence of broader impact, 0 otherwise. The second list item is a short summary sentence describing broader impact if it is present. If the dummy variable is 0, simply return [0, NA].\"\n\ncreate_request <- function(custom_id, abstract_fragment) {\n  list(\n    custom_id = custom_id,\n    method = \"POST\",\n    url = \"/v1/chat/completions\",\n    body = list(\n      model = \"gpt-4o\",\n      messages = list(\n        list(role = \"system\", content = instructions),\n        list(role = \"user\", content = abstract_fragment)\n      )\n    )\n  )\n}\n\nsend_subbatch <- function(idx_1, idx_2) {\n  requests <-\n    map2(grant_sample14_16_fn[idx_1:idx_2],\n         grant_sample14_16_abs[idx_1:idx_2],\n         create_request)\n  \n  jsonl_fn <- glue::glue(\"requests_{idx_1}_{idx_2}.jsonl\")\n  fileConn <- file(jsonl_fn, open = \"w\")\n  for (request in requests) {\n    writeLines(toJSON(request, auto_unbox = TRUE), fileConn)\n  }\n  \n  close(fileConn)\n  \n  files_response <- POST(\n    'https://api.openai.com/v1/files',\n    add_headers(Authorization = paste(\"Bearer\", api_key)),\n    body = list(\n      purpose = \"batch\",\n      file = upload_file(jsonl_fn)\n    ),\n    encode = \"multipart\"\n  )\n  # Define the JSON body\n  body <- list(\n    input_file_id = content(files_response)$id,\n    endpoint = \"/v1/chat/completions\",\n    completion_window = \"24h\"\n  )\n  \n  # Make the POST request\n  batch_response <- POST(\n    \"https://api.openai.com/v1/batches\",\n    add_headers(\n      Authorization = paste(\"Bearer\", api_key),\n      `Content-Type` = \"application/json\"\n    ),\n    body = body,\n    encode = \"json\"\n  )\n  \n  batch_response\n}\n\nget_batch_status <- function(batch_response_id) {\n  response_status <- GET(\n    paste0(\"https://api.openai.com/v1/batches/\",\n           batch_response_id), \n    add_headers(\n      Authorization = paste(\"Bearer\", api_key),\n      `Content-Type` = \"application/json\"\n    )\n  )\n  \n  cat(content(response_status, as = 'text', encoding = 'UTF-8'))\n}\n\nbatch_status_string <- function(batch_response_id) {\n  response_status <- GET(\n    paste0(\"https://api.openai.com/v1/batches/\",\n           batch_response_id), \n    add_headers(\n      Authorization = paste(\"Bearer\", api_key),\n      `Content-Type` = \"application/json\"\n    )\n  )\n  \n  content(response_status, as = 'parsed', encoding = 'UTF-8')$status\n}\n\ncount_completed <- function(batch_response_id) {\n  response_status <- GET(\n    paste0(\"https://api.openai.com/v1/batches/\",\n           batch_response_id), \n    add_headers(\n      Authorization = paste(\"Bearer\", api_key),\n      `Content-Type` = \"application/json\"\n    )\n  )\n  \n  content(response_status, as = 'parsed', encoding = 'UTF-8')$request_counts$completed\n}\n```\n:::\n\n\n### The Dis-batcher\n\nAnother title was going to be the 'Dis-Bachelor'\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# start_idx <- 601L\nincrement <- 90L\n\nno_fails <- TRUE\n\nwhile (no_fails){\n  current_batch_response <- send_subbatch(start_idx, (start_idx + increment))\n  Sys.sleep(10)\n  bss <- batch_status_string(content(current_batch_response)$id)\n  attempts <- 0L\n  completed_old <- 0L\n  \n  while (bss == \"in_progress\") {\n    # what would be better is to only increment waiting if there hasn't been progress\n    # specifically, once there's progress, ping once per minute\n    Sys.sleep(pmin(attempts * 30, 800))\n    bss <- batch_status_string(content(current_batch_response)$id)\n    completed <- count_completed(content(current_batch_response)$id)\n    if (completed == completed_old) {\n      attempts <- attempts + 1\n    }\n    completed_old <- completed\n    \n  }\n  \n  if (bss == 'completed') start_idx <- start_idx + increment + 1L\n  # This should maybe be different\n  # I hadn't counted on a \"fail mode\" being that I still overshoot the limit\n  # In that case, what it should do is (at least temporarily) reduce the increment\n  if (bss == 'failed') no_fails <- FALSE\n}\n```\n:::\n\n## Other Code for Reading in Output\n\nHonstly I should probably just delete what's below, but it's easier to paste it into the appendix. This code read in output that I didn't end up using for the post (I had made a mistake in the prompt permuter that led to prompts with double the description of one of the dimensions and no description of the other dimension).\n\n### Reading in and minimally processing data AHHHH\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_jsonl_path <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.jsonl'\nfirst_jsonl_as_txt <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.txt'\n\nfirst_jsonl_cleaned <- \n  readLines(first_jsonl_path) %>% \n  # str_remove_all(fixed('\\n')) %>%\n  # str_remove_all('\\\\n') %>%\n  # str_remove_all('\\\\\\n') %>%\n  str_remove_all('\\\\\\\\n') %>%\n  # str_remove_all('\\\\\\\\\\n') %>%\n  str_remove_all('\\\\\\\\') %>% \n  str_remove_all('json') %>% \n  str_remove_all(\"`\") %>% \n  str_replace_all(fixed('\\\"'), '\"')\n\nfirst_jsonl_cleaned[1]\n\nfromJSON(first_jsonl_cleaned[1])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_jsonl_as_txt <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.txt'\n\nfirst_jsonl_cleaned_txt <- \n  readLines(first_jsonl_as_txt) %>% \n  # str_remove_all(fixed('\\n')) %>%\n  # str_remove_all('\\\\n') %>%\n  # str_remove_all('\\\\\\n') %>%\n  str_remove_all('\\\\\\\\n') %>%\n  # str_remove_all('\\\\\\\\\\n') %>%\n  str_remove_all('\\\\\\\\') %>% \n  str_remove_all('json') %>% \n  str_remove_all(\"`\") %>% \n  str_replace_all(fixed('\\\"'), '\"') %>% \n  str_remove_all('[^\\x20-\\x7E]+')\n\nfirst_jsonl_cleaned_txt[1]\n\nfromJSON(first_jsonl_cleaned_txt[1])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsr <- fromJSON(readLines(''))\nclass(sr)\n\n# Function to process a single JSONL file\nprocess_jsonl <- function(file_path) {\n  # Read the file and correct the JSON in each line\n  corrected_lines <- readLines(file_path) %>%\n    str_replace_all(\"\\\\n\", \"\") %>%\n    str_replace_all(\"(?<=content\\\": \\\")```json\", \"\") %>%\n    str_replace_all(\"```\", \"\") \n\n  # Parse the corrected JSON lines\n  json_list <- map(corrected_lines, fromJSON, simplifyVector = FALSE)\n\n  # Extract the relevant data\n  extracted_data <- map_dfr(json_list, function(entry) {\n    content_list <- entry$response$body$choices\n    tibble(\n      content = map_chr(content_list, ~ .x$message$content),\n      file_name = basename(file_path)  # Add file name for tracking\n    )\n  })\n  \n  #Parse the content field as JSON now\n  extracted_data %>% mutate(content = map_chr(content, fromJSON)) %>%\n                          unnest_wider(content)\n\n}\n\nfirst_jsonl_path <- 'data/experiment_batches/batch_0mcdj68xutq2IytYqjOKj0j5_output.jsonl'\n\n# 1. Read and Correct JSON\nraw_lines <- readLines(first_jsonl_path)\ncorrected_lines <- raw_lines %>%\n  # str_replace_all(\"\\\\n\", \"\") %>%\n  str_replace_all(\"\\n\", \"\") %>%\n  str_replace_all(\"(?<=content\\\": \\\")```json\", \"\") %>%\n  str_replace_all(\"```\", \"\")\n\n# 2. Parse Corrected JSON\njson_list <- map(corrected_lines, fromJSON, simplifyVector = FALSE)\n\nextracted_data <- map_dfr(json_list, function(entry) {\n  content_list <- entry$response$body$choices\n  tibble(\n    content = map_chr(content_list, ~ .x$message$content),\n    file_name = basename(first_jsonl_path)  \n  )\n})\n\nparsed_content <- extracted_data %>% \n    mutate(parsed_content = map(content, fromJSON)) %>%\n    select(-content) %>%  # You can drop the original `content` if you want\n    unnest_wider(parsed_content) # This will unnest the JSON to the dataframe\n\nprocess_jsonl <- function(file_path) {\n  # Read and correct JSON (remove \\n and extra quotes)\n  corrected_lines <- readLines(file_path) %>%\n    str_replace_all(\"\\\\n\", \"\") %>%\n    str_replace_all(\"(?<=content\\\": \\\")```json\", \"\") %>%\n    str_replace_all(\"```\", \"\") %>%\n    str_replace_all(\"\\\\\\\\\", \"\") %>% # Remove extra backslashes \n    str_replace_all('\"', '') # Remove extra quotation marks\n\n  # Parse corrected JSON\n  json_list <- map(corrected_lines, fromJSON, simplifyVector = FALSE)\n\n  # Extract the data\n  extracted_data <- map_dfr(json_list, function(entry) {\n    content_list <- entry$response$body$choices\n    tibble(\n      content = map_chr(content_list, ~ .x$message$content),\n      file_name = basename(file_path)  \n    )\n  })\n  \n  # Return the extracted data for now\n  extracted_data \n}\n\nprocess_jsonl(first_jsonl_path)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd('data/experiment_batches')\nfiles <- list.files()\n\ndf <- fromJSON(test_json)\ndf\n\ntest_json <- \n  readLines(files[1], 1) #%>%\n  # map(., jsonlite::fromJSON) %>%\n  # unlist()\n\ncat(str_remove_all(test_json, fixed(\"\\\\n\")))\n\n\ndf <- stream_in(file(files[1]))\n\nView(df)\n\ncat(df$response$body$choices[[2]]$message$content)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd('data/experiment_batches')\nfiles <- list.files()\n\n# Read the JSONL file into a list\njson_list <- stream_in(file(files[[1]]), simplifyVector = FALSE)\n\n# Extract relevant content and handle multiple responses\nextracted_data <- map_dfr(json_list, function(entry) {\n  content_list <- entry$response$body$choices \n  \n  tibble(\n    content = map_chr(content_list, ~ fromJSON(.x$message$content)$broader_impact_description)\n  )\n})\n```\n:::\n\n\n### Trying to clean ChatGPT's totally farkakte output\n\nYou'll have to redo the working directory here\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd('data/experiment_batches')\njsonls_fn <- list.files(pattern = 'jsonl$')\n  \nunfuck_text <- function(fucked_text) {\n  fucked_text %>%\n    str_remove_all('\\\\\\\\n') %>%\n    str_replace_all('\\\\\\\\\"', '\"') %>%\n    str_remove_all('```') %>%\n    str_remove_all('\\\\s{2,}') %>%\n    str_remove_all('json[l]*')\n}\n\nfor (jsonl_fn in jsonls_fn){\n  t <- readLines(jsonl_fn)\n  writeLines(\n    unfuck_text(t),\n    con = str_replace(jsonl_fn, 'jsonl$', 'txt'))\n}\n\nget_imm_incl_codes <- function(text, which_one){\n  if (which_one == 'immediacy') {\n    temp_matrix <-\n    str_match_all(text, 'immediacy_code[\"\\']: [\"\\']([a-z]+)') %>%\n    `[[`(., 1)\n  } else {\n    temp_matrix <-\n    str_match_all(text, 'inclusion_code[\"\\']: [\"\\']([a-z]+)') %>%\n    `[[`(., 1)\n  }\n  temp_matrix[, 2]\n}\n\ntxt_file_to_tbl <- function(dot_txt) {\n  text_txt <- readLines(dot_txt)\n  \n  ids <- vector('character', length(text_txt))\n  inclusions <- vector('list', length(text_txt))\n  immediacies <- vector('list', length(text_txt))\n  \n  for (i in seq_along(text_txt)) {\n    ids[[i]] <-\n      str_match(text_txt[[i]], '\\\\\"custom_id\\\\\": \\\\\"(\\\\d+;[-a-z_]+)')[1, 2]\n    immediacies[[i]] <- get_imm_incl_codes(text_txt[[i]], 'immediacy')\n    inclusions[[i]] <- get_imm_incl_codes(text_txt[[i]], 'inclusion')\n  }\n  \n  # You'll throw and error if any of these don't work\n  test_for_equality <-\n    all(map2_lgl(immediacies, inclusions, ~ length(.x) == length(.y)))\n  all(map_lgl(immediacies, ~ length(.x) > 0))\n  all(map_lgl(inclusions, ~ length(.x) > 0))\n  \n  \n  tibble(\n    id = rep(ids, times = map_int(immediacies, length)),\n    immediacy = unlist(immediacies),\n    inclusion = unlist(inclusions)\n  ) %>%\n    tidyr::separate_wider_delim(id, ';', names = c('grant', 'condition'), cols_remove = FALSE)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd('data/experiment_batches')\ntxt_fns <- list.files(pattern = 'txt$')\n\ncoded_tbl <- \n  map(txt_fns, txt_file_to_tbl) %>% \n  list_rbind()\n\ncoded_tbl <- \n  coded_tbl %>% \n  separate_wider_delim(condition, delim = '-', names = c('opening', 'order_inc', 'order_imm')) %>% \n  relocate(everything(), id)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}