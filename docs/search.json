[
  {
    "objectID": "section_text.html",
    "href": "section_text.html",
    "title": "Text Analysis",
    "section": "",
    "text": "Many moons ago, I read Julia Silge and David Robinson’s Text Mining with R and then Emil Hvitfeldt and Silge’s and Supervised Machine Learning for Text Analysis in R. Though I have never done anything professional with text analysis, I’ve done a few things here and there. Some of them are here, below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn LLM Experiment\n\n\nUsing a Certain Generative Pre-trained Transformer to Code NSF Grants\n\n\n\nJustin Dollman\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Sentiment Dictionaries\n\n\nFor Reference Purposes\n\n\n\nJustin Dollman\n\n\nJan 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ezra Klein Show\n\n\nWhat can we learn through text?\n\n\n\nJustin Dollman\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "section_ur.html",
    "href": "section_ur.html",
    "title": "Urban-Rural Polarization",
    "section": "",
    "text": "Forthcoming section.\nOne of my dissertation projects examines the urban-rural ideological divide globally. Most academic studies and journalistic commentary alike analyze the American urban-rural divide, but insofar as urbanity and rurality per se either cause or reflect dispositions and preferences linked to political ideology, this should be apparent in ideological self-placement, values, and voting the world over. Is it?\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "section_about.html",
    "href": "section_about.html",
    "title": "About Me",
    "section": "",
    "text": "Since this site’s raison d’être is professional, I should say a word or two about me qua wage laborer.\nI’m a PhD candidate in political science at SUNY Stony Brook where I’m working on my dissertation under the advisement of Michael Peress. During my time here I’ve had the pleasure of taking courses with such luminaries as Stanley Feldman, Andy Delton, Yanna Krupnikov, and Vittorio Mérola.\nAt Stony Brook I’ve been the ‘instructor of record’ for Introduction to Statistics (undergraduate and graduate) and Experiments in Political Science (undergraduate). In addition to those bureaucratically sanctioned courses, I also taught an Introductory Math Camp to incoming PhD students and, along with my esteemed former colleague Pei-Hsun Hsieh, I co-taught a weekly introduction to R during the Fall 2019 semester to graduate students from sundry social sciences (the infamous flyer). Simultaneously with the courses here at Stony Brook, I’ve been teaching statistics and data analysis to students from other graduate programs as well as MBA students.\nBefore landing on the Ye Longe Isle of Bagels & Hockey Fanatics, I taught English for a few years in Ecuador and Peru. And before that, I triple majored in philosophy, psychology, and German at the University of Arkansas (with a year spent at Karl-Franzens Universität in Graz, Austria)."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html",
    "href": "posts_text/2024-01-24-klein-text/index.html",
    "title": "The Ezra Klein Show",
    "section": "",
    "text": "For reasons lost to oblivion, I decided to analyze transcripts of Ezra Klein’s podcasts using elementary text analysis. The following was the result."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#interviewing-style",
    "href": "posts_text/2024-01-24-klein-text/index.html#interviewing-style",
    "title": "The Ezra Klein Show",
    "section": "Interviewing Style",
    "text": "Interviewing Style\nBefore even looking at the content of the episodes, before looking at what words are said, sentiments are expressed and topics are covered, we can look at volume of speaking.\n  \nWe can also compare this distribution of how much Ezra speaks as interviewer to how much other interviewers speak. After all, it’s hard to really feel what the numbers in the graph above mean without comparing them to the relative talkativeness of other interviewers. Below is a comparison of Ezra with two other interviewers, both of whom have had Ezra on as a guest on their programs: Rob Wiblin (80,000 Hours) and Tyler Cowen (Conversations with Tyler).\n\nThis is a little more interesting, I think.\nMy hypothesis before crunching these data was that Tyler was speak the least (and it wouldn’t be close), then Rob, then Ezra. And that turned out to be right.1 After all, Ezra speaks quiet a bit because people tune into Ezra’s show in order to listen to Ezra. Though Tyler’s show does have “Tyler” in the name, he frequently announces that his interviews are the interviews that he wants, not what the listeners might want. And his listener-be-damned approach often results in quick questions and abrupt topic changes. The 80,000 Hours podcast has this intermediate position where Rob often positions himself as a listener might and restates what the speaker said, gives the conventional wisdom against which the interviewee can react, and ask follow-up questions."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#ezras-words",
    "href": "posts_text/2024-01-24-klein-text/index.html#ezras-words",
    "title": "The Ezra Klein Show",
    "section": "Ezra’s Words",
    "text": "Ezra’s Words\nAlright, so what does this ingenious interviewer from Irvine actually talk about? Among the ways of answering that question, one is to just look at the words he uses. The simplest way of operationalizing ‘words used’ is to look at “unigrams,” which result from splitting the unstructured text at every whitespace2, making each whitespace-separated unit into a row in a dataframe, and counting how many rows each token (word) appears in that resulting dataframe. Ezra’s result is below.3\n\n \nBut you can also get slighly more sophisticated and look at contiguous words. So, instead of splitting at every whitespace, you split at every second or third whitespace. The nice thing about that is you start to see phrases, turns of speech. For example, if you were curious why “people,” “lot,” and” bit” appear in the unigrams, the bottom-most panel will give you your answer.\n  \nAmong the bigrams and trigrams, an Ezra Klein Show fan will also be pleased to see Ezra’s pat phrases for parts of his podcast: “final question”, “books you’d recommend,” and “email [is] ezrakleinshow@nytimes.com.” I’m not sure if seeing these phrases is interesting, but they are nice proofs of concept for the method. You also see that the bigrams are dominated by pairs of words that, yes, do have a space between them, but they are lexically one thing. I’d argue that “past couple” and “minute ago” are only instances of more-or-less independent words coming together.\nNow let’s try something potentially more interesting. We can also look at how he uses words differentially. Why would that be useful? Well, just looking at the words he uses might tell us that he’s a guy with a show that covers topics X, Y, and Z that use the words associated with those topics.4 One way we can “control for” topic is to contrast Ezra’s words with those of his guest. Abstractly, the idea is that if person A and B are talking about subject S, the words that differentiate A and B are not going to be those that generically describe subject S. Rather, they will be the words idiosyncratically associated with each interlocutor.\n\nThis surprised me. Sure, one would expect most words to be along the diagonal line which indicates that the word is used in equal proportions by both Ezra and his guests, but I really expected this to show more unique usages in the off-diagonal space.\nWe can also compare the words of the Ezra Klein Show to the words used in introductory political science textbooks. The idea of such an analysis would be to ask, “When Ezra talks politics, which topics (proxied by words) does he dedicate space to than an introduction to politics? Which topics does he spend less time one?” From these results one could hope to find out something about which aspects of politics Ezra finds interesting and important, and which less so. Unfortunately, I carried out the analysis and the result is below:\n \n I call this ‘analysis’ “unfortunate” because it doesn’t generate any insight. I mean, what are you really supposed to get from this? One takeaway for the practitioner of text analysis is that you might have to do a decent amount of preprocessing before you can make impactful deliverables. Two useful ways you can preprocess raw text here are lemmatizing (or stemming) and using a parts-of-speech tagger to extract only nouns. What both of those steps share is the winnowing out (or consolidating) of noise/chaff. In a subsequent post I’ll try these feature-space-shrinking techniques out."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#sentiment-analysis",
    "href": "posts_text/2024-01-24-klein-text/index.html#sentiment-analysis",
    "title": "The Ezra Klein Show",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nI think most listeners of The Ezra Klein Show have noticed that Ezra is unique among interviewers in his emotionality. Not emotionality in a histrionic sense, but more like “general emotional urgency.”5 So, in theory it would be interesting to use text analysis to get at this aspect of The Ezra Klein Show that sets the show out from others and uniquely characterizes Ezra as an interviewer. My hunch, however, is that the basic methods of sentiment analysis are not up to the task. In a forthcoming post I’ll dive deeper into why (and what methods would yield more insightful results), but for now let’s very briefly look at the results of these dictionary-based methods.\nAs Tolstoy famously noted, if there are bad ways of doing something, they’ll bad in different ways. Let’s put that to the test with sentiment dictionaries. There are various ‘sentiment lexicons’ that will limpet onto your text to give you the raw materials for calculating sentiment scores at whatever level aggregation you choose. Here I’ve chosen three supposedly general-purpose dictionaries and calculated the sentiment score for each episode of TEKS. As you can see in the table below, their episode-level correlations are very high.\n\n\n\n\nBING\nAFINN\nNRC\n\n\n\n\nBING\n1\n\n\n\n\nAFINN\n0.91\n1\n\n\n\nNRC\n0.83\n0.85\n1\n\n\n\nNice. Convergent validity✅. And now I don’t have to read Anna Karenina.\nIf we want to be a little more aesthetic (💅🏻), we can look at sentiment scores with a plot over time.\n\nSure enough, it looks like the three dictionaries’ scores also (unsurprisingly) strongly correlate at the week level as well. We also see that, by and large, the overall sentiment of episodes is positive.\nI’ve also highlighted which episode each dictionary picks out as the most extreme, both positive and negative While they disagreed on most positive (hence the three positive episodes), there was three-way agreement on the most negative: his interview Rachel Zoffness’s author of The Pain Management Workbook. So it’s hardly surprising that these out-of-the-box algorithms would rank it as the most negative. It is disappointing, though. I went back and listened to the episode after finding out it was universally declared the most negative and … isn’t actually that negative. The post October 7th episodes on the situation in Israel and Gaza are much much more negative. Lesson: convergent validity does not imply construct validity.\nRight, so the simple valence continuum (positive/negative) was mostly a flop. How about using a sentiment dictionary that maps words to eight different emotions and seeing which emotions predominate in The Ezra Klein Show? Eight is four times bigger than two, so that sounds promising. Below is what that looks like over time.\n\nOk, this could be interesting. We see that trust is consistently the great contributor to the show’s positivity and fear to its negativity. Ideally we’d compare these numbers to those of another show. But one way we can kick the tires on this analysis is by seeing what words drive trust and fear’s respective high-rankings.\n\nI’m just … not sure about this. You see why dictionary-based isn’t a particularly valid way of inferring emotion from word usage. Just looking at the Trust side of things, “kind” appears in collocations like “kind of bad,” (which would have the valence wrong) and, “kind of [entity],” which is not usually valenced. It’s certainly not always (probably not even usually) an indicator of trust. You can go down the list and find similar (and other!) problems with every other word. I’m honestly not sure how people take this seriously."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#concluding-thoughts",
    "href": "posts_text/2024-01-24-klein-text/index.html#concluding-thoughts",
    "title": "The Ezra Klein Show",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nI’m not sure if I would have predicted this beforehand, but by far the most interesting plots here are the two looking at simple volume of speaking. Why is this? I think it has to be because it’s the only case where the method (counting, in this case) adequately represents the thing it purports to measure. I suppose word counts do that as well, but there’s no spice when there’s no implicit contrast (as is the case simple word counts) and then when there is contrast (in those graphs that plot proportions against each other), nothing pops out. Ex ante it might have be unknowable that was going to happen."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#acknowledgements-and-arigatos",
    "href": "posts_text/2024-01-24-klein-text/index.html#acknowledgements-and-arigatos",
    "title": "The Ezra Klein Show",
    "section": "Acknowledgements and Arigatos",
    "text": "Acknowledgements and Arigatos\nAndrew Heiss’s data visualization course inspired (and enabled) me to have nice(ish) looking text on my graphs. If you know some ggplot2 and are looking to become high-intermediate in your ggplot2 skills, I highly recommend his course.\nThe vast majority of the analyses you see here I learned back in the day from Julia Silge and Dave Robinson’s Tidy Text Mining with R book. I recommend it to anyone as a jumping off point for doing text analysis. Most of the analyses here don’t go (too far) beyond what you can do with the tools you pick up in that book"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#afinn",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#afinn",
    "title": "Notes on Sentiment Dictionaries",
    "section": "AFINN",
    "text": "AFINN\nThe subtitle of the original publication says it all, “Evaluation of a word list for sentiment analysis in microblogs.” In the paper’s abstract, the great Dane and presumably the namesake of the lexicon, Finn Arup Nielsen, lays out more explicitly why he created a new sentiment lexicon, “There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs.” There’s AFINN’s origin story.3\nOne of the unique features of the AFINN lexicon is that words are mapped to integers instead of merely {positive, negative}. Here you can see a few words at each value:\n\nset.seed(1)\n\nget_sentiments('afinn') %>%\n  group_by(value) %>% \n  add_count(name = 'count') %>% \n  slice_sample(n = 4) %>%\n  summarise(\n    `no. words` = mean(count),\n    words = glue_collapse(word, sep = ', ')) %>% \n  arrange(value) %>% filter(value != 0)\n\n# A tibble: 10 × 3\n   value `no. words` words                                      \n   <dbl>       <dbl> <glue>                                     \n 1    -5          16 motherfucker, bitches, cocksuckers, bastard\n 2    -4          43 scumbag, fucking, fraudsters, fucked       \n 3    -3         264 moron, destroy, despair, scandals          \n 4    -2         966 animosity, censors, robs, touts            \n 5    -1         309 imposing, unclear, demonstration, uncertain\n 6     1         208 share, extend, feeling, commit             \n 7     2         448 tranquil, consent, supportive, sympathetic \n 8     3         172 audacious, classy, luck, gracious          \n 9     4          45 exuberant, wonderful, rejoicing, wowww     \n10     5           5 hurrah, outstanding, superb, thrilled      \n\n\nAnd yes, I did set the seed above to avoid randomly showing you certain words.\nYou might be wondering\n\nwhy -5 to 5 and\nhow he assigned words to those numbers\n\nQuoting him on the former, “As SentiStrength it uses a scoring range from −5 (very negative) to +5 (very positive).” Convention is powerful. As for the latter, the question of how numbers were assigned to words, “Most of the positive words were labeled with +2 and most of the negative words with –2 […]. I typically rated strong obscene words […] with either –4 or –5.” So he was basically winging it.\nAnother unique feature: the dictionary has 15 bigrams, 10 of which are below.4\n\nset.seed(12)\nget_sentiments('afinn') %>%\n  filter(str_detect(word, ' ')) %>% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 2\n   word          value\n   <chr>         <dbl>\n 1 cashing in       -2\n 2 no fun           -3\n 3 green wash       -3\n 4 not good         -2\n 5 dont like        -2\n 6 not working      -3\n 7 some kind         0\n 8 green washing    -3\n 9 cool stuff        3\n10 messing up       -2\n\n\n\nSize: 2477 entries\nCoverage: In addition to the bog-standard sentiment words, it has words all the cool kids were saying in the late 2000, early 2010s.\nR Packages: tidytext, lexicon, textdata\nPublication: Here\nBottom Line: It’s been superseded by VADER (below)"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#bing-aka-hu-and-liu",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#bing-aka-hu-and-liu",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Bing (aka Hu and Liu)5",
    "text": "Bing (aka Hu and Liu)5\nAccording to the man himself, “This list [i.e., the lexicon] was compiled over many years starting from our first paper (Hu and Liu, KDD-2004).” I’m not sure what the post-publication compilation process was, but the original process is well-described in the original publication. Essentially, they started with adjectives6 with obvious polarity (e.g., great, fantastic, nice, cool, bad, dull) as ‘seed words’ and collected synonyms (and antonyms) of those words, then synonyms and antonyms of those words, and so on, iteratively. To do this, they used WordNet, a chill-ass semantic network. One thing that’s nice about the resulting lexicon is that it’s topic general. That is, though they developed this lexicon for the specific purpose of determining people’s opinions about product features in product reviews, it has a generality beyond that.\nActually looking at the words, you’ll notice there’s some weirdness.\n\nhead(get_sentiments('bing'), 10)\n\n# A tibble: 10 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n\n\nFirst, I’m not sure what “2-faces” is. If you say that it’s a solecistic rendering of “two-faced,” I’d say probably. In their appropriate alphabetic order, both “two-faced” and “two-faces” appear later in the dictionary. Anyway, you’ll notice as well that a lot of the words would reduce to a single lemma if we lemmatized the dictionary. You can think of that as a positive feature of the BING dictionary. It means you don’t have to have lemmatized (or stemmed) text. But its inclusion of derived words seems a bit haphazard. The abort-aborted pair is there, but abolish is hanging out along without its past tense.\n\nSize: In the tidytext package the BING dictionary has 6786 terms (matching what his website says, “around 6800 words”)\nR Packages: tidytext, lexicon, textdata\nBottom Line: It’s a classic and got wide coverage, but not as good as VADER or NRC-EIL."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#loughran-mcdonald-master-dictionary-w-sentiment-word-lists",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#loughran-mcdonald-master-dictionary-w-sentiment-word-lists",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Loughran-McDonald Master Dictionary w/ Sentiment Word Lists",
    "text": "Loughran-McDonald Master Dictionary w/ Sentiment Word Lists\nI’m honestly why this dictionary is included in packages – not because it’s bad7, but because it’s so (so so so) niche. If you’re doing text analysis on financial text (or aware of cool research doing this), please drop me a line and tell me about it.\nIf you want to learn about it, here’s the page.\n\nR Packages: tidytext, lexicon, textdata"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-original",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-original",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC (original)",
    "text": "NRC (original)\nSaif Mohammad and friends developed a few National Research Council (NRC) of Canada-sponsored sentiment dictionaries. The first of them assigns words not only polarity labels (positive, negative), but emotion labels as well:\n\ntidytext::get_sentiments('nrc') %>% \n  count(sentiment, sort = TRUE)\n\n# A tibble: 10 × 2\n   sentiment        n\n   <chr>        <int>\n 1 negative      3318\n 2 positive      2308\n 3 fear          1474\n 4 anger         1246\n 5 trust         1230\n 6 sadness       1187\n 7 disgust       1056\n 8 anticipation   837\n 9 joy            687\n10 surprise       532\n\n\nThese eight emotions below negative and positive were theorized by Bob Plutchik to be fundamental, or something.8 I’m going to ignore those emotions in this subsection. I’m also not going to talk too much about this dictionary, because it’s superseded by the real-valued its successors, the NRC-EIL and NRC-VAD (below).\nBefore I leave this lexicon, though, one bizarre thing about it: 81 words are both positive and negative (???)\n\nget_sentiments('nrc') %>% \n  filter(sentiment %in% c('negative', 'positive')) %>% \n  add_count(word) %>% \n  filter(n > 1) %>% select(-n)\n\n# A tibble: 162 × 2\n   word       sentiment\n   <chr>      <chr>    \n 1 abundance  negative \n 2 abundance  positive \n 3 armed      negative \n 4 armed      positive \n 5 balm       negative \n 6 balm       positive \n 7 boast      negative \n 8 boast      positive \n 9 boisterous negative \n10 boisterous positive \n# ℹ 152 more rows\n\n\nAs a matter of semantics, I get it for some of these (balm I do not get, though). Practically, if you’re doing an analysis with this dictionary, you’re probably going to want to remove all these terms before calculating sentiment scores.\n\nR Packages: tidytext, lexicon, textdata\nSize: 5464 words (positive and negative, not words with ambiguous polarity)\nBottom Line: Superseded by Saif Mohammed’s subsequent efforts."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-eil",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-eil",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC-EIL",
    "text": "NRC-EIL\nThis thing’s value-added, as my economist friend likes to say, is that instead of a simple ‘positive’ or ‘negative’ value for each sentiment entry, there’s a number between -1 and 1. “Real-valued,” as measurement-heads say. This is actually extremely important if you’re aggregating word-level sentiment into something bigger (which … honestly, email me if you’re doing anything other than that.) How they got these real-valued polarity scores is actually a pretty interesting methods story if you’re into that kind of thing, but I won’t go into ‘MaxDiff scaling’ here. One very important thing to note about this dataset, though, is that valence is that polarity isn’t in this dataset. This vexed me for a minute before I realized that it’s in the real-valued NRC-VAD (below). So, on the off chance you’re looking for the best measurement of Plutchik’s eight basic emotions (and that’s a very off chance), this is the best place to look.\n\nR Packages: textdata\nExamples: You can see an example of an analysis of Ezra Klein’s podcasts here.\nBottom Line: I’m not sure why it exists, but it’s the only lexicon doing what it’s doing."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-vad",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-vad",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC-VAD",
    "text": "NRC-VAD\nOnce you have a hammer, everything starts looking like a nail. That’s how I explain the existence of this dictionary to myself. Saif Mohammed & Co. found these cool scaling technique and were like, “On what dimensions can we scale more words?” They seem to have stumbled on this idea the three most fundamental dimensions in concept space are valence, arousal, and dominance. Maybe it’s an indictment of my memory, perhaps an indictment of the psychology department at the University of Arkansas, but I managed to graduate summa cum laude with a degree in psychology without ever hearing of this. Regardless of supposed fundamental dimensions in concept space, valence is fundamental and is just another word for polarity which is the main thing people are doing with sentiment dictionaries.\nThis also led to my favorite table in I’ve ever seen in an academic article\n\nWhenever I need to express that something is pure bad valence, I now reach for the phrase ‘toxic nightmare shit.’\n\nR Packages: textdata\nBottom Line: Hell yeah. This is a good one."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#socal",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#socal",
    "title": "Notes on Sentiment Dictionaries",
    "section": "SOCAL",
    "text": "SOCAL\nIt’s the Semantic Orientation CALculator! Like VADER (below), SOCAL is both a sentiment dictionary and rules for modifying words’ sentiment given the context in which they appear. Below, I’ll only briefly consider the dictionary part. I recommend reading the publication both for more details on SOCAL itself as well as sentiment analysis generally. In a sea of sentiment articles that are slapdash publications from some “Proceedings of blah blah blah” or “Miniconference on yak yak yak” this one really stands out for its professionalism and thoroughness.9 As for whether or not you should actually use this dictionary, uh, you should keep reading.\nOne fun thing to note about SOCAL’s dictionary: it has more entries with spaces than any other dictionary I’ve seen.\n\nhash_sentiment_socal_google %>% \n  group_by(n_gram = str_count(x, ' ') + 1) %>% \n  count(n_gram) %>% \n  ungroup()\n\n# A tibble: 8 × 2\n  n_gram     n\n   <dbl> <int>\n1      1  2071\n2      2  1097\n3      3    97\n4      4    17\n5      5     3\n6      6     3\n7      7     1\n8      8     1\n\n\nThis dictionary has not only a huge bigrams:unigrams ratio, but it has sextagrams, a septagram, and even an octogram! This never happens! Let’s look at the n-grams where n > 4\n\nhash_sentiment_socal_google %>% \n  filter(str_count(x, ' ') > 3)\n\nKey: <x>\n                                                           x         y\n                                                      <char>     <num>\n1:                          darker and funnier than expected -4.092375\n2:                                     every other word is f -1.955614\n3:                          in your face everywhere you turn -5.357512\n4: lowbump bump bump bump bump bump bumpbumpbumpbump lowbump  5.622471\n5:                     throw your peanut shells on the floor -5.186874\n6:                                      trying to get on top -2.573428\n7:                               type stype suh huh uh huh's -4.281949\n8:                            write an awesome story to sell -6.262021\n\n\nI, uh, don’t really know what to make of these. There’s actually a restaurant named “Lambert’s Cafe” in Ozark, Missouri where you get peanuts in tin buckets and “throw your peanut shells on the floor” and, unless I’m remembering it wrong, it’s something people like about the place.\nSpeaking of things that are starting to be concerning, the distribution of scores:\n\nlibrary(ggtext)\nggplot(hash_sentiment_socal_google, aes(y)) +\n  geom_density() +\n  labs(\n    y = '',\n    x = 'Valence Score',\n    title = '**What tale tell ye**, ye two thick tails?'\n  ) +\n  theme(plot.title = element_markdown(face = 'italic'))\n\n\n\n\nNo, that density plot isn’t glitching. There really are words out there in the extremes:\n\nhash_sentiment_socal_google %>% \n  filter(abs(y) > 15)\n\nKey: <x>\n                            x         y\n                       <char>     <num>\n 1:    almost mafiosio styled  15.61625\n 2: automatically bestselling  17.60440\n 3:        coming of age isms  17.62518\n 4:           cushion handled  23.45929\n 5:                 hop ified -30.16008\n 6:          keyboard crafted  30.73891\n 7:      more than palateable  25.61696\n 8:          oven to stovetop  19.49040\n 9:             piano blessed  30.40257\n10:   rustic yet contemporary  16.31055\n11:           slotted spooned  18.19201\n12:              thick spoked  20.43281\n\n\nAt this point, you might be wondering … what scale is this? And what are values for our vanilla valence-indicators “good” and “bad”?\n\nhash_sentiment_socal_google %>% \n  filter(x %in% c('good', 'bad'))\n\nKey: <x>\n        x         y\n   <char>     <num>\n1:    bad -1.636520\n2:   good  1.872093\n\n\nOk, that’s fine. Maybe. But here’s something that probably isn’t fine:\n\nhash_sentiment_socal_google %>% \n  filter(str_detect(x, 'good|bad'))\n\nKey: <x>\n                  x          y\n             <char>      <num>\n1:     average good  0.4205258\n2:              bad -1.6365198\n3:        feel good  1.2984294\n4:             good  1.8720931\n5: good intentioned -4.1537399\n6:     good natured -1.5298719\n7:         half bad -3.9237520\n\n\nHere is where I lost all faith. “Good intentioned” and “good natured” are negative?!\nAt this point I’m going to call it a day with SOCAL. At some point I might write to the SOCAL authors or Tyler Rinker to see if something has gone wrong.\n\nPublication: Again, I truly recommend it\nSize: 3290 entries\nR Packages: lexicon"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#syuzhet",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#syuzhet",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Syuzhet",
    "text": "Syuzhet\n“The default method,”syuzhet” is a custom sentiment dictionary developed in the Nebraska Literary Lab. The default dictionary should be better tuned to fiction as the terms were extracted from a collection of 165,000 human coded sentences taken from a small corpus of contemporary novels.”\nNow, it does include a lot of words I find to be neutral (e.g., “yes”, “true”)\nWe can check out the distribution of the terms in my experimental sideways histogram below:\n\n# ggplot(key_sentiment_jockers, aes(value)) +\n#   geom_histogram(breaks = seq(-1, 1, by = 0.1), color = 'white') +\n#   theme(\n#     panel.grid.major.x = element_blank(),\n#     panel.grid.minor.x = element_blank(),\n#     axis.text.x = element_text(margin = margin(t = -10, b = 5)),\n#     plot.title = element_text(face = 'bold', size = rel(1.3))) +\n#   scale_y_continuous(breaks = c(500, 1000, 1500)) +\n#   labs(\n#     x = 'Jockers/Syuzhet Value',\n#     y = 'Terms in Dictionary',\n#     title = 'Distribution of Sentiment Values in Syuzhet Dictionary'\n#   )\n\n\ndistinct_values <- pull(distinct(key_sentiment_jockers, value))\n\nggplot(key_sentiment_jockers, aes(value)) +\n  geom_bar(width = .07, alpha = 0.8, fill = c(viridis::magma(8, direction = -1), viridis::mako(8))) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), \n            hjust = -0.1,\n            position = position_stack(),\n            family = 'IBM Plex Sans',\n            face = 'bold') +\n  scale_x_continuous(breaks = distinct_values) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    plot.title = element_text(face = 'bold'),\n    axis.text.y = element_text(margin = margin(r = -20))) +\n  coord_flip() +\n  labs(x = 'Valence Value in Syuzhet',\n       y = 'Counts',\n       title = 'Distribution of Sentiment Values in Syuzhet Dictionary') +\n  geom_vline(xintercept = 0)\n\n\n\n\n\nR Packages: syuzhet, lexicon\nSize: 10,748 words\nBottom Line: Pending."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#vader",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#vader",
    "title": "Notes on Sentiment Dictionaries",
    "section": "VADER",
    "text": "VADER\nIs VADER evil? Maybe. But it also stands for Valence Aware Dictionary and sEntiment Reasoner.10 Impressively, the found “that VADER outperforms individual human raters” when classifying tweets as positive, accurate, or neutral.11 Part (most?) of that impressiveness is due to the ‘ER’ of VADER. Nevertheless, here I’m only considering the VAD part. If you want to check out its ruled-based sentiment reasoning, check out its github page or publication.\nThe lexicon has an impressive 7,500 entries, each with its associated polarity and intensity (-4 to 4). Did they get those intensities just winging it like Finn? Nope. Each potential entry was placed on the -4 to 4 scale by 10 Amazon Mechanical Turk workers.12 The score you do see in the dictionary means that a) raters’ scores had a standard deviation of less than 2.513 and b) that the mean rating among the 10 raters was not 0.14\nOne interesting feature of the lexicon is its inclusion of emoticons.\n\nvader <- read_delim('https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt',\n                    delim = '\\t', col_names = FALSE) %>% magrittr::set_names(c('token', 'score', 'sd', 'scores'))\n\nvader %>% \n  filter(str_detect(token, '[A-Za-z1-9]', negate = TRUE)) %>% \n  group_by(bin = score %/% 1) %>% \n  mutate(y = row_number()) %>% \n  ungroup() %>% \n  ggplot(aes(bin, y, label = token)) +\n  geom_text(check_overlap = TRUE, fontface = 'bold', family = 'IBM Plex Sans') +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(\n    x = '',\n    caption = 'To reduce the real-valued chaos, I rounded down emoticons\\' scores to the nearest integer'\n  ) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    axis.text = element_text(face = 'bold'),\n    panel.grid.minor.x = element_line(linetype = 2, color = 'grey35')\n  ) +\n  guides(color = 'none')\n\n\n\n\nI don’t want to make this dictionary seem trivial. Its creators also validated it using sentences from New York Times editorials, as well. It’s just not every day that you can make a histogram of emoticons.\n\nR Packages: As far as I know, it’s not in any. You can get it directly from its github repository.\nSide Benefit: This is probably the one that your “pythonista” friends are familiar with, since it’s in the nltk library."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#coverage-comparison-for-afinn-bing-and-nrc",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#coverage-comparison-for-afinn-bing-and-nrc",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Coverage Comparison for AFINN, BING, and NRC",
    "text": "Coverage Comparison for AFINN, BING, and NRC\n\nafinn <- get_sentiments(lexicon = 'afinn')\nbing <- get_sentiments(lexicon = 'bing')\nnrc_emotions <- filter(get_sentiments(lexicon = 'nrc'), \n                    !(sentiment %in% c('positive', 'negative')))\nnrc_polar <- filter(get_sentiments(lexicon = 'nrc'), \n                    sentiment %in% c('positive', 'negative'))\n\n\nbind_rows(\n  select(mutate(afinn, lexicon = 'AFINN'), word, lexicon),\n  select(mutate(bing, lexicon = 'BING'), word, lexicon),\n  select(mutate(nrc_polar, lexicon = 'NRC'), word, lexicon)) %>% \n  summarise(Lexica = list(lexicon), .by = word) %>% \n  ggplot(aes(Lexica)) +\n  geom_bar() +\n  scale_x_upset(n_intersections = 7) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    title = element_text(family = \"IBM Plex Sans\"),\n    plot.title = element_text(face = 'bold'),\n    plot.subtitle = element_markdown(),\n    axis.text = element_text(family = \"IBM Plex Sans\")\n  ) +\n  labs(\n    y = 'Set Size',\n    x = 'Set',\n    title = 'Are Bigger Dictionaries Mostly Supersets of Smaller Dictionaries?',\n    subtitle = \"*No*, and that's weird\"\n  )\n\n\n\n\nWe can see how many entries each dictionary has:\n\nc(nrow(afinn), nrow(bing), nrow(nrc_polar))\n\n[1] 2477 6786 5626\n\n\nHere we see a surprising amount of non-overlap. Of Bing’s 6786 terms, almost 4,000 do not appear in either of the other two dictionaries. Almost 3,000 of NRC’s polarity entries aren’t in either of the other two, as well. The third bar indicates that BING and NRC share just over 1,500 words. The short and long of this is that it might be important which dictionary we choose. They have very different coverages.15"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#still-to-do",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#still-to-do",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Still to do",
    "text": "Still to do\nThis page, like the rest of my life, is a work in progress.\n\nI still have a few dictionaries to add. In Tyler Rinker’s lexicon package there are: jockers (and jockers_rinker), emojis (and emojis_sentiment), senticnet, sentiword, slangsd. I’ve already covered all dictionaries in tidytext and textdata.\nFurther comparison of dictionary overlap. 2a. Right now I look at three dictionaries’ overlap. These three aren’t the best, they’re just the first ones I checked out. That’s not a principled criterion for selecting dictionaries. I’ll redo that section programmatically. 2b. I’m going to lemmatize/stem the dictionaries covered and get their size and overlaps as I did with AFINN, BING, and NRC above. It’s possible that sizes are much more similar once you remove a bunch of morphological tangle.\nInspired by this sentence from Nielsen, “The word list have a bias towards negative words (1598, corresponding to 65%) compared to positive words (878)” I’m also going to see what the respective positive/negative balances are of these dictionaries."
  },
  {
    "objectID": "section_immigration.html",
    "href": "section_immigration.html",
    "title": "Immigration Policy",
    "section": "",
    "text": "One of my dissertation chapters is about immigration opinion and policy. Here you’ll (soon) find assorted posts related to that project. For an overview of half of the project, you can check out my AAPOR presentation.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html",
    "href": "posts_text/2024-05-28-nsf-bic/index.html",
    "title": "An LLM Experiment",
    "section": "",
    "text": "Note to readers: I’ve published this in a beta stage so that the fine folks at Sage can read it. The prose is unpolished and the plots aren’t as aesthetic as I’d like. There is at least one mention of a technical document that I’m yet to upload, and two sections end with an unceremonious “forthcoming.” That said, I think it is very readable in its current state. Enjoy!"
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#nsf-grants-and-broader-impacts",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#nsf-grants-and-broader-impacts",
    "title": "An LLM Experiment",
    "section": "NSF Grants and Broader Impacts",
    "text": "NSF Grants and Broader Impacts\nFor the purposes of this mostly technical post, the only thing you need to know about the National Science Foundation (NSF) is something you likely already know: it’s like a science sugar daddy. You, a researcher, have this cool project you want to do. If you can convince the NSF that your project is indeed is as cool as you say it is, you’ll get an award1 to carry out this project. Now maybe for the part you don’t know. Since 1997, the NSF has used two criteria to determine grant funding: intellectual merit and broader impact.2 Respectively, these are “the potential to advance knowledge” and “the potential to benefit society and contribute to the achievement of specific, desired societal outcomes” (here). This NSF site is a bit more direct about what a broader impact proposal should be. “It answers the following question: How does your research benefit society?” (here).3\nAs with most things, you’ll get a better idea of what broader impacts are all about much quicker via examples. The NSF website gives nice examples of the types of benefits can fall into:\n\nAnd because I know you’re still curious, here are three specific broader impacts written up by NSF grantees. In 2021 the Durham Technical Community College received a grant to create an inclusive makerspace and implement support services in order to promote gender equity in technical fields and create welcoming learning environments for all students. In that case, the broader impact of the grant took center stage. In another case, a researcher at Northeastern University received a grant to develop a tool to detect code injection vulnerabilities. The researcher described the broader impact thusly, “This project involves undergraduate and graduate students in research. All software and curricula resulting from this project will be freely and publicly available; the resulting tools will be publicly disseminated and are expected to be useful for other testing and security researchers.” And sometimes the broader impact is dispatched with in just a few words, as when a grantee ended his abstract on using Deep Neural Networks to learn error-correcting codes by noting that the project “should lead to both theoretical and practical advances of importance to the communications and computer industry.”\nNow that your curiosity about broader impacts is slaked, I’ll move on.\nAs I mentioned in the opening paragraph, I was brought on as a data guy to analyze associations between different types of broader impact activities and other grant characteristics (e.g., number of publications resulting from the grant, $ize of the award, the award’s directorate, etc.). These types of broader impacts don’t come pre-labeled. They have to be coded using the unstructured text made public by the NSF (big footnote on coding if you’re interested).4 That project used the Inclusion-Immediacy Criterion (IIC) developed by Thomas Woodson rubric to code grants’ broader impacts. Inclusion and immediacy are two independent dimensions along which a grant’s BI can vary. The first dimension refers to who the primary beneficiaries of the grant are. The inclusion dimension’s possible values are {universal, advantaged, inclusive}. These labels characterize grants that, respectively, benefit everyone indiscriminately (as a public good), grants that benefit those currently in positions of power, and then those that primarily benefit marginalized groups. The second dimension, immediacy, refers the degree of interconnectedness between the main activity of the grant and its broader impact. Its three values are {intrinsic, direct, extrinsic}. As described by the NSF, “‘Broader Impacts’ may be accomplished through the research itself [inherent], through activities that are directly related to specific research projects [direct], or through activities that are directly supported by, but are complementary to, the project [extrinsic]” (there will be more explanation and examples in the prompt where I explain these categories to gpt-4o).\nIn tabular form:\n\n\n\nFrom Woodson (2022)\n\n\nBefore going on, it’s important to note that this is a difficult coding task, both in relation to the more common coding situations in which LLMs have been employed and on its own merits. The authors of the 2022 paper wrote that they did an initial round of coding, resulting in a “low” interrater reliability score.5 After discussing discrepancies and coding more data, they achieved a “moderate” degree of reliability, though it’s unclear how much of the ‘unreliability’ resulted from categories extraneous to the IIC (for details, you’ll have to read the paper). This difficulty stems from both the complexity and fuzziness of the concepts in the coding rubric and the vague descriptions given by NSF grantees writing up their projects.6 On the last point, Woodson and Boutillier wrote in a section titled “Lack of Detail,” “At times, the BIs in the PORS were vague and difficult to code. Some researchers included an aspirational statement about the potential impacts of their work without explaining the specific interventions intended to bring about those impacts.” I say this to make readers aware that even humans, which are presumed to be the “gold standard” coders, often are unsure of what the correct coding is and disagree amongst themselves. I’ll come back to this point in the conclusion, but given where the vibes are at in this cottage industry, I think it’s important to say that LLM coding implementations should be judged in relation to the human coding implementations they’re likely to supplant, not perfection."
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#comparison-with-human-coder",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#comparison-with-human-coder",
    "title": "An LLM Experiment",
    "section": "Comparison with Human Coder",
    "text": "Comparison with Human Coder\nIn the current transitional era from humans to LLMs, the main comparison of interest still isn’t AIs with themselves, but AIs with humans. That’s what we’ll look at in this section.\n\nFirst, the similarities. Both found that {direct, advantaged} was the most prevalent joint label and that the three least populated categories were {extrinsic, advantaged}, {extrinsic, inclusive}, and {instrinsic, inclusive}. That agreement is nice. They also both show high discrimination, neither uniformly assigning the same pair of labels to all broader impacts nor deterministically assigning one inclusion code to a given immediacy code, or vice versa. There is, however, a big disagreement about what the second most prevalent class is. The human researcher found almost as many {instrinsic, advantaged} as the consensus majority category, {direct, advantaged}. The LLM’s second-preferred category, however, was {instrinsic, universal}. This indicates a likely disagreement about the inclusion dimension, with the LLM seeing benefits redounding to society at large (universal) where the human sees benefits as accruing to the advantaged. To get more insight, we can look at the univariate (marginal) densities of the two dimensions:\n\nAgain, looking at the two subplots in the right column, we see that for the human, advantaged is far and away the dominant label whereas for the LLM it’s just a hair over universal. If this post were a formal attempt to develop an LLM-based coding of all NSF grants, this is sufficient cause to hit the pause button and iterate over codebook developement and prompt engineering until we reached higher levels of agreement. But there’s actually more a fundamental disagreement coming up. You might have noticed that if you were to stack all the LLM’s bars atop each other in the graph above, they’d be taller than the human coders. We can also look at the heatmaps again, but with raw counts instead of percentages:\n\nThe eagle-eyed observer will notice that, in every single cell, the LLM has more broader impacts than the human coder. That’s not a data error. It’s something much weighter. Here’s a grant-level scatterplot that’s going to blow open this whole enerprise:\n\nThe way I’d read this is to go column by column, noting the distribution of number BIs found by the LLM while holding constant the number of BIs found by the human. So, for example, when the human found 0, the modal number found by the LLM was 1. When the human found 1, the modal number found by the LLM was also 1, but the LLM found more than 1 in the majority of grants. Moving further rightward, the grey dots along the diagnonal represent agreement about number of BIs present in a POR, while purple dots are cases where the LLM found more, and pink dots are cases where the human found more.\nI’d argue that this issue, the radical and prevalent disagreement about the number of BIs present in a text, is at least as fundamental as disagreement as to the labeling of BIs along relevant dimensions. Coincidentally, in a project of my own where I’m coding immigration laws, this exact same issue cropped up but with only human RAs. Large laws, especially annual or biennial budgets, potentially contain many provisions that affect immigrants. Sometimes it’s clear where one legislative goal ends and another begins. Almost as frequently, however, two equally reasonable and motivated researchers could disagree about whether two sections are serving the same goal and thus should be counted together, or whether the sections of the bill are indepdent policies. I bring this different project up to point out that the issue of ‘relevant entity’ discretization might be a uniquely difficult task generally.\nAt this point the question of interrater reliability is moot. I’m going to go ahead and calculate it, but the more interesting remainder of this post lies in comparing the LLM output with itself, which I’ll get to shortly.\nSo, there are a different ways to think about IRR in this application. I’ll go with the way the Woodson and Boutillier calcuated it in their paper where each grant is hanging out in nine-dimensional space, occupying either a 0 or a 1 on each. That is, each grant is dummy coded along nine dimensions where each dimension is a combination like, {advantaged-intrinsic}, {inclusive-extrinsic}, or {universal-direct}. This means transforming the data into the following format for all nine joint codings:\n\n\n           [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nllm_detect    1    0    0    1    1    0    0    1    0     0     1     0\nra_detect     1    1    0    0    1    1    0    1    0     0     1     0\n\n\nEven without know what the specific formula is for calculating IRR, your prior should be that the IRR shouldn’t be very high since the LLM detected so many more BIs than the human. Well, get ready to update downward, because it’s even worse than that:\n\nAs you can see, none of the nine are in the neighborhood of an adequate \\(\\alpha\\). Two of the IRR values are almost precisely zero, and one is slightly negative. If this were to happen to you as you were trying to get an LLM to code some text for an important project, it would definitely be a moment to close your laptop and taste test a few Belgian quadrupels before trying to rectify the situation.\nMy suspicion that a large proprotion of this IRR disappointment comes from the 80 broader impacts found by the LLM when the human found none (and to a lesser extent all the other purple points above the diagonal). In this project’s next phase, diagnosing the source of this disagreement will be the top priority. After reaching a provisional agreement about what caused the low reliability between the human and the LLM, the next step will be to modify the prompt. Though exactly how the prompt will need to be modified will depend on the diagnosis, one thing is certain. The prompt will have to mention the possibility that there is no broader impact. It will also likely have something to the effect of, “A project outcome report should usually one or two broader impacts. At most, it can have four. If you believe you have found more, please consolidate them into each other until there are three or four.”"
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#opening-statement-experiment",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#opening-statement-experiment",
    "title": "An LLM Experiment",
    "section": "Opening Statement Experiment",
    "text": "Opening Statement Experiment\nIn addition to comparing the performance of the LLM against the human coder, we can also compare it against itself. Until now, I’ve kept mum about a few details of the experiment. For each grant, I didn’t only prompt GPT once, I prompted it three times, each time changing the first sentence. Here are the three openings, the first of which you read above.\n\n\n“You are a sharp, conscientious, and unflagging researcher who skilled at evaluating scientific grants across all areas of study.”\n\n“You are an intelligent, meticulous, and tireless researcher with expertise in reviewing scientific grants across various fields.”\n\n“You are a bright, punctilious, and indefatigable researcher who is an expert at reading scientific grants across all disciplines.”\n\nNow, if you’re not familiar with prompting lore, you’re probably wondering, Why would he do that?10 This was motivated by previous findings that seemingly insignificant changes to prompts can produce surprisingly different outputs. So, my thought was, why not try out different versions of the opening sentences that are synonmous to English speakers, but might set gpt-4o down different paths and generate weird results.\nFirst, let’s see if the different openings led to different numbers of grants found:\n\nI’m going to say no, not really. And also, no – you’re not seeing double. For each opening there are two dots. You’ll have to read to the end to find out why I have two estimates for each LLM identity. For now, don’t sweat it, really.\nWe can also see if different first sentences led to different label distributions:\n\nI’ve chosen to go with the barplot of marginal distributions instead of the heatmaps of joint distributions, but the takeaway would be the same either way: the three openings led to the same distributions in both variables.11\nIf I were doing this over, I’d probably choose less synonymous opening sentences. Though it’s important that each of the three roles is associated with competence in the task at hand, I really stacked the deck against myself by making these openings so similar. For example, instead of the LLM’s identity always being that of a researcher, the LLM could have also been a journalist or a well-informed reader of popular science. Or, instead of being a generic researcher, the LLM might have been a sociologist, a historian, or even a political scientist. Hindsight is 20/20."
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#order-experiment",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#order-experiment",
    "title": "An LLM Experiment",
    "section": "Order Experiment",
    "text": "Order Experiment\nSimultaneously with the AI identity experiment described above, I carried out an experiment wherein I shuffled around the order in which the three values of the two dimensions were described in the prompt. For example, in the prompt as presented above, the order of the inclusion values was universal, then advantaged, and finally inclusive. But that order only happened around 1/6 of the time. The other approximately 5/6 of the time, the ordering was one of the other five permutations of those three values.12 While this randomization of inclusion’s values was happening, the exact same thing was happening with immediacy’s three values. This results in a bunch of conditions, 13 but I was most interested in seeing if there were analogous effects to the primacy and recency effects observed in humans, which means just looking at the likelihood of a grant being assigned a label if that label was mentioned first, second, or third within its category. The fact that there’s a ton of other randomization whirring about actually makes these results more robust, pace the misguided notion that it’s better to only randomize one factor at a time.\nBefore peeking at the results, you should ask yourself if you think order will matter. Some relevant information to inform your priors: the mechanism behind the order effects observed in humans almost certainly isn’t relevant here. Moreover, I’m not sure even humans would experience either primacy or recency effects when the bits being ordered are a) this small and b) only three in number. But what about order effects in LLMs? In a different context (one where the text provided to an LLM was extremely long), AI reseachers have found order effects. Specifically, they found the canonical order effect of primacy and recency bias as they’re observed in humans. But again, these texts are extremely small, making the current situation substantively different from that one. In any case, it was trivial to program this randomization, so I went for it.\n\nAs I mentioned, you can think of these points as conditional probabilities. If we look at the first group of three points (the pink ones under Advantaged), from left to right they are the probability of a broader impact’s inclusion dimension being classified as advantaged if advantaged is mentioned first, then second, then third. It’s the same logic for the other five sets of three points. Each reader should draw their own conclusions, but my tentative takeaway is that the differences across order aren’t large enough to justify worry that order matters.14 I suppose there’s a (second-order) worry that we’re even (first-order) worried about this. We wouldn’t be worried that, for example, human coders would be vulnerable to these small differences in order. Since it’s always going to be an empirical question if a particular LLM is subject to order effects in a particular application, I hope that the eventual best practice when using LLMs to code text will be to come up with small prompt perturbations and then show that they indeed don’t matter. Conceptually, it’s the exact same robustness check that’s best practice in regression modeling where you run various defensible models of a phenomenon and declare yout results robust to the extent that they replicate across models. Unfortunately, since I’m not seeing this done, it seems like we’re heading down a path where researchers use a single prompt that has been shown to be acceptable enough against some criteria. But it’s pretty clear that using only one prompt will lead to underestimates of uncertainty, both in the technical sense of undercoverage of standard errors and in the “wake up you in cold sweat” sense of everything hinging on seemingly irrelevant prompt choices."
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#irr",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#irr",
    "title": "An LLM Experiment",
    "section": "IRR",
    "text": "IRR\nJust as I calculated the interrater reliability between one run of the LLM and the human RA, I can also calculated the IRR between different versions of the LLM. Because I sent each grant to be coded various times, I can do a couple different versions of this. The main question I can answer is if the different openings led to different codings. We saw that the different openings led to similar distributions of codings, but that’s a different question from whether or not they coded similarly at the POR level. And remember, the difference between the LLM’s and the human’s codings didn’t seem so large when looking at the distributions, but oh boy were they big when we calculated IRR.\nI should at this point mention there’s actually a third and final moving part of this experiment I’ve yet to apprise you of, dear reader. In addition to the two ‘experiments’ above, I took advantage of a feature of OpenAI’s chat-completions API that people seem to be sleeping on. For any prompt you send to this endpoint, you can request n responses (“choices”). If I understand the sparse documentation correctly, these are independent draws from the models response space given a certain prompt. To be clear, these messages (choices) are not sequential messages. Rather, they’re n different forking paths the LLM could have gone down given a certain prompt.15 This lets researchers get a sense of uncertainty of a response.16 We can then compare the resulting IRR when the LLM was given the same prompt with the IRR when the prompt is slighly modified (e.g., when the prompt began with a different opening sentence). In this particular case, we’ll have six (!) IRR estimates: 3 “within” IRRs for each opening and then three “between” IRRs from the pairwise comparisons (A and B, A and C, B and C). Excited? Well, …\nResults Forthcoming"
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#technical-documentation",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#technical-documentation",
    "title": "An LLM Experiment",
    "section": "Technical Documentation",
    "text": "Technical Documentation\nForthcoming"
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#abs-por",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#abs-por",
    "title": "An LLM Experiment",
    "section": "Abstract v. Project Outcome Report",
    "text": "Abstract v. Project Outcome Report\n((This is more relevant to an earlier version of this post, but I’m keeping it here for future reference.))\nIn the interest of completeness, here’s a side-by-side comparison of an award’s abstract and project outcome report."
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#abstract",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#abstract",
    "title": "An LLM Experiment",
    "section": "Abstract",
    "text": "Abstract\nThis grant funds the National Research Experiences for Undergraduates Program (NREUP), administered by the Mathematical Association of America (MAA), headquartered in Washington, D.C., and held at various locations throughout the United States. The program is designed to provide undergraduates from underrepresented groups majoring in mathematics or a closely related field with challenging research opportunities and will offer these experiences during the summers of 2014, 2015 and 2016. The length of the experiences can vary from location to location but will be a minimum of six weeks. The program will reach minority students at a critical point in their career, midway through their undergraduate program through an undergraduate faculty member with whom they have a strong connection.\nNREUP will continue its track record of increasing minority students’ interest in obtaining advanced degrees in mathematics. NREUP will invite mathematical sciences faculty to host MAA Student Research Programs on their own campuses. The MAA will select the best from competing proposals. NREUP provides key components to encourage students to pursue graduate studies and careers in mathematics: enriching and rewarding mathematical experiences, mentoring by active researchers, and intellectual networking with peers. It also provides an annual full day program to assist the new REU director with management details of the program and the potential REU director with further refinement of his/her concept and proposal writing skills. It also assists attendees in expanding their knowledge of federal and non-federal funding sources and increasing their knowledge of and skills in student mentoring. Moreover, by supporting faculty at a diverse group of institutions to direct summer research experiences, this project supports not only undergraduate students but also the development of a community of skilled faculty mentors who are expected to provide ever-increasing opportunities for undergraduate research. This project is jointly supported by the Workforce and Infrastructure programs within the Division of Mathematical Sciences"
  },
  {
    "objectID": "posts_text/2024-05-28-nsf-bic/index.html#por",
    "href": "posts_text/2024-05-28-nsf-bic/index.html#por",
    "title": "An LLM Experiment",
    "section": "POR",
    "text": "POR\nNational Research Experience for Undergraduates Program (NREUP) of the Mathematical Association of America (MAA) provided challenging research experiences to undergraduates to increase their interest in obtaining advanced degrees in mathematics during the summers of 2014, 2015 and 2016. The students were from underrepresented minority groups and were majoring in mathematics or a closely related field. NREUP served a total of 130 undergraduates. Of these student participants 46.92% (61) were female, 53.08% (69) were African American, 38.46% (50) were Hispanic or Latino and 2.31% (3) were Native Americans. This project also received supplemental funding from the National Security Agency in 2015.\nNREUP sites were located in states across the nation, including Arkansas, California, the District of Columbia, Indiana, Michigan, North Carolina, New Jersey, New York, Pennsylvania, Puerto Rico, Texas and Virginia. A typical NREUP site consisted of one or two faculty members and four to six students who worked together on a research project for a minimum of six weeks. Funding was provided for faculty and student stipends, transportation, room and board, and supplies. Research areas included financial mathematics, graph theory, game theory, mathematical economics, mathematical modeling, and partial differential equations. Some research was of a theoretical nature and some was done in the context of a real-world problem such as disease modeling, forensic science and hurricane evacuation.\nEach student prepared a PowerPoint presentation or a written paper for submission to a undergraduate research journal. All gave on-campus presentations on their research at the end of the summer, and many presented posters and papers at regional and national mathematics conferences. Some continued working on their research during the following academic year.\nProject evaluation focused on student attitudes and pursuit of graduate degrees. Data confirmed that many students gained confidence in their ability to do mathematical research and succeed at the graduate level."
  }
]