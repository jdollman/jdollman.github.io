[
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "",
    "text": "A while back I was brought on as part of a project in which the principal investigator was looking at the broader impacts of NSF grants. I’ll go into a little more detail below, but broader impacts are basically a researcher’s statement of how their research benefits society generally. A prior research assistant had manually coded 400 broader impacts along two dimensions: who are the project’s primary beneficiaries and to what degree is the project’s broader impact connected to its intellectual merit, the core scientific part of the endeavor. By the time I was brought onto the project, the task was to figure out if certain types of broader impact strategies were associated with more or fewer academic publications. Given the complexities of the models I wanted to run (zero-inflated this-and-that, controlling for various institutional factors), the \\(N = 400\\) sample was pretty meager. There were some marginally significant parameters, but most estimates were very imprecise, meaning we oftentimes couldn’t say whether a given association was positive or negative, much less if it was large or not meaningfully different from zero.\nThat was three years ago. Since then, using LLMs to label text has started to gain acceptance as an alternative to the slow and costly procedure of having humans do it. This post is my initial foray into seeing if I can go from \\(N = 400\\) to \\(N \\rightarrow\\infty\\), which means first seeing how seeing how an LLM behaves when it codes the same 400 NSF grants already coded by a (human) researcher. Different readers will find different parts of the post useful. That said, I imagine it will be most useful for a social scientist looking to code data herself. Even if the topic of NSF grants and broader impacts per se might not be of interest, the steps I go through below address the issues anyone labelling data will also have to address.\nLastly, this post is implicitly a “Part 1” of an \\(n\\)-part series on this project.\n\n\nFor the purposes of this mostly technical post, the only thing you need to know about the National Science Foundation (NSF) is something you likely already know: it’s like a science Patronus Munificus Dulcedinis.1 You, a researcher, have this cool project you want to do. If you can convince the NSF that your project is indeed is as cool as you say it is, you’ll get an award2 to carry out this project. Now maybe for the part you don’t know. Since 1997, the NSF has used two criteria to determine grant funding: intellectual merit and broader impact.3 Respectively, these are “the potential to advance knowledge” and “the potential to benefit society and contribute to the achievement of specific, desired societal outcomes” (here). This NSF site is a bit more direct about what a broader impact proposal should be. “It answers the following question: How does your research benefit society?” (here).\nAs with most things, you’ll get a better idea of what broader impacts are all about much quicker via examples. The NSF website gives nice examples of the types of benefits can fall into:\n\nAnd because I know you’re still curious, here are three specific broader impacts written up by NSF grantees. In 2021 the Durham Technical Community College received a grant to create an inclusive makerspace and implement support services in order to promote gender equity in technical fields and create welcoming learning environments for all students. In that case, the broader impact of the grant took center stage. In another case, a researcher at Northeastern University received a grant to develop a tool to detect code injection vulnerabilities. The researcher described the broader impact thusly, “This project involves undergraduate and graduate students in research. All software and curricula resulting from this project will be freely and publicly available; the resulting tools will be publicly disseminated and are expected to be useful for other testing and security researchers.” And sometimes the broader impact is dispatched with in just a few words, as when a grantee ended his abstract on using Deep Neural Networks to learn error-correcting codes by noting that the project “should lead to both theoretical and practical advances of importance to the communications and computer industry.”\nNow that your curiosity about broader impacts is slaked, I’ll move on.\nAs I mentioned in the opening paragraph, I was brought on as a data guy to analyze associations between different types of broader impact activities and other grant characteristics (e.g., number of publications resulting from the grant, $ize of the award, the award’s directorate, etc.). These types of broader impacts don’t come pre-labeled. They have to be coded using the unstructured text made public by the NSF (big footnote on coding if you’re interested).4 That project used the Inclusion-Immediacy Criterion (IIC) developed by Thomas Woodson rubric to code grants’ broader impacts. Inclusion and immediacy are two independent dimensions along which a grant’s BI can vary. The first dimension refers to who the primary beneficiaries of the grant are. The inclusion dimension’s possible values are {universal, advantaged, inclusive}. These labels characterize grants that, respectively, benefit everyone indiscriminately (as a public good), grants that benefit those currently in positions of power, and then those that primarily benefit marginalized groups. The second dimension, immediacy, refers the degree of interconnectedness between the main activity of the grant and its broader impact. Its three values are {intrinsic, direct, extrinsic}. As described by the NSF, “‘Broader Impacts’ may be accomplished through the research itself [inherent], through activities that are directly related to specific research projects [direct], or through activities that are directly supported by, but are complementary to, the project [extrinsic]” (there will be more explanation and examples in the prompt where I explain these categories to gpt-4o).\nIn tabular form:\n\n\n\nFrom Woodson (2022)\n\n\nBefore going on, it’s important to note that this is a difficult coding task, both in relation to the more common coding situations in which LLMs have been employed and on its own merits. The authors of the 2022 paper wrote that they did an initial round of coding, resulting in a “low” interrater reliability score.5 After discussing discrepancies and coding more data, they achieved a “moderate” degree of reliability, though it’s unclear how much of the ‘unreliability’ resulted from categories extraneous to the IIC (for details, you’ll have to read the paper). This difficulty stems from both the complexity and fuzziness of the concepts in the coding rubric and the vague descriptions given by NSF grantees writing up their projects.6 On the last point, Woodson and Boutillier wrote in a section titled “Lack of Detail,” “At times, the BIs in the PORS were vague and difficult to code. Some researchers included an aspirational statement about the potential impacts of their work without explaining the specific interventions intended to bring about those impacts.” I say this to make readers aware that even humans, which are presumed to be the “gold standard” coders, often are unsure of what the correct coding is and disagree amongst themselves. I’ll come back to this point in the conclusion, but given where the vibes are at in this cottage industry, I think it’s important to say that LLM coding implementations should be judged in relation to the human coding implementations they’re likely to supplant, not perfection."
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#comparison-with-human-coder",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#comparison-with-human-coder",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "Comparison with Human Coder",
    "text": "Comparison with Human Coder\nIn the current transitional era from humans to LLMs, the main comparison of interest still isn’t AIs with themselves, but AIs with humans. That’s what we’ll look at in this section.\n\nFirst, the similarities. Both found that {direct, advantaged} was the most prevalent joint label and that the three least populated categories were {extrinsic, advantaged}, {extrinsic, inclusive}, and {instrinsic, inclusive}. That agreement is nice. They also both show high discrimination, neither uniformly assigning the same pair of labels to all broader impacts nor deterministically assigning one inclusion code to a given immediacy code, or vice versa. There is, however, a big disagreement about what the second most prevalent class is. The human researcher found almost as many {instrinsic, advantaged} as the consensus majority category, {direct, advantaged}. The LLM’s second-preferred category, however, was {instrinsic, universal}. This indicates a likely disagreement about the inclusion dimension, with the LLM seeing benefits redounding to society at large (universal) where the human sees benefits as accruing to the advantaged. To get more insight, we can look at the univariate (marginal) densities of the two dimensions:\n\nAgain, looking at the two subplots in the right column, we see that for the human, advantaged is far and away the dominant label whereas for the LLM it’s just a hair over universal. If this post were a formal attempt to develop an LLM-based coding of all NSF grants, this is sufficient cause to hit the pause button and iterate over codebook developement and prompt engineering until we reached higher levels of agreement. But there’s actually more a fundamental disagreement coming up. You might have noticed that if you were to stack all the LLM’s bars atop each other in the graph above, they’d be taller than the human coders. We can also look at the heatmaps again, but with raw counts instead of percentages:\n\nThe eagle-eyed observer will notice that, in every single cell, the LLM has more broader impacts than the human coder. That’s not a data error. It’s something much weighter. Here’s a grant-level scatterplot that’s going to blow open this whole enerprise:\n\nThe way I’d read this is to go column by column, noting the distribution of number BIs found by the LLM while holding constant the number of BIs found by the human. So, for example, when the human found 0, the modal number found by the LLM was 1. When the human found 1, the modal number found by the LLM was also 1, but the LLM found more than 1 in the majority of grants. Moving further rightward, the grey dots along the diagnonal represent agreement about number of BIs present in a POR, while purple dots are cases where the LLM found more, and pink dots are cases where the human found more.\nI’d argue that this issue, the radical and prevalent disagreement about the number of BIs present in a text, is at least as fundamental as disagreement as to the labeling of BIs along relevant dimensions. Coincidentally, in a project of my own where I’m coding immigration laws, this exact same issue cropped up but with only human RAs. Large laws, especially annual or biennial budgets, potentially contain many provisions that affect immigrants. Sometimes it’s clear where one legislative goal ends and another begins. Almost as frequently, however, two equally reasonable and motivated researchers could disagree about whether two sections are serving the same goal and thus should be counted together, or whether the sections of the bill are indepdent policies. I bring this different project up to point out that the issue of ‘relevant entity’ discretization might be a uniquely difficult task generally.\nAt this point the question of interrater reliability is moot. I’m going to go ahead and calculate it, but the more interesting remainder of this post lies in comparing the LLM output with itself, which I’ll get to shortly.\nSo, there are a different ways to think about IRR in this application. I’ll go with the way the Woodson and Boutillier calcuated it in their paper where each grant is hanging out in nine-dimensional space, occupying either a 0 or a 1 on each. That is, each grant is dummy coded along nine dimensions where each dimension is a combination like, {advantaged-intrinsic}, {inclusive-extrinsic}, or {universal-direct}. This means transforming the data into the following format for all nine joint codings:\n\n\n           [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nllm_detect    1    0    0    1    1    0    0    1    0     0     1     0\nra_detect     1    1    0    0    1    1    0    1    0     0     1     0\n\n\nEven without know what the specific formula is for calculating IRR, your prior should be that the IRR shouldn’t be very high since the LLM detected so many more BIs than the human. Well, get ready to update downward, because it’s even worse than that:\n\nAs you can see, none of the nine are in the neighborhood of an adequate \\(\\alpha\\). Two of the IRR values are almost precisely zero, and one is slightly negative. If this were to happen to you as you were trying to get an LLM to code some text for an important project, it would definitely be a moment to close your laptop and taste test a few Belgian quadrupels before trying to rectify the situation.\nMy suspicion that a large proprotion of this IRR disappointment comes from the 80 broader impacts found by the LLM when the human found none (and to a lesser extent all the other purple points above the diagonal). In this project’s next phase, diagnosing the source of this disagreement will be the top priority. After reaching a provisional agreement about what caused the low reliability between the human and the LLM, the next step will be to modify the prompt. Though exactly how the prompt will need to be modified will depend on the diagnosis, one thing is certain. The prompt will have to mention the possibility that there is no broader impact. It will also likely have something to the effect of, “A project outcome report should usually one or two broader impacts. At most, it can have four. If you believe you have found more, please consolidate them into each other until there are three or four.”"
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#opening-statement-experiment",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#opening-statement-experiment",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "Opening Statement Experiment",
    "text": "Opening Statement Experiment\nIn addition to comparing the performance of the LLM against the human coder, we can also compare it against itself. Until now, I’ve kept mum about a few details of the experiment. For each grant, I didn’t only prompt GPT once, I prompted it three times, each time changing the first sentence. Here are the three openings, the first of which you read above.\n\n\n“You are a sharp, conscientious, and unflagging researcher who skilled at evaluating scientific grants across all areas of study.”\n\n“You are an intelligent, meticulous, and tireless researcher with expertise in reviewing scientific grants across various fields.”\n\n“You are a bright, punctilious, and indefatigable researcher who is an expert at reading scientific grants across all disciplines.”\n\nNow, if you’re not familiar with prompting lore, you’re probably wondering, Why would he do that?15 This was motivated by previous findings that seemingly insignificant changes to prompts can produce surprisingly different outputs. So, my thought was, why not try out different versions of the opening sentences that are synonmous to English speakers, but might set gpt-4o down different paths and generate weird results.\nFirst, let’s see if the different openings led to different numbers of grants found:\n\nI’m going to say no, not really. And also, no – you’re not seeing double. For each opening there are two dots. You’ll have to read to the end to find out why I have two estimates for each LLM identity. For now, don’t sweat it, really.\nWe can also see if different first sentences led to different label distributions:\n\nI’ve chosen to go with the barplot of marginal distributions instead of the heatmaps of joint distributions, but the takeaway would be the same either way: the three openings led to the same distributions in both variables.16\nIf I were doing this over, I’d probably choose less synonymous opening sentences. Though it’s important that each of the three roles is associated with competence in the task at hand, I really stacked the deck against myself by making these openings so similar. For example, instead of the LLM’s identity always being that of a researcher, the LLM could have also been a journalist or a well-informed reader of popular science. Or, instead of being a generic researcher, the LLM might have been a sociologist, a historian, or even a political scientist. Hindsight is 20/20."
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#order-experiment",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#order-experiment",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "Order Experiment",
    "text": "Order Experiment\nSimultaneously with the AI identity experiment described above, I carried out an experiment wherein I shuffled around the order in which the three values of the two dimensions were described in the prompt. For example, in the prompt as presented above, the order of the inclusion values was universal, then advantaged, and finally inclusive. But that order only happened around 1/6 of the time. The other approximately 5/6 of the time, the ordering was one of the other five permutations of those three values.17 While this randomization of inclusion’s values was happening, the exact same thing was happening with immediacy’s three values. This results in a bunch of conditions, 18 but I was most interested in seeing if there were analogous effects to the primacy and recency effects observed in humans, which means just looking at the likelihood of a grant being assigned a label if that label was mentioned first, second, or third within its category. The fact that there’s a ton of other randomization whirring about actually makes these results more robust, pace the misguided notion that it’s better to only randomize one factor at a time.\nBefore peeking at the results, you should ask yourself if you think order will matter. Some relevant information to inform your priors: the mechanism behind the order effects observed in humans almost certainly isn’t relevant here. Moreover, I’m not sure even humans would experience either primacy or recency effects when the bits being ordered are a) this small and b) only three in number. But what about order effects in LLMs? In a different context (one where the text provided to an LLM was extremely long), AI reseachers have found order effects. Specifically, they found the canonical order effect of primacy and recency bias as they’re observed in humans. But again, these texts are extremely small, making the current situation substantively different from that one. In any case, it was trivial to program this randomization, so I went for it.\n\nAs I mentioned, you can think of these points as conditional probabilities. If we look at the first group of three points (the pink ones under Advantaged), from left to right they are the probability of a broader impact’s inclusion dimension being classified as advantaged if advantaged is mentioned first, then second, then third. It’s the same logic for the other five sets of three points. Each reader should draw their own conclusions, but my tentative takeaway is that the differences across order aren’t large enough to justify worry that order matters.19 I suppose there’s a (second-order) worry that we’re even (first-order) worried about this. We wouldn’t be worried that, for example, human coders would be vulnerable to these small differences in order. Since it’s always going to be an empirical question if a particular LLM is subject to order effects in a particular application, I hope that the eventual best practice when using LLMs to code text will be to come up with small prompt perturbations and then show that they indeed don’t matter. Conceptually, it’s the exact same robustness check that’s best practice in regression modeling where you run various defensible models of a phenomenon and declare yout results robust to the extent that they replicate across models. Unfortunately, since I’m not seeing this done, it seems like we’re heading down a path where researchers use a single prompt that has been shown to be acceptable enough against some criteria. But it’s pretty clear that using only one prompt will lead to underestimates of uncertainty, both in the technical sense of undercoverage of standard errors and in the “wake up you in cold sweat” sense of everything hinging on seemingly irrelevant prompt choices."
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#irr",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#irr",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "IRR",
    "text": "IRR\nJust as I calculated the interrater reliability between one run of the LLM and the human RA, I can also calculated the IRR between different versions of the LLM. Because I sent each grant to be coded various times, I can do a couple different versions of this. The main question I can answer is if the different openings led to different codings. We saw that the different openings led to similar distributions of codings, but that’s a different question from whether or not they coded similarly at the POR level. And remember, the difference between the LLM’s and the human’s codings didn’t seem so large when looking at the distributions, but oh boy were they big when we calculated IRR.\nI should at this point mention there’s actually a third and final moving part of this experiment I’ve yet to apprise you of, dear reader. In addition to the two ‘experiments’ above, I took advantage of a feature of OpenAI’s chat-completions API that people seem to be sleeping on. For any prompt you send to this endpoint, you can request n responses (“choices”). If I understand the sparse documentation correctly, these are independent draws from the models response space given a certain prompt. To be clear, these messages (choices) are not sequential messages. Rather, they’re n different forking paths the LLM could have gone down given a certain prompt.20 This lets researchers get a sense of uncertainty of a response.21 We can then compare the resulting IRR when the LLM was given the same prompt with the IRR when the prompt is slighly modified (e.g., when the prompt began with a different opening sentence). In this particular case, we’ll have six (!) IRR estimates: 3 “within” IRRs for each opening and then three “between” IRRs from the pairwise comparisons (A and B, A and C, B and C). Excited? Well, …\nResults Forthcoming"
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#technical-documentation",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#technical-documentation",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "Technical Documentation",
    "text": "Technical Documentation\nAll materials for replication can be found here."
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#abs-por",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#abs-por",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "Abstract v. Project Outcome Report",
    "text": "Abstract v. Project Outcome Report\n((This is more relevant to an earlier version of this post, but I’m keeping it here for future reference.))\nIn the interest of completeness, here’s a side-by-side comparison of an award’s abstract and project outcome report."
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#abstract",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#abstract",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "Abstract",
    "text": "Abstract\nThis grant funds the National Research Experiences for Undergraduates Program (NREUP), administered by the Mathematical Association of America (MAA), headquartered in Washington, D.C., and held at various locations throughout the United States. The program is designed to provide undergraduates from underrepresented groups majoring in mathematics or a closely related field with challenging research opportunities and will offer these experiences during the summers of 2014, 2015 and 2016. The length of the experiences can vary from location to location but will be a minimum of six weeks. The program will reach minority students at a critical point in their career, midway through their undergraduate program through an undergraduate faculty member with whom they have a strong connection.\nNREUP will continue its track record of increasing minority students’ interest in obtaining advanced degrees in mathematics. NREUP will invite mathematical sciences faculty to host MAA Student Research Programs on their own campuses. The MAA will select the best from competing proposals. NREUP provides key components to encourage students to pursue graduate studies and careers in mathematics: enriching and rewarding mathematical experiences, mentoring by active researchers, and intellectual networking with peers. It also provides an annual full day program to assist the new REU director with management details of the program and the potential REU director with further refinement of his/her concept and proposal writing skills. It also assists attendees in expanding their knowledge of federal and non-federal funding sources and increasing their knowledge of and skills in student mentoring. Moreover, by supporting faculty at a diverse group of institutions to direct summer research experiences, this project supports not only undergraduate students but also the development of a community of skilled faculty mentors who are expected to provide ever-increasing opportunities for undergraduate research. This project is jointly supported by the Workforce and Infrastructure programs within the Division of Mathematical Sciences"
  },
  {
    "objectID": "posts_assorted/2024-05-28-nsf-bic/index.html#por",
    "href": "posts_assorted/2024-05-28-nsf-bic/index.html#por",
    "title": "Using ChatGPT to Code NSF Grants",
    "section": "POR",
    "text": "POR\nNational Research Experience for Undergraduates Program (NREUP) of the Mathematical Association of America (MAA) provided challenging research experiences to undergraduates to increase their interest in obtaining advanced degrees in mathematics during the summers of 2014, 2015 and 2016. The students were from underrepresented minority groups and were majoring in mathematics or a closely related field. NREUP served a total of 130 undergraduates. Of these student participants 46.92% (61) were female, 53.08% (69) were African American, 38.46% (50) were Hispanic or Latino and 2.31% (3) were Native Americans. This project also received supplemental funding from the National Security Agency in 2015.\nNREUP sites were located in states across the nation, including Arkansas, California, the District of Columbia, Indiana, Michigan, North Carolina, New Jersey, New York, Pennsylvania, Puerto Rico, Texas and Virginia. A typical NREUP site consisted of one or two faculty members and four to six students who worked together on a research project for a minimum of six weeks. Funding was provided for faculty and student stipends, transportation, room and board, and supplies. Research areas included financial mathematics, graph theory, game theory, mathematical economics, mathematical modeling, and partial differential equations. Some research was of a theoretical nature and some was done in the context of a real-world problem such as disease modeling, forensic science and hurricane evacuation.\nEach student prepared a PowerPoint presentation or a written paper for submission to a undergraduate research journal. All gave on-campus presentations on their research at the end of the summer, and many presented posters and papers at regional and national mathematics conferences. Some continued working on their research during the following academic year.\nProject evaluation focused on student attitudes and pursuit of graduate degrees. Data confirmed that many students gained confidence in their ability to do mathematical research and succeed at the graduate level."
  },
  {
    "objectID": "posts_assorted/2024-01-24-klein-text/index.html",
    "href": "posts_assorted/2024-01-24-klein-text/index.html",
    "title": "The Ezra Klein Show",
    "section": "",
    "text": "For reasons lost to oblivion, I decided to analyze transcripts of Ezra Klein’s podcasts using elementary text analysis. The following was the result."
  },
  {
    "objectID": "posts_assorted/2024-01-24-klein-text/index.html#interviewing-style",
    "href": "posts_assorted/2024-01-24-klein-text/index.html#interviewing-style",
    "title": "The Ezra Klein Show",
    "section": "Interviewing Style",
    "text": "Interviewing Style\nBefore even looking at the content of the episodes, before looking at what words are said, sentiments are expressed and topics are covered, we can look at volume of speaking.\n  \nWe can also compare this distribution of how much Ezra speaks as interviewer to how much other interviewers speak. After all, it’s hard to really feel what the numbers in the graph above mean without comparing them to the relative talkativeness of other interviewers. Below is a comparison of Ezra with two other interviewers, both of whom have had Ezra on as a guest on their programs: Rob Wiblin (80,000 Hours) and Tyler Cowen (Conversations with Tyler).\n\nThis is a little more interesting, I think.\nMy hypothesis before crunching these data was that Tyler was speak the least (and it wouldn’t be close), then Rob, then Ezra. And that turned out to be right.1 After all, Ezra speaks quiet a bit because people tune into Ezra’s show in order to listen to Ezra. Though Tyler’s show does have “Tyler” in the name, he frequently announces that his interviews are the interviews that he wants, not what the listeners might want. And his listener-be-damned approach often results in quick questions and abrupt topic changes. The 80,000 Hours podcast has this intermediate position where Rob often positions himself as a listener might and restates what the speaker said, gives the conventional wisdom against which the interviewee can react, and ask follow-up questions."
  },
  {
    "objectID": "posts_assorted/2024-01-24-klein-text/index.html#ezras-words",
    "href": "posts_assorted/2024-01-24-klein-text/index.html#ezras-words",
    "title": "The Ezra Klein Show",
    "section": "Ezra’s Words",
    "text": "Ezra’s Words\nAlright, so what does this ingenious interviewer from Irvine actually talk about? Among the ways of answering that question, one is to just look at the words he uses. The simplest way of operationalizing ‘words used’ is to look at “unigrams,” which result from splitting the unstructured text at every whitespace2, making each whitespace-separated unit into a row in a dataframe, and counting how many rows each token (word) appears in that resulting dataframe. Ezra’s result is below.3\n\n \nBut you can also get slighly more sophisticated and look at contiguous words. So, instead of splitting at every whitespace, you split at every second or third whitespace. The nice thing about that is you start to see phrases, turns of speech. For example, if you were curious why “people,” “lot,” and” bit” appear in the unigrams, the bottom-most panel will give you your answer.\n  \nAmong the bigrams and trigrams, an Ezra Klein Show fan will also be pleased to see Ezra’s pat phrases for parts of his podcast: “final question”, “books you’d recommend,” and “email [is] ezrakleinshow@nytimes.com.” I’m not sure if seeing these phrases is interesting, but they are nice proofs of concept for the method. You also see that the bigrams are dominated by pairs of words that, yes, do have a space between them, but they are lexically one thing. I’d argue that “past couple” and “minute ago” are only instances of more-or-less independent words coming together.\nNow let’s try something potentially more interesting. We can also look at how he uses words differentially. Why would that be useful? Well, just looking at the words he uses might tell us that he’s a guy with a show that covers topics X, Y, and Z that use the words associated with those topics.4 One way we can “control for” topic is to contrast Ezra’s words with those of his guest. Abstractly, the idea is that if person A and B are talking about subject S, the words that differentiate A and B are not going to be those that generically describe subject S. Rather, they will be the words idiosyncratically associated with each interlocutor.\n\nThis surprised me. Sure, one would expect most words to be along the diagonal line which indicates that the word is used in equal proportions by both Ezra and his guests, but I really expected this to show more unique usages in the off-diagonal space.\nWe can also compare the words of the Ezra Klein Show to the words used in introductory political science textbooks. The idea of such an analysis would be to ask, “When Ezra talks politics, which topics (proxied by words) does he dedicate space to than an introduction to politics? Which topics does he spend less time one?” From these results one could hope to find out something about which aspects of politics Ezra finds interesting and important, and which less so. Unfortunately, I carried out the analysis and the result is below:\n \n I call this ‘analysis’ “unfortunate” because it doesn’t generate any insight. I mean, what are you really supposed to get from this? One takeaway for the practitioner of text analysis is that you might have to do a decent amount of preprocessing before you can make impactful deliverables. Two useful ways you can preprocess raw text here are lemmatizing (or stemming) and using a parts-of-speech tagger to extract only nouns. What both of those steps share is the winnowing out (or consolidating) of noise/chaff. In a subsequent post I’ll try these feature-space-shrinking techniques out."
  },
  {
    "objectID": "posts_assorted/2024-01-24-klein-text/index.html#sentiment-analysis",
    "href": "posts_assorted/2024-01-24-klein-text/index.html#sentiment-analysis",
    "title": "The Ezra Klein Show",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nI think most listeners of The Ezra Klein Show have noticed that Ezra is unique among interviewers in his emotionality. Not emotionality in a histrionic sense, but more like “general emotional urgency.”5 So, in theory it would be interesting to use text analysis to get at this aspect of The Ezra Klein Show that sets the show out from others and uniquely characterizes Ezra as an interviewer. My hunch, however, is that the basic methods of sentiment analysis are not up to the task. In a forthcoming post I’ll dive deeper into why (and what methods would yield more insightful results), but for now let’s very briefly look at the results of these dictionary-based methods.\nAs Tolstoy famously noted, if there are bad ways of doing something, they’ll bad in different ways. Let’s put that to the test with sentiment dictionaries. There are various ‘sentiment lexicons’ that will limpet onto your text to give you the raw materials for calculating sentiment scores at whatever level aggregation you choose. Here I’ve chosen three supposedly general-purpose dictionaries and calculated the sentiment score for each episode of TEKS. As you can see in the table below, their episode-level correlations are very high.\n\n\n\n\nBING\nAFINN\nNRC\n\n\n\n\nBING\n1\n\n\n\n\nAFINN\n0.91\n1\n\n\n\nNRC\n0.83\n0.85\n1\n\n\n\nNice. Convergent validity✅. And now I don’t have to read Anna Karenina.\nIf we want to be a little more aesthetic (💅🏻), we can look at sentiment scores with a plot over time.\n\nSure enough, it looks like the three dictionaries’ scores also (unsurprisingly) strongly correlate at the week level as well. We also see that, by and large, the overall sentiment of episodes is positive.\nI’ve also highlighted which episode each dictionary picks out as the most extreme, both positive and negative While they disagreed on most positive (hence the three positive episodes), there was three-way agreement on the most negative: his interview Rachel Zoffness’s author of The Pain Management Workbook. So it’s hardly surprising that these out-of-the-box algorithms would rank it as the most negative. It is disappointing, though. I went back and listened to the episode after finding out it was universally declared the most negative and … isn’t actually that negative. The post October 7th episodes on the situation in Israel and Gaza are much much more negative. Lesson: convergent validity does not imply construct validity.\nRight, so the simple valence continuum (positive/negative) was mostly a flop. How about using a sentiment dictionary that maps words to eight different emotions and seeing which emotions predominate in The Ezra Klein Show? Eight is four times bigger than two, so that sounds promising. Below is what that looks like over time.\n\nOk, this could be interesting. We see that trust is consistently the great contributor to the show’s positivity and fear to its negativity. Ideally we’d compare these numbers to those of another show. But one way we can kick the tires on this analysis is by seeing what words drive trust and fear’s respective high-rankings.\n\nI’m just … not sure about this. You see why dictionary-based isn’t a particularly valid way of inferring emotion from word usage. Just looking at the Trust side of things, “kind” appears in collocations like “kind of bad,” (which would have the valence wrong) and, “kind of [entity],” which is not usually valenced. It’s certainly not always (probably not even usually) an indicator of trust. You can go down the list and find similar (and other!) problems with every other word. I’m honestly not sure how people take this seriously."
  },
  {
    "objectID": "posts_assorted/2024-01-24-klein-text/index.html#concluding-thoughts",
    "href": "posts_assorted/2024-01-24-klein-text/index.html#concluding-thoughts",
    "title": "The Ezra Klein Show",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nI’m not sure if I would have predicted this beforehand, but by far the most interesting plots here are the two looking at simple volume of speaking. Why is this? I think it has to be because it’s the only case where the method (counting, in this case) adequately represents the thing it purports to measure. I suppose word counts do that as well, but there’s no spice when there’s no implicit contrast (as is the case simple word counts) and then when there is contrast (in those graphs that plot proportions against each other), nothing pops out. Ex ante it might have be unknowable that was going to happen."
  },
  {
    "objectID": "posts_assorted/2024-01-24-klein-text/index.html#acknowledgements-and-arigatos",
    "href": "posts_assorted/2024-01-24-klein-text/index.html#acknowledgements-and-arigatos",
    "title": "The Ezra Klein Show",
    "section": "Acknowledgements and Arigatos",
    "text": "Acknowledgements and Arigatos\nAndrew Heiss’s data visualization course inspired (and enabled) me to have nice(ish) looking text on my graphs. If you know some ggplot2 and are looking to become high-intermediate in your ggplot2 skills, I highly recommend his course.\nThe vast majority of the analyses you see here I learned back in the day from Julia Silge and Dave Robinson’s Tidy Text Mining with R book. I recommend it to anyone as a jumping off point for doing text analysis. Most of the analyses here don’t go (too far) beyond what you can do with the tools you pick up in that book"
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#afinn",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#afinn",
    "title": "Notes on Sentiment Dictionaries",
    "section": "AFINN",
    "text": "AFINN\nThe subtitle of the original publication says it all, “Evaluation of a word list for sentiment analysis in microblogs.” In the paper’s abstract, the great Dane and presumably the namesake of the lexicon, Finn Arup Nielsen, lays out more explicitly why he created a new sentiment lexicon, “There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs.” There’s AFINN’s origin story.3\nOne of the unique features of the AFINN lexicon is that words are mapped to integers instead of merely {positive, negative}. Here you can see a few words at each value:\n\nset.seed(1)\n\nget_sentiments('afinn') %>%\n  group_by(value) %>% \n  add_count(name = 'count') %>% \n  slice_sample(n = 4) %>%\n  summarise(\n    `no. words` = mean(count),\n    words = glue_collapse(word, sep = ', ')) %>% \n  arrange(value) %>% filter(value != 0)\n\n# A tibble: 10 × 3\n   value `no. words` words                                      \n   <dbl>       <dbl> <glue>                                     \n 1    -5          16 motherfucker, bitches, cocksuckers, bastard\n 2    -4          43 scumbag, fucking, fraudsters, fucked       \n 3    -3         264 moron, destroy, despair, scandals          \n 4    -2         966 animosity, censors, robs, touts            \n 5    -1         309 imposing, unclear, demonstration, uncertain\n 6     1         208 share, extend, feeling, commit             \n 7     2         448 tranquil, consent, supportive, sympathetic \n 8     3         172 audacious, classy, luck, gracious          \n 9     4          45 exuberant, wonderful, rejoicing, wowww     \n10     5           5 hurrah, outstanding, superb, thrilled      \n\n\nAnd yes, I did set the seed above to avoid randomly showing you certain words.\nYou might be wondering\n\nwhy -5 to 5 and\nhow he assigned words to those numbers\n\nQuoting him on the former, “As SentiStrength it uses a scoring range from −5 (very negative) to +5 (very positive).” Convention is powerful. As for the latter, the question of how numbers were assigned to words, “Most of the positive words were labeled with +2 and most of the negative words with –2 […]. I typically rated strong obscene words […] with either –4 or –5.” So he was basically winging it.\nAnother unique feature: the dictionary has 15 bigrams, 10 of which are below.4\n\nset.seed(12)\nget_sentiments('afinn') %>%\n  filter(str_detect(word, ' ')) %>% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 2\n   word          value\n   <chr>         <dbl>\n 1 cashing in       -2\n 2 no fun           -3\n 3 green wash       -3\n 4 not good         -2\n 5 dont like        -2\n 6 not working      -3\n 7 some kind         0\n 8 green washing    -3\n 9 cool stuff        3\n10 messing up       -2\n\n\n\nSize: 2477 entries\nCoverage: In addition to the bog-standard sentiment words, it has words all the cool kids were saying in the late 2000, early 2010s.\nR Packages: tidytext, lexicon, textdata\nPublication: Here\nBottom Line: It’s been superseded by VADER (below)"
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#bing-aka-hu-and-liu",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#bing-aka-hu-and-liu",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Bing (aka Hu and Liu)5",
    "text": "Bing (aka Hu and Liu)5\nAccording to the man himself, “This list [i.e., the lexicon] was compiled over many years starting from our first paper (Hu and Liu, KDD-2004).” I’m not sure what the post-publication compilation process was, but the original process is well-described in the original publication. Essentially, they started with adjectives6 with obvious polarity (e.g., great, fantastic, nice, cool, bad, dull) as ‘seed words’ and collected synonyms (and antonyms) of those words, then synonyms and antonyms of those words, and so on, iteratively. To do this, they used WordNet, a chill-ass semantic network. One thing that’s nice about the resulting lexicon is that it’s topic general. That is, though they developed this lexicon for the specific purpose of determining people’s opinions about product features in product reviews, it has a generality beyond that.\nActually looking at the words, you’ll notice there’s some weirdness.\n\nhead(get_sentiments('bing'), 10)\n\n# A tibble: 10 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n\n\nFirst, I’m not sure what “2-faces” is. If you say that it’s a solecistic rendering of “two-faced,” I’d say probably. In their appropriate alphabetic order, both “two-faced” and “two-faces” appear later in the dictionary. Anyway, you’ll notice as well that a lot of the words would reduce to a single lemma if we lemmatized the dictionary. You can think of that as a positive feature of the BING dictionary. It means you don’t have to have lemmatized (or stemmed) text. But its inclusion of derived words seems a bit haphazard. The abort-aborted pair is there, but abolish is hanging out along without its past tense.\n\nSize: In the tidytext package the BING dictionary has 6786 terms (matching what his website says, “around 6800 words”)\nR Packages: tidytext, lexicon, textdata\nBottom Line: It’s a classic and got wide coverage, but not as good as VADER or NRC-EIL."
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#loughran-mcdonald-master-dictionary-w-sentiment-word-lists",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#loughran-mcdonald-master-dictionary-w-sentiment-word-lists",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Loughran-McDonald Master Dictionary w/ Sentiment Word Lists",
    "text": "Loughran-McDonald Master Dictionary w/ Sentiment Word Lists\nI’m honestly why this dictionary is included in packages – not because it’s bad7, but because it’s so (so so so) niche. If you’re doing text analysis on financial text (or aware of cool research doing this), please drop me a line and tell me about it.\nIf you want to learn about it, here’s the page.\n\nR Packages: tidytext, lexicon, textdata"
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#nrc-original",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#nrc-original",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC (original)",
    "text": "NRC (original)\nSaif Mohammad and friends developed a few National Research Council (NRC) of Canada-sponsored sentiment dictionaries. The first of them assigns words not only polarity labels (positive, negative), but emotion labels as well:\n\ntidytext::get_sentiments('nrc') %>% \n  count(sentiment, sort = TRUE)\n\n# A tibble: 10 × 2\n   sentiment        n\n   <chr>        <int>\n 1 negative      3318\n 2 positive      2308\n 3 fear          1474\n 4 anger         1246\n 5 trust         1230\n 6 sadness       1187\n 7 disgust       1056\n 8 anticipation   837\n 9 joy            687\n10 surprise       532\n\n\nThese eight emotions below negative and positive were theorized by Bob Plutchik to be fundamental, or something.8 I’m going to ignore those emotions in this subsection. I’m also not going to talk too much about this dictionary, because it’s superseded by the real-valued its successors, the NRC-EIL and NRC-VAD (below).\nBefore I leave this lexicon, though, one bizarre thing about it: 81 words are both positive and negative (???)\n\nget_sentiments('nrc') %>% \n  filter(sentiment %in% c('negative', 'positive')) %>% \n  add_count(word) %>% \n  filter(n > 1) %>% select(-n)\n\n# A tibble: 162 × 2\n   word       sentiment\n   <chr>      <chr>    \n 1 abundance  negative \n 2 abundance  positive \n 3 armed      negative \n 4 armed      positive \n 5 balm       negative \n 6 balm       positive \n 7 boast      negative \n 8 boast      positive \n 9 boisterous negative \n10 boisterous positive \n# ℹ 152 more rows\n\n\nAs a matter of semantics, I get it for some of these (balm I do not get, though). Practically, if you’re doing an analysis with this dictionary, you’re probably going to want to remove all these terms before calculating sentiment scores.\n\nR Packages: tidytext, lexicon, textdata\nSize: 5464 words (positive and negative, not words with ambiguous polarity)\nBottom Line: Superseded by Saif Mohammed’s subsequent efforts."
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#nrc-eil",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#nrc-eil",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC-EIL",
    "text": "NRC-EIL\nThis thing’s value-added, as my economist friend likes to say, is that instead of a simple ‘positive’ or ‘negative’ value for each sentiment entry, there’s a number between -1 and 1. “Real-valued,” as measurement-heads say. This is actually extremely important if you’re aggregating word-level sentiment into something bigger (which … honestly, email me if you’re doing anything other than that.) How they got these real-valued polarity scores is actually a pretty interesting methods story if you’re into that kind of thing, but I won’t go into ‘MaxDiff scaling’ here. One very important thing to note about this dataset, though, is that valence is that polarity isn’t in this dataset. This vexed me for a minute before I realized that it’s in the real-valued NRC-VAD (below). So, on the off chance you’re looking for the best measurement of Plutchik’s eight basic emotions (and that’s a very off chance), this is the best place to look.\n\nR Packages: textdata\nExamples: You can see an example of an analysis of Ezra Klein’s podcasts here.\nBottom Line: I’m not sure why it exists, but it’s the only lexicon doing what it’s doing."
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#nrc-vad",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#nrc-vad",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC-VAD",
    "text": "NRC-VAD\nOnce you have a hammer, everything starts looking like a nail. That’s how I explain the existence of this dictionary to myself. Saif Mohammed & Co. found these cool scaling technique and were like, “On what dimensions can we scale more words?” They seem to have stumbled on this idea the three most fundamental dimensions in concept space are valence, arousal, and dominance. Maybe it’s an indictment of my memory, perhaps an indictment of the psychology department at the University of Arkansas, but I managed to graduate summa cum laude with a degree in psychology without ever hearing of this. Regardless of supposed fundamental dimensions in concept space, valence is fundamental and is just another word for polarity which is the main thing people are doing with sentiment dictionaries.\nThis also led to my favorite table in I’ve ever seen in an academic article\n\nWhenever I need to express that something is pure bad valence, I now reach for the phrase ‘toxic nightmare shit.’\n\nR Packages: textdata\nBottom Line: Hell yeah. This is a good one."
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#socal",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#socal",
    "title": "Notes on Sentiment Dictionaries",
    "section": "SOCAL",
    "text": "SOCAL\nIt’s the Semantic Orientation CALculator! Like VADER (below), SOCAL is both a sentiment dictionary and rules for modifying words’ sentiment given the context in which they appear. Below, I’ll only briefly consider the dictionary part. I recommend reading the publication both for more details on SOCAL itself as well as sentiment analysis generally. In a sea of sentiment articles that are slapdash publications from some “Proceedings of blah blah blah” or “Miniconference on yak yak yak” this one really stands out for its professionalism and thoroughness.9 As for whether or not you should actually use this dictionary, uh, you should keep reading.\nOne fun thing to note about SOCAL’s dictionary: it has more entries with spaces than any other dictionary I’ve seen.\n\nhash_sentiment_socal_google %>% \n  group_by(n_gram = str_count(x, ' ') + 1) %>% \n  count(n_gram) %>% \n  ungroup()\n\n# A tibble: 8 × 2\n  n_gram     n\n   <dbl> <int>\n1      1  2071\n2      2  1097\n3      3    97\n4      4    17\n5      5     3\n6      6     3\n7      7     1\n8      8     1\n\n\nThis dictionary has not only a huge bigrams:unigrams ratio, but it has sextagrams, a septagram, and even an octogram! This never happens! Let’s look at the n-grams where n > 4\n\nhash_sentiment_socal_google %>% \n  filter(str_count(x, ' ') > 3)\n\nKey: <x>\n                                                           x         y\n                                                      <char>     <num>\n1:                          darker and funnier than expected -4.092375\n2:                                     every other word is f -1.955614\n3:                          in your face everywhere you turn -5.357512\n4: lowbump bump bump bump bump bump bumpbumpbumpbump lowbump  5.622471\n5:                     throw your peanut shells on the floor -5.186874\n6:                                      trying to get on top -2.573428\n7:                               type stype suh huh uh huh's -4.281949\n8:                            write an awesome story to sell -6.262021\n\n\nI, uh, don’t really know what to make of these. There’s actually a restaurant named “Lambert’s Cafe” in Ozark, Missouri where you get peanuts in tin buckets and “throw your peanut shells on the floor” and, unless I’m remembering it wrong, it’s something people like about the place.\nSpeaking of things that are starting to be concerning, the distribution of scores:\n\nlibrary(ggtext)\nggplot(hash_sentiment_socal_google, aes(y)) +\n  geom_density() +\n  labs(\n    y = '',\n    x = 'Valence Score',\n    title = '**What tale tell ye**, ye two thick tails?'\n  ) +\n  theme(plot.title = element_markdown(face = 'italic'))\n\n\n\n\nNo, that density plot isn’t glitching. There really are words out there in the extremes:\n\nhash_sentiment_socal_google %>% \n  filter(abs(y) > 15)\n\nKey: <x>\n                            x         y\n                       <char>     <num>\n 1:    almost mafiosio styled  15.61625\n 2: automatically bestselling  17.60440\n 3:        coming of age isms  17.62518\n 4:           cushion handled  23.45929\n 5:                 hop ified -30.16008\n 6:          keyboard crafted  30.73891\n 7:      more than palateable  25.61696\n 8:          oven to stovetop  19.49040\n 9:             piano blessed  30.40257\n10:   rustic yet contemporary  16.31055\n11:           slotted spooned  18.19201\n12:              thick spoked  20.43281\n\n\nAt this point, you might be wondering … what scale is this? And what are values for our vanilla valence-indicators “good” and “bad”?\n\nhash_sentiment_socal_google %>% \n  filter(x %in% c('good', 'bad'))\n\nKey: <x>\n        x         y\n   <char>     <num>\n1:    bad -1.636520\n2:   good  1.872093\n\n\nOk, that’s fine. Maybe. But here’s something that probably isn’t fine:\n\nhash_sentiment_socal_google %>% \n  filter(str_detect(x, 'good|bad'))\n\nKey: <x>\n                  x          y\n             <char>      <num>\n1:     average good  0.4205258\n2:              bad -1.6365198\n3:        feel good  1.2984294\n4:             good  1.8720931\n5: good intentioned -4.1537399\n6:     good natured -1.5298719\n7:         half bad -3.9237520\n\n\nHere is where I lost all faith. “Good intentioned” and “good natured” are negative?!\nAt this point I’m going to call it a day with SOCAL. At some point I might write to the SOCAL authors or Tyler Rinker to see if something has gone wrong.\n\nPublication: Again, I truly recommend it\nSize: 3290 entries\nR Packages: lexicon"
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#syuzhet",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#syuzhet",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Syuzhet",
    "text": "Syuzhet\n“The default method,”syuzhet” is a custom sentiment dictionary developed in the Nebraska Literary Lab. The default dictionary should be better tuned to fiction as the terms were extracted from a collection of 165,000 human coded sentences taken from a small corpus of contemporary novels.”\nNow, it does include a lot of words I find to be neutral (e.g., “yes”, “true”)\nWe can check out the distribution of the terms in my experimental sideways histogram below:\n\n# ggplot(key_sentiment_jockers, aes(value)) +\n#   geom_histogram(breaks = seq(-1, 1, by = 0.1), color = 'white') +\n#   theme(\n#     panel.grid.major.x = element_blank(),\n#     panel.grid.minor.x = element_blank(),\n#     axis.text.x = element_text(margin = margin(t = -10, b = 5)),\n#     plot.title = element_text(face = 'bold', size = rel(1.3))) +\n#   scale_y_continuous(breaks = c(500, 1000, 1500)) +\n#   labs(\n#     x = 'Jockers/Syuzhet Value',\n#     y = 'Terms in Dictionary',\n#     title = 'Distribution of Sentiment Values in Syuzhet Dictionary'\n#   )\n\n\ndistinct_values <- pull(distinct(key_sentiment_jockers, value))\n\nggplot(key_sentiment_jockers, aes(value)) +\n  geom_bar(width = .07, alpha = 0.8, fill = c(viridis::magma(8, direction = -1), viridis::mako(8))) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), \n            hjust = -0.1,\n            position = position_stack(),\n            family = 'IBM Plex Sans',\n            face = 'bold') +\n  scale_x_continuous(breaks = distinct_values) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    plot.title = element_text(face = 'bold'),\n    axis.text.y = element_text(margin = margin(r = -20))) +\n  coord_flip() +\n  labs(x = 'Valence Value in Syuzhet',\n       y = 'Counts',\n       title = 'Distribution of Sentiment Values in Syuzhet Dictionary') +\n  geom_vline(xintercept = 0)\n\n\n\n\n\nR Packages: syuzhet, lexicon\nSize: 10,748 words\nBottom Line: Pending."
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#vader",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#vader",
    "title": "Notes on Sentiment Dictionaries",
    "section": "VADER",
    "text": "VADER\nIs VADER evil? Maybe. But it also stands for Valence Aware Dictionary and sEntiment Reasoner.10 Impressively, the found “that VADER outperforms individual human raters” when classifying tweets as positive, accurate, or neutral.11 Part (most?) of that impressiveness is due to the ‘ER’ of VADER. Nevertheless, here I’m only considering the VAD part. If you want to check out its ruled-based sentiment reasoning, check out its github page or publication.\nThe lexicon has an impressive 7,500 entries, each with its associated polarity and intensity (-4 to 4). Did they get those intensities just winging it like Finn? Nope. Each potential entry was placed on the -4 to 4 scale by 10 Amazon Mechanical Turk workers.12 The score you do see in the dictionary means that a) raters’ scores had a standard deviation of less than 2.513 and b) that the mean rating among the 10 raters was not 0.14\nOne interesting feature of the lexicon is its inclusion of emoticons.\n\nvader <- read_delim('https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt',\n                    delim = '\\t', col_names = FALSE) %>% magrittr::set_names(c('token', 'score', 'sd', 'scores'))\n\nvader %>% \n  filter(str_detect(token, '[A-Za-z1-9]', negate = TRUE)) %>% \n  group_by(bin = score %/% 1) %>% \n  mutate(y = row_number()) %>% \n  ungroup() %>% \n  ggplot(aes(bin, y, label = token)) +\n  geom_text(check_overlap = TRUE, fontface = 'bold', family = 'IBM Plex Sans') +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(\n    x = '',\n    caption = 'To reduce the real-valued chaos, I rounded down emoticons\\' scores to the nearest integer'\n  ) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    axis.text = element_text(face = 'bold'),\n    panel.grid.minor.x = element_line(linetype = 2, color = 'grey35')\n  ) +\n  guides(color = 'none')\n\n\n\n\nI don’t want to make this dictionary seem trivial. Its creators also validated it using sentences from New York Times editorials, as well. It’s just not every day that you can make a histogram of emoticons.\n\nR Packages: As far as I know, it’s not in any. You can get it directly from its github repository.\nSide Benefit: This is probably the one that your “pythonista” friends are familiar with, since it’s in the nltk library."
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#coverage-comparison-for-afinn-bing-and-nrc",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#coverage-comparison-for-afinn-bing-and-nrc",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Coverage Comparison for AFINN, BING, and NRC",
    "text": "Coverage Comparison for AFINN, BING, and NRC\n\nafinn <- get_sentiments(lexicon = 'afinn')\nbing <- get_sentiments(lexicon = 'bing')\nnrc_emotions <- filter(get_sentiments(lexicon = 'nrc'), \n                    !(sentiment %in% c('positive', 'negative')))\nnrc_polar <- filter(get_sentiments(lexicon = 'nrc'), \n                    sentiment %in% c('positive', 'negative'))\n\n\nbind_rows(\n  select(mutate(afinn, lexicon = 'AFINN'), word, lexicon),\n  select(mutate(bing, lexicon = 'BING'), word, lexicon),\n  select(mutate(nrc_polar, lexicon = 'NRC'), word, lexicon)) %>% \n  summarise(Lexica = list(lexicon), .by = word) %>% \n  ggplot(aes(Lexica)) +\n  geom_bar() +\n  scale_x_upset(n_intersections = 7) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    title = element_text(family = \"IBM Plex Sans\"),\n    plot.title = element_text(face = 'bold'),\n    plot.subtitle = element_markdown(),\n    axis.text = element_text(family = \"IBM Plex Sans\")\n  ) +\n  labs(\n    y = 'Set Size',\n    x = 'Set',\n    title = 'Are Bigger Dictionaries Mostly Supersets of Smaller Dictionaries?',\n    subtitle = \"*No*, and that's weird\"\n  )\n\n\n\n\nWe can see how many entries each dictionary has:\n\nc(nrow(afinn), nrow(bing), nrow(nrc_polar))\n\n[1] 2477 6786 5626\n\n\nHere we see a surprising amount of non-overlap. Of Bing’s 6786 terms, almost 4,000 do not appear in either of the other two dictionaries. Almost 3,000 of NRC’s polarity entries aren’t in either of the other two, as well. The third bar indicates that BING and NRC share just over 1,500 words. The short and long of this is that it might be important which dictionary we choose. They have very different coverages.15"
  },
  {
    "objectID": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#still-to-do",
    "href": "posts_assorted/2024-01-31-sentiments-dictionaries/index.html#still-to-do",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Still to do",
    "text": "Still to do\nThis page, like the rest of my life, is a work in progress.\n\nI still have a few dictionaries to add. In Tyler Rinker’s lexicon package there are: jockers (and jockers_rinker), emojis (and emojis_sentiment), senticnet, sentiword, slangsd. I’ve already covered all dictionaries in tidytext and textdata.\nFurther comparison of dictionary overlap. 2a. Right now I look at three dictionaries’ overlap. These three aren’t the best, they’re just the first ones I checked out. That’s not a principled criterion for selecting dictionaries. I’ll redo that section programmatically. 2b. I’m going to lemmatize/stem the dictionaries covered and get their size and overlaps as I did with AFINN, BING, and NRC above. It’s possible that sizes are much more similar once you remove a bunch of morphological tangle.\nInspired by this sentence from Nielsen, “The word list have a bias towards negative words (1598, corresponding to 65%) compared to positive words (878)” I’m also going to see what the respective positive/negative balances are of these dictionaries."
  },
  {
    "objectID": "section_about.html",
    "href": "section_about.html",
    "title": "About Me",
    "section": "",
    "text": "Since this site’s raison d’être is professional, I should say a word or two about me qua wage laborer.\nI’m a PhD candidate in political science at SUNY Stony Brook where I’m working on my dissertation under the guidance of Michael Peress. During my time here I’ve had the pleasure of taking courses with such luminaries as Stanley Feldman, Andy Delton, Yanna Krupnikov, and Vittorio Mérola. The dissertation I’m currently wrapping up current consists of three mostly independent chapters, covering immigration opinion’s effect on immigration policymaking at the level of US states, cross-national comparisons of the urban-rural divide across the globe, and an intranational study of urban-rural movements and ideological shifts in the Netherlands. These topics represent my circa 2019 interests preserved in amber, with my current interests having moved on to descriptive political epistemology and understanding the interaction between public opinion and legislative institutions regarding artificial intelligence lawmaking.\nAt Stony Brook I’ve been the ‘instructor of record’ for Introduction to Statistics (undergraduate and graduate) and Experiments in Political Science (undergraduate). In addition to those bureaucratically sanctioned courses, I also taught an Introductory Math Camp to incoming PhD students and, along with my esteemed former colleague Pei-Hsun Hsieh, I co-taught a weekly introduction to R during the Fall 2019 semester to graduate students from sundry social sciences (the infamous flyer). Simultaneously with the courses here at Stony Brook, I’ve been teaching statistics and data analysis to students from other graduate programs as well as MBA students.\nBefore landing on the Ye Longe Isle of Bagels & Hockey Fanatics, I taught English for a few years in Ecuador and Peru. And before that, I triple majored in philosophy, psychology, and German at the University of Arkansas (with a year spent at Karl-Franzens Universität in Graz, Austria)."
  },
  {
    "objectID": "section_dissertation.html",
    "href": "section_dissertation.html",
    "title": "Dissertation Bits and Bobs",
    "section": "",
    "text": "When I’m done with my current round of data science and AI job applications and get back to writing my dissertation, I intend to post the most interesting parts as bite-sized posts."
  },
  {
    "objectID": "section_dissertation.html#immigration-opinion-and-policy",
    "href": "section_dissertation.html#immigration-opinion-and-policy",
    "title": "Dissertation Bits and Bobs",
    "section": "Immigration Opinion and Policy",
    "text": "Immigration Opinion and Policy\nOne of my dissertation chapters is about immigration opinion and policy. Here you’ll (soon) find assorted posts related to that project. For an overview of half of the project, you can check out my AAPOR presentation."
  },
  {
    "objectID": "section_dissertation.html#urban-rural-polarization",
    "href": "section_dissertation.html#urban-rural-polarization",
    "title": "Dissertation Bits and Bobs",
    "section": "Urban Rural Polarization",
    "text": "Urban Rural Polarization\nForthcoming section\nThe other two chapters of my dissertation projects examine the urban-rural ideological divide globally. Most academic studies and journalistic commentary alike analyze the American urban-rural divide, but insofar as urbanity and rurality per se either cause or reflect dispositions and preferences linked to political ideology, this should be apparent in ideological self-placement, values, and voting the world over. Is it?"
  },
  {
    "objectID": "section_assorted.html",
    "href": "section_assorted.html",
    "title": "Assorted Posts / Portfolio",
    "section": "",
    "text": "The Gaza Conflict\n\n\nAn investigation of power and norms.\n\n\n\nJustin Dollman\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Dislocation and Economic Ideology\n\n\nFully Automated Luxury Statuslessness\n\n\n\nJustin Dollman\n\n\nJan 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebunkbot Analysis\n\n\nReplicating Costell, Rand, Pennycook (2024)\n\n\n\nJustin Dollman\n\n\nJan 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT to Code NSF Grants\n\n\nAn LLM Experiment\n\n\n\nJustin Dollman\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Sentiment Dictionaries\n\n\nFor Reference Purposes\n\n\n\nJustin Dollman\n\n\nJan 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ezra Klein Show\n\n\nWhat can we learn through text?\n\n\n\nJustin Dollman\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_assorted/2024-07-19-rl/index.html",
    "href": "posts_assorted/2024-07-19-rl/index.html",
    "title": "RL Certificates",
    "section": "",
    "text": "Until I’ve finished working through the exercises in Barto and Sutton’s classic, I won’t try my hand at a fun RL application. For now I’ll just point to a few certificates as indications of my increasing competence in reinforcement learning.\nI’ve completed two DataCamp courses on Reinforcement Learning, Reinforcement Learning in Gymnasium Deep Reinforcement Learning, as well as Hugging Face’s DRL course."
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html",
    "title": "Debunkbot Analysis",
    "section": "",
    "text": "For their paper, they relied on a CloudResearch Connect sample quota-matched to the US Census along various demographic dimensions to make it demographically representative. But knowing that their study would make a splash, they set up a website to which the inevitable media coverage could direct traffic to and generate additional participants. Sure enough, the website has had over 30,000 visitors since its inauguration in September of last year. This post will be an analysis of the data created by those participants."
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#on-certainty",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#on-certainty",
    "title": "Debunkbot Analysis",
    "section": "On Certainty",
    "text": "On Certainty\nAs indicated by the title of the paper, “Durably reducing conspiracy beliefs through dialogues with AI” (emphasis added), the key result was the reduction in conspiracy belief from pre- to post-interaction. Did that also happen in the online sample?\n\n\n\n\n\n\n\n\nThere you see that there was this mass of respondents between 50 and 100 that flattened. To see that more clearly we can go from looking at the pre- and post- certainty distributions and instead visualize the distribution of certainty’s change:\n\n\n\n\n\n\n\n\nYou can see from the histogram above that individuals’ delta scores run the entire gamut of possibility, from -100 (a respondent starts at complete certainty and ends at 0) to +100 (where a respondent initially reports 0% credence and ends up with 100% credence). 23% have a certainty delta of 0, i.e. no change. 17% of respondents report being more certain of their conspiracy after the debunking intervention, and thin plurality of respondents, 61%, report that they are less certain about the conspiracy they described.\nLooking at the raw “certainty trajectories” gives an indication of how people use the certainty scale:\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Start --> End\n                Delta\n                Pct.\n              \n        \n        \n        \n                \n                  100 --> 100\n                  0\n                  9%\n                \n                \n                  0 --> 0    \n                  0\n                  6%\n                \n                \n                  50 --> 50  \n                  0\n                  3%\n                \n        \n      \n    \n\n\n\nBefore analyzing any data, I would not have predicted the most common response being 100% both pre- and post-treatment and I definitely would not have predicted it being several times larger than the next most common certainty trajectory.9 The 0%-to-0% group is likely one that you just want to exclude because they simply do not believe in the conspiracy that the LLM is purportedly debunking.109 This is one of the cases where it would be nice to have some conditional logic in one’s survey that detects an individual’s 100%-to-100% trajectory and asks them why. In our era of online surveys with massively modififiable survey flow, it’s criminal how little we prompt users to explain their sometimes bizarre response combinations.10 For those who understand how credence works (or, is supposed to work), you’ll also notice that these people who report 100% (or 0%) certainty in their conspiracy either do not understand what 100% (or 0%) certainty means or are trolling.\nThere are two explanations for why the 50%-to-50% group is as prominent as it is. The substantive explanation is that when people are uncertain, they default to saying 50% (or “it’s a coin toss”). That is, they map any certainty level between 10% and 90% to 50%. The ‘methods’ (or artifact) explanation for the prominence of the 50%-to-50% group is that the certainty scale is initialized with the dial(?) at 50%. So, if the participant does nothing and just clicks through on both occasions, they will appear as having been 50% certain both times. And participants love nothing more than just clicking through.\nThis pattern of responses – particularly the clustering at 0%, 50%, and 100% – reminded me of my brief prior life when I was really into forecasting (i.e., trying to predict the future), a key subarea of which is probability elicitation. I suspect that no researcher would say that Joe Sixpack is going to faithfully map their subjective probability onto a 0-100 scale. So, looking at the English-language anchors on the scale, I discretized it according to the following mapping:\n\n\n\nCategory\nRange\n\n\n\n\nDef. False\n0-15\n\n\nProb. False\n15-35\n\n\nUncertain (Lean False)\n35-45\n\n\nUncertain\n45-55\n\n\nUncertain (Lean True)\n55-65\n\n\nProb. True\n65-85\n\n\nDef. True\n85-100\n\n\n\nThis allows me to visualize the trajectories thus:\n\n\n\n\n\nOr, if you just want to see the distributions side by side:"
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#conspiracy-descriptions-forthcoming",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#conspiracy-descriptions-forthcoming",
    "title": "Debunkbot Analysis",
    "section": "Conspiracy Descriptions (Forthcoming)",
    "text": "Conspiracy Descriptions (Forthcoming)\nIn addition to replicating the certainty reduction result of the original paper, the much greater sample size of the online sample allows for a text analysis of people’s conspiracy beliefs. For now, I had ChatGPT categorize the user-generated conspiracy text into both narrow and broader categories (species and genus, if you will). As a preview, what are these (mostly) highly educated Democrats conspiratorial about? The top 10 conspiracies mentioned were:\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Conspiracy\n                Mentions\n              \n        \n        \n        \n                \n                  9/11                       \n                  2910\n                \n                \n                  JFK Assassination          \n                  2621\n                \n                \n                  COVID-19                   \n                  2277\n                \n                \n                  Moon Landing               \n                  2062\n                \n                \n                  UFOs                       \n                  1347\n                \n                \n                  2020 Election              \n                  1136\n                \n                \n                  Flat Earth                 \n                  1046\n                \n                \n                  Epstein                    \n                   684\n                \n                \n                  Trump Assassination Attempt\n                   665\n                \n                \n                  Climate Change             \n                   591\n                \n        \n      \n    \n\n\n\nQuite a few classics. The people who (claim to) believe that “bIrDS AreNT rEaL” also showed up:"
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#a-typology-of-users-and-their-conversations-forthcoming",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#a-typology-of-users-and-their-conversations-forthcoming",
    "title": "Debunkbot Analysis",
    "section": "A Typology of Users and Their Conversations (Forthcoming)",
    "text": "A Typology of Users and Their Conversations (Forthcoming)\nThis will be the most interesting part of the post (or perhaps publication further down the road). What aspects of conversations are associated with persuasion? Did gpt-4-1106-preview (or gpt-4o-2024-08-06) deploy different strategies with different users to more or less effect? Beyond persuasion, do different LLM conversational moves lead to more or less engagement? Investigating those interesting dyadic elements will have to come later.\nFor now, let’s look at the most basic measure of conversational quality: message length. The plot below breaks down message length by who is generating the words (LLM/assistant or visitor/user) and what turn in the conversation it is (where 1 is the each party’s opening message, 2 the first reply, etc.):\n\n\n\n\n\nFirst, the fact that the left and right panes have different x-axis scales makes it less obvious that the ‘Assistant’ (the LLM) is writing a lot more than the user. Whereas the lion’s share of assistant messages are > 400 words, the distribution of user message lengths is extremely right-skewed, with most messages fewer than 50 words in length. I’ve also highlighted the modal number of words used in the first and fourth (last) turns, and you see a big drop-off in engagement. Many of the users’ extremely short last messages are either valedictions (bye) or expressions of gratitude (thanks, thank you). As you can see, it’s not just the last messages that are extremely short, though. Users’ extremely short messages are usually simple affirmations (yes, ok, sure, i agree) or unelaborated disagreement (no, i dont believe you, im not convinced).\nThe savvy consumers of survey research among you will wonder about mode effects. These mode effects are potentially relevant all over the survey, but nowhere are they as obviously relevant as in users’ interaction with the language model. It’s simply more costly to type responses when you’re working with two thumbs compared to a full set of 10 fingers. The plots below, which disaggregate by mobil vs. desktop use (and re-aggregate over all four turns) were surprising to me:1111 Transparency note: I filtered out the 1% longest messages both among the users and assistants to prevent tails from overtaking both plots.\n\nHistogramBoxplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI would have suspected that the distributions would be much more different than they in fact are. With the boxplot, you can eyeball that the 25th, 50th, and 75th percentile desktop responders are only writing about 4, 7, 15 more words, respectively, than their mobile counterparts.\n\nA Typology of Users (Forthcoming)\nFrom a theoretical perspective, it’s interesting to think about what moderates treatment effects. Are Republicans more resistant to corrections? What about strong partisans versus leaners? Which type of conspiracy theories are most ‘debunk-resistant’? The list goes on and on.\nFrom a practical perspective, a researcher would want to classify respondents’ “engagement type.” Some unengaged respondents do the open-text version of straight-lining, while others are more engaged, but “in the wrong way” (i.e., trolling).12 Ideally, one would have measured these people’s demographics and political background information at a prior time so that you could model engagment quality (including propensity to troll), but the way information is collected here there’s simply no way to do that. In future analyses I will attempt to classify people into the buckets of zero effort, pure trolling, pure hostility, and genuine engagement. Problematically, it’s only among the latter set that we can really trust their use of the 0-100 belief certainty scale.12 Of course, trolling as a result of treatment is itself interesting. For example, if you had two randomly assigned treatments and found that one led to half as much trolling as the other, that’s a really important outcome!"
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#in-survey-attrition",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#in-survey-attrition",
    "title": "Debunkbot Analysis",
    "section": "In-Survey Attrition",
    "text": "In-Survey Attrition\nThe initiated among you will know about the scourge of missing data. Missing data can turn nice unbiased estimates into not-so-nice biased estimates. It turns random samples into SLOP and, depending on when nonresponse happens, it turns random assignment into non-random assignment, degrading causal quantities to mere correlations. It’s vexatious, en fin.\nThat said, this online survey has no pretention to be a random sample, nor is there random assignment to treatment. This makes it unclear how much of an additional problem missing data is. Nevertheless, here I walk the reader through where visitors attrit in the survey.\nVisitors can be lost in two different ways. The first is that they simply close the survey (whereafter the rest of the values are simply NA). The second happens when the visitor makes makes a choice that means their data will not be useful. Let’s start with the latter class of cases.\n\nAttrition via Respondent Choices\n\n\n\nFirst, the visitor could click the (Optional) - Do not use my data button between assenting to the consent paragraph and cliking the Captcha. 34% of respondents do this.\n\n\n\n\n\nAll respondents’ conspiracy theory descriptions are sent to ChatGPT to check that what the visitor described is, in fact, a conspiracy. If ChatGPT determines that the participant has not described a conspiracy, the participant sees the following page:\n\n\n\n\n\nThose who click Try other version are shuttled into the Disagree with experts arm, but the majority choose Continue. The AI determined that, among those who wrote at least 30 characers describing a purported conspiracy theory, 44% of them failed to describe a conspiracy theory. Of those 44%, 67% chose to continue in the conspiracy arm anyway. The data from those in the latter group is not straightforwardly useful in analyses about conspiracy theories since these people have not described a conspiracy. As a reminder, this failure to describe a conspiracy is prior to a failure to believe in the conspiracy. Which brings me to an interesting wrinkle.\nBelow, you see the scale the subjects used to indicate the extent to which they believe in their conspiracy theory.\n\n\n\n\n\nWhat should we do if a person rates their conspiracy on the false side of uncertain? On the one hand, it might seem strange to talk about belief in conspiracies among people who don’t believe in conspiracies (where lack of belief in \\(p\\) is defined as giving less than 50% probability to \\(p\\)). On the other hand, and I believe it’s the more sophisticated hand, it makes just as much sense to say that an intervention reduced conspiracy belief from 40% to 20% as it does to talk about an 80% to 40% reduction. In both cases you reduced conspiracy belief by half, and there’s nothing magical about the fact that the 80-to-40 reduction crosses 50. So, in all the analyses here, I allow for initial belief to be less than 50%.1313 Much as the distinction in hypothesis testing between ‘significant’ and ‘insignificant’ is not itself significant, the distinction between >50% credence and <50% credence is likewise not significant. Imagine you’re working at a nuclear reactor where most people believe the probability it will “go Chernobyl” in the next month is 0.0000001%, but there are two weirdos who believe that probability is 45% and 55%, respectively. It would be extremely bizarre to group the 45% fella with the 0.0000001% group, but that’s what treating 50% as a magical credence line does. In fact, not reifying 50% as a magical belief line seems especially relevant when talking about conspiracy belief. For example, imagine a person who gives “only” a 40% probability to the proposition that the earth is flat. Though they might not “believe” the earth is flat, they are wildly miscalibrated. If this all seems like pedantic necrotic horse over-flagellation, the Forecasting Research Institute released a study last year where they pitted a group concerned about AI safety against a group of superforecasters unconcerned about AI as an X-risk. If we let \\(\\textbf A\\) mean AI causes existential catastrophe by year 2100, the concerned group came into the study with \\(\\Pr(\\textbf A) = 28.4\\%\\), while the unconcerned group came in at \\(\\Pr(\\textbf A) =0.5\\%\\). If they had to binarized belief at 50%, the entire debate would have evaporated since no one would have ‘believed’ in the existential risk posed by AI. It would also result in strange descriptions of reality, such as “There is a group of extremely concerned people dedicating their lives to reducing AI risk, even though they don’t believe in it.”\n\n\nAttrition via Exit\nNow, what about those who simply click out of the study? In the interactive plot below, you can hover your mouse over each point to see how many subjects were present at each point.1414 This plot is currently malfunctioning. The points are in the correct places and the descriptions that appear when hovered over are correct, but the % still in study and % reduction information is not."
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#disagreement-with-experts-description-and-descriptives",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#disagreement-with-experts-description-and-descriptives",
    "title": "Debunkbot Analysis",
    "section": "Disagreement with Experts Description and Descriptives",
    "text": "Disagreement with Experts Description and Descriptives\nAs mentioned above, not everyone who completes the survey completes the conspiracy reduction arm. After the visitor fills in the two text boxes available to describe a conspiracy and the evidence for it, those responses are sent to a language model to determine if the visitor has in fact described a conspiracy. If it determines that the user has not described a conspiracy, the user will see the following screen:\n\n\n\n\n\nIf they fill in the Try other version radio button (as pictured), they will flow into the Disagreement with experts stream. This results in a very self-selected group.\nSee the screenshots below if you’d like, but basically visitors are asked to describe a belief that most people (possibly including experts) would disagree with and to give the reasons why they find it compelling. If you want to see what an entire conversation looks like, those screenshots are below as well.\nThe counts below are very provisional, but I had ChatGPT code what groups were the implied groups the visitor disagreed with, and the 8 most common were:\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                No. Disagreements\n              \n        \n        \n        \n                \n                  Scientists        \n                  77\n                \n                \n                  Historians        \n                  63\n                \n                \n                  Economists        \n                  58\n                \n                \n                  Theologians       \n                  58\n                \n                \n                  Political analysts\n                  57\n                \n                \n                  Psychologists     \n                  55\n                \n                \n                  Medical experts   \n                  53\n                \n                \n                  Experts           \n                  45\n                \n        \n      \n    \n\n\n\nI also had ChatGPT code both what the implied consensus was and what alternative take the subject had on. Here are just a few of the more entertaining (users’ text followed by three columns of ChatGPT’s coding):"
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#sec-disagreement",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#sec-disagreement",
    "title": "Debunkbot Analysis",
    "section": "Disagreement with Experts Screens",
    "text": "Disagreement with Experts Screens\nFor completeness, here’s what the visitor sees once they are in the Disagreement with Experts arm."
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#sec-conspiracy",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#sec-conspiracy",
    "title": "Debunkbot Analysis",
    "section": "Sample Conspiracy Conversation: Lab Leak",
    "text": "Sample Conspiracy Conversation: Lab Leak\nFor transparency, I’ve posted the conversation as screenshots.15 If the text is too wee for ye eyes, you can right-click any given image and then normal-click Open Image in New Tab on the menu that magically appears.15 “Receipts,” as the kids say"
  },
  {
    "objectID": "posts_assorted/2025-01-09-debunkbot/index.html#sec-keto-konvo",
    "href": "posts_assorted/2025-01-09-debunkbot/index.html#sec-keto-konvo",
    "title": "Debunkbot Analysis",
    "section": "Sample Experts Conversation: Keto Diet",
    "text": "Sample Experts Conversation: Keto Diet\nComments below.\n   \nIn the conversation above, I stumbled into a conversational ‘failure mode.’ My disagreement with experts was, “I think that I think that the health benefits of ketogenic diets are more than just via reduction in calories consumed. Sugars, particularly simple sugars, might be uniquely bad for health (where weight is just one facet of overall health).”16 Now, I only gave this proposition a 55% chance of being true.17 And though I gave the LLM a claim that is at least plausible and I expressed almost maximal uncertainty about it, the LLM seemed forced to argue against my claim as if I had given it a ludricrous claim and said I was certain (e.g. smoking doesn’t actually cause lung cancer, 95%!). That is, neither the direction nor the degree to which user is miscalibrated is taken into account. It would make the behind-the-scenes design a bit more complicated, but ideally you would let the model change its argumentative tack based on the (lack of) discrepancy between the plausibility of the visitor’s view (conspiracy, disagreement with experts) and the probability the user assigns to that view. In that case, instead of “DebunkBot” it would be more “CalibrationBot” that reasons with the user in such a way to get to that probability.1816 And my elaboration was, “I’ve known people who have lost a lot of weight on the keto diet, more so than any other diet. Also, I have a very vague but positive impression of Gary Taubes, a researcher associated with these ideas.”17 In retrospect, 55% is pretty high. Were I to do it again I think I’d put it at 40%. I think I had some sort of glitch in my ‘credence to number’ mapping where I started (anchored) on 50% and then inched my probability upward to indicate that I thought the view has some merit. But a less dumb approach would have been to anchor on \\(\\Pr(\\text{non-obvious health claim is true)}\\) and then increment that upward to the extent the keto claim is more likely than a random non-obvious health claim.18 The survey actually already calculates the plausibility of the claim, but as far as I can make out that information is only used to subset respondents’ change scores to generate personalized feedback like, “When compared to others who hold beliefs in conspiracies with similar plausibility and personal importance levels, your change in belief ranks at the 61st percentile. This means your level of belief change was greater than that of approximately 61% of similar respondents.”"
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html",
    "title": "AI Dislocation and Economic Ideology",
    "section": "",
    "text": "Since Trump’s first presidential victory in 2016, the idea of “economic anxiety” or “economic threat” as a way of explaining Trump’s appeal has been commonplace.1 Presumably, this would operate through people’s economic ideology, as displayed in this diagram:1 Less materialist authors pointed out that, perhaps instead of/in addition to the (purely) economic threat these people felt, they also felt a status threat (here, but here).\nThis study is part of that universe. Here, we’re asking, If learning that your economic horizon is bleak (or positive), do you shift leftward (or rightward) in your economic ideology? Specifically, do you endorse more socialist (or capitalist) norms and descriptions of the world?\nIn writing the materials for the study, we took advantage of the fact that artificial intelligence is widely believed be on the precipice of revolutionizing (at least) the labor market. In the history of research on economic self-interest, it has always been those who are already less well-off who find out they are even worse off. For example, it was the declining manufacturing sector that was hit by the ‘China shock’ of the 1990s. AI is unique in the history of economic shocks because it at least has the potential to hit those at the top hardest. This enables us to credibly inform participants who may be successful in the labor market that that success might be coming to a sudden end.\nSomewhat unique among other studies in this area, we wanted to pinpoint the relative status facet of economic well-being and try to find out what would happen when people’s relative economic standing increased or decreased. Speficially, does this change in relative status change people’s views of the most desirable economic institutions (understood in a broad sense to include the government’s role in the economy as well as the structural characteristics of an economy)? Soon I will get to why I’m not sure we pinpointed that aspect of economic interest so precisely, but that is best accomplished in the context of describing the sample and the study itself."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#sample",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#sample",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Sample",
    "text": "Sample\nBefore we begin, a brief word on the sample. The respondents are only one arm of an experiment I ran with Adam Panish in mid-2024. We started with a demographically representative sample of around 700 from CloudResearch, but most of the participants went into an experiment designed to test a ‘strategic social dominance’ hypothesis we had (see the post). Those that ended up in this experiment did so because they did not identify with either the Israeli or Palestinian sides of that conflict. Specifically, after answering demographic questions, participants were asked if they identified with either Israel or Palestine in the Gaza conflict (with Neither being an explicit third option). In a follow-up question, those who chose Neither were asked if they lean toward either side. Those who identified with either side, either in response to the initial or follow-up questions, were shunted into the Israel-Palestin’ study arm. You can think of that experimental arm as the primary one, but for its materials to make sense we needed subjects that identified with one of those two sides. Those for whom the materials would have been meaningless wound up here. Because most subjects did identify with one of the two sides, this AI-dislocation study had 204 subjects enter it, 178 of which passed a minimal requirement of having spent at least 15 minutes on the study and are included in these results. This, then, is a small sample by post-Replication Crisis standards.\nBecause they’re fun, here’s a Sankey diagram showing who ended up in the sample:\n\n\n\n\n\nIn case you continuous to be curious about the sample, there is more information in the appendix."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#experimental-treatment-and-outcomes",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#experimental-treatment-and-outcomes",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Experimental Treatment and Outcomes",
    "text": "Experimental Treatment and Outcomes\nTo set up the experimental treatment, all participants read a short, two-paragraph excerpt from an article3 about how AI would upend the economy. The article emphasized the two points that “AI will reshape wealth and income distribution, [and] the ultimate winners and losers of the coming AI revolution are far from obvious” You can read the full text below. We then gave them what we sold as a personality test in the style of THESE FOUR QUESTIONS YOU’D NEVER SUSPECT SAY EVERYTHING ABOUT YOUR FUTURE. The quiz was simply four questions that were prima facie relevant to one’s economic prospects (questions below), though we told participants that we would be able “to calculate the most likely effects of AI on your future employment.” After they answered the questions, we told them we were processing the results with a fancy algorithm. This was not the case. In reality, we randomly assigned half of participants to receive a positive message and the other half to receive a negative message. Those messages are printed below:43 It was really a pastiche of information I picked up4 The subjects also saw those images. They were displayed on the screen while the participant was waiting for the oracular algorithm to calculate their future.\n\n\n\nNegative Feedback\n\nYour responses indicate a potential vulnerability to the disruptions AI will bring. Of course, within each group of respondents with similar profiles, some will do better and others worse. That said, respondents with your socio-occupational profile are projected to move down in the income distribution. As a group, people with such profiles will likely lose prestige and relative income.\n\n\nPositive Feedback\n\nWe have good news! Those with your profile are projected to do well in the upcoming labor market transformation, sitting atop5 the income distribution. As AI becomes more important in the economy, you and similar respondents are likely to experience gains in prestige and income relative to those with different profiles.5 Notice the spatially-encoded status language in the feedback: The unfortunate souls are likely to “move down in the income distribution” and “lose prestige and relative income,” while the happy ones will “sit atop the income distribution and”experience gains in prestige and [relative] income.” The spatial bit may not be important, but George Lakoff approves.\n\n\n\nThat was it! Maybe it’s hard to believe, but that kind of thing passes for a treatment in political psychology. You might notice that the main point of the feedback was to impress them that they will lose out in a relative sense. That is, we say nothing about their absolute quality of life in the post-AGI regime, we instead focus on relative income, their place in the income distribution, and prestige (classic relational good). This specificity was very important to us at the time of writing the materials, because we had in mind a specific theory about how a person’s relative standing is an important input to their attitudes about supporting norms. Though I still give credit to the theory, I do not think it’s profitable to think about this experiment in such specific terms. Instead, the treatment represents a kind of economic shock and the outcomes will measure broad left-right economic ideology.\n\nCheck yourself, check your manipulation\nTo gauge respondents’ opinions of the four-question quiz and the ensuing feedback, we asked them, “Briefly, what do you think of the results of the accuracy of feedback you just received?” and they responded in an open text box. I also had my crackerjack research assistant “Claude” code all the responses as positive, negative, or neutral in order to see if those in the positive feedback treatment group actually a positive evaluation of the feedback.66 I told Claude, “You are an expert coder of open-ended responses. Below you will see five responses to the question, \"What do you think of the results of the accuracy of feedback you just received?\" For each, create a set keywords that describes the respondent’s stance toward, or beliefs about, the feedback. The only constraint is that you must describe the stance as positive, negative, or neutral as the first of the keywords. Example responses:[positive, accurate, exciting][negative, inaccurate][neutral, irrelevant]Your response should be five sets of keywords in square brackets separated by newline characters.”\nThe plot shows the probability of a negative, neutral, or positive evaluation between the two treatment groups:\n\n\n\n\n\nNow that’s social science! If you give people negative feedback, people will think (more) negatively of that feedback. If you give people positive feedback, people will think (more) positively of that feedback. But you might also worry about the fact that, among the group that received positive feedback, a full 23% had a negative evaluation of it (and only 57% had a positive opinion of it). As I’ll mention later regarding respondent inattentiveness, ideally I would have had a larger sample so as to be able to look at treatment effect heterogeneity among different groups of treatment evaluation. Given my limited sample size, the most rectitudinous thing I can do is display these data an say C’est la vie.\nHere are some illustrative responses for the raw-data crowd:\n\n\n\nPositive Evaluations\n\n“The results are interesting and I would say accurate”\n“The result makes me happy. But I’m not sure if that would happen.”\n“I extremely relieved. I am also thankful for those results.”\n“Accurate”\n\n\n\nNeutral Evaluations\n\n“Immaterial as I am close to retirement”\n“Okay”\n“I believe AI will take over portions of my job”\n“Hard to say, maybe 50% accurate?”\n\n\n\nNegative Evaluations\n\n“I doubt it’s accurate and I don’t trust it for a second.”\n“I think it’s made up”\n“I think it is fake and predetermined by the researchers.”\n“Totally inaccurate. I am retired and get my income from SS and savings. I’ll be fine.”7\n\n7 One thing I learned in all this is to screen out retirees if you’re (even directly) studying labor market dynamics.\n\n\nIn addition to seeing what people thought of the feedback, we can (and did) ask respondents what they thought of AI, generally. We asked the following question with response categories below:\n“More generally, what is your opinion of artificial intelligence?”\n\nImportant and a force for good (Important/Good)8\nImportant and likely bringing about negative consequences (Important/Bad)\nMostly unimportant, but will cause small positive changes (Unimportant/Good)\nMostly unimportant, like to cause negative consequences if any (Unimportant/Bad)\n\n8 This bold text to the right of the response did not appear to the participants, but allows you to link the response wording to the labels in the plot belowSimilar to the last plot, the plot below gives the probabilities of selecting one of those four responses broken down by treatment group.,]\n\n\n\n\n\nHere, our treatment was (directionally) effective: those who received positive feedback were almost twice as likely to pick the statement ‘AI is important and a force for good’ than those who received negative feedback.9 The positive-feedback receivers were also half as likely to say that AI is unimportant and bad (6% versus 12%, but this effect is less dramatic in absolute difference, which is what pops out on the plot).9 This is one of those cases where a control group would have been nice.\nWe thanked them for completing that exercise and let them know we wanted to “know about your economic views more generally.” These economic views were the outcome variables of the study."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#results",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#results",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Results",
    "text": "Results\nRespondents’ economic views were measured with four groups of outcomes:\n\nA single item measuring respondents’ willingness to punitively ‘soak’ the rich even at the cost of redistributing less to the poor. Its authors describe it as measuing a “wealthy-harming preference.”\nA scale consisting of seven items measuing ‘capitalist values.’\nA 10-item scale measure (dis)agreement with statements expressing either a socialist or laissez faire worldview.\nThe 13-item anti-hierarchical aggression subscale from Costello et al.’s (2022) left-wing authoritarianism scale.10\n\n10 Doing my best imitation of a p-hacker, I also extracted post hoc four items from this scale that mention “the rich” after seeing that they form a unique cluster. That’s the genesis of “A-HA (Rich Items)” in the plot below.This is the results section for those who want to read the short of it. For those interested in the long of it (looking at individual items instead of pre-summated scales, etc.), a longer version of this section appears in the appendix.\nI analyzed each outcome by regressing it on nothing other than treatment (positive feedback, negative feedback). Specifically, I regressed the outcome onto a dummy for having been in the positive treatment group.11 This means that the expected effects are negative, since larger outcome values mean a more ‘lefty’ response and larger treatment variables (i.e., 1 instead of 0) means ‘in the positive feedback group (and thus not negative feedback group).’ For those whose eyes glazed over reading the immediately foregoing, no worries. The results are actually very easy to interpret. In fact, I can summarize the results with the following plot:11 In the case of the three scales, I performed this regression as the structral part of a measurement model where each scale item was taken as an indicator of a latent (endogeneous) variable. For those who care, I used lavaan::sem() in R. In the case of the socialism/laissez faire items, I treated the indicators as ordinal. In the case of the seven-category response to the A-HA items, the reported results come from a model treating the indicators as interval variables. The results don’t change either way.\n\n\n\n\n\nAs you can see, the uncertainty bars for all four (/five) outcomes overlap 0, meaning that none of the results are statistically distinguishable from 0 (ergo no “significant” effects). The point estimate for the capitalist values scale even ruins the story that at least all point estimates were on the expected side of 0."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#post-mortem",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#post-mortem",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Post-Mortem",
    "text": "Post-Mortem\nSo, wherefore this null finding? Leaving the possibility that the null is true for last, let’s briefly run through the ‘boring’ possibilities.\nFirst, could it have been respondent in inattentiveness? As mentioned in the sample section, I excluded participants who took the survey in quicker than 15 minutes. That is an extremely conservative cutoff in the sense that it was really only remove the people who did a speedrun through this thing. Ideally, what I would do were the sample larger, would be to calculate the treatment effect among different subsets of respondents defined by their level of attentiveness (which time in survey is a proxy for). For example, one could do a tercile split of response times to divide them into the quickest, middle, and fastest terciles. If those who were more attentive ‘absorbed’ more of the treatment (and time is a good proxy for attentiveness), then we would see the treatment effect grow across the terciles.\nSecond, perhaps the sample size was too small. Every survey experiment is an exercise in signal detection, sample size is a key signal amplifier, and in this case the sample just couldn’t beat out the noise. Given that so many of the estimates were on the “right side” of zero, I could choose to hold onto this thread of hope. Regardless, I also have to chastise myself a little, because had this been a study with big insitutional backing the fact that I didn’t to an ex ante sample size calculation would be quite damning – perhaps there was no real chance I would have seen a true positive from any plausible effect size.1212 See here for an example of how one would determine a minimum viable sample size.\nRelated to the small sample size, there is the issue that the sample might be of the wrong type (age-wise).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the curious, here is age’s histogram.\n\n\n\n\nLet’s say that you think the treatment could only be effective among an age subgroup defined by being less than a certain age. For example, you might think that those 55+ are close enough any news they hear about the labor market, they think, “Well, sounds like a problem for the next guy to sort out!” In that case, you could look at the cumulative distribution, draw a vertical line up from the x-axis where 55 would be, eyeball the point where it intersects the cumulative distribution, and that point’s y-coordinate is what proportion of our sample even could have produced a theoretically interesting result.13 You can use this to calculate something like an “effective sample size,” which would be some (smaller) proportion of the nominal sample size of 178.^For example, [\\(F(\\text{age}) \\times\\) 178, where \\(F(\\text{age})\\) is the \\(y\\)-coordinate given a certain age, a number between 0 and 1.]13 In this case it would be 75%\nSo, the sample might be inattentive, generically too small, or too small in the demo. But another reason for the null accuses the researcher a bit more. That is simply that the materials were shite-proximate.14 As I show in the appendix, the outcome variable scales really do seem to be picking up meaningful differences among the respondents, so bad dependent variables does not seem to be the culprit. That leaves a potentially ineffectual manipulation. In its favor, you might adduce that you wouldn’t be able to pick out the stimuli used here from a lineup of lab psychology studies. On the other hand, you might point out that psychology was until very recently a methodological cesspool. In this particular case, I’d ask if we even should be trying to manipulate beliefs about the self, in this case perceived (relative) wealth or status, through (mostly) passive, (mostly) text-only interventions. Even economic windfalls, such as winning a lottery or participating in a universal-basic income experiment, seem to have only inconsistent effects on economic ideology ( These two papers find the effect, while these two largely do not). If that is the case, then what hope did we have in trying to manipulate economic views with some text? I think this is an extremely strong point. If actually changing someone’s socio-economic status doesn’t reliably shift their beliefs, then how would telling people that their socio-economic status might change possibly affect their beliefs?1514 Terminus technicus, don’t look at me.15 This doesn’t rule out using humble manipulations such as those here, as long as they use normative argumentation rather than appealing weakly to self-interest.\n\n\n\n\n\nLastly, could the null be true?16 Yes, of course! But I find it very implausible that information about one’s economic future has no impact on one’s economic ideology. Just imagine that you are a down-on-your-luck center-left person in this study and you “find out” that your economic prospects are bleak. In happier moments, you might slightly disagree with “We need to replace the established order by any means necessary,” but mightn’t the camel’s back have broken with the information that your economic future is downhill and now you instead slightly agree? I don’t know. It’s worth a thought.16 Yes, I know. The null is never going to be true, but you know what I mean. The actual effect is smaller than anything we’d care about."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#res-long",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#res-long",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Results (Long)",
    "text": "Results (Long)\nFor the time-wealthy, I have a more detailed break down of the outcomes and results. Instead of presenting the scales as aggregates, you can see them disaggregated by individual question. I also show their partisan and ideological breakdowns, so you can get an idea of whether or not you think these scales are measuring anything at all!\n\nTo Soak or Not to Soak\nThe first item17 presents respondents with the following two scenarios and asks them to select the one they prefer:17 I believe it originates with Sznycer et al. (2017).\n\nThe top 1% wealthiest individuals pay an extra 50% of their income in additional taxes, and as a consequence of that the poor get an additional $100 million (the extra 50% in taxes paid in former fiscal years leaving the wealthiest with relatively less taxable income.)\nThe top 1% wealthiest individuals pay an extra 10% of their income in additional taxes, and as a consequence of that the poor get an additional $200 million (the extra 10% in taxes paid in former fiscal years leaving the wealthiest with relatively more taxable income.)18\n\n18 The bold text also appears bold in the experiment.This item is nice because it pits two (potential) desires against each other: the punitive desire to harm the rich and the make the less well-off better-off. If you think that e.g., ‘Every billionaire is a policy failure’, then you’ll be drawn toward the first option. If, however, you think that the contemporary focus on inequality is a misguided worry about the poor, you’ll be drawn to the second.1919 A sophisticated survey taker might also have positional and efficiency considerations, but I’ll leave those aside.\nUnfortunately for me, but also consistent reasonable prior of minimal effects, there was no significant effect of the treatment on this outcome. Had I seen the plot below, which shows that punitive taxation (perhaps surprisingly) does not change that much across the ideological spectrum, the null finding should not be surprising.\n\n\n\n\n\nAnd although we see substantively different point estimates (e.g., liberals are more than twice as likely to assent than conservatives), the within-group heterogeneity is such that we would have needed a much larger sample to detect any differences caused by the AI treatment.2020 For the curious, Sznycer et. al (2017) found that “dispositional envy” was the only reliable predictor of this outcome, and Lin et al. (2024) similarly found a strong (and unique) association between they called “malicious envy” and this measure. So perhaps this measure only works to the extent that a treatment induces envy.\n\n\nCapitalist Values\nWe took the following seven items from the capitalist values scale from Chong, McClosky, and Zaller’s (1983) Opinions and Values Survey.2121 Their original capitalist values scale contained 28 (!!) items.\n\nGetting ahead in the world is mostly a matter of …\n\ngood luck\nability and hard work\n\nWhen it comes to taxes, corporations and wealthy people …\n\ndon’t pay their fair share\npay their fair share and more\n\nFree-market economies …\n\nsurvive by keeping the poor down\ngive everyone a fair chance\n\nThe profits a company or business can earn should be …\n\nstrictly limited by law to a certain level\nas large as they can fairly earn\n\nCompetition, whether in school, work, or business …\n\nis often wasteful and destructive\nleads to better performance and a desire for excellence\n\nMost business people …\n\ndo important work and deserve high salaries\nreceive more income than they deserve\n\nThe government taking over a larger share of the economy to limit the profit motive would be …\n\na good idea\na bad idea\n\n\nAs with the ‘soak the rich’ item, each capitalist values item presents respondents with a binary choice (this time in a sentence-completion format. One choice is (directionally) congruent with a pro-capitalism worldview while the other choice is not.22 For the purpose of this appendix, however, I will eschew theorizing and merely note that the items ‘work’ in that they pick up different priorities among ideological groups:22 If the reader is interested in an analysis of America’s twin traditions of capitalism and democracy, I encourage readers to go read either the original 1983 article whence these items came or McClosky and Zaller’s book, The American Ethos: Public Attitudes Toward Capitalism and Democracy, that came out the following year.\n\n\n\n\n\nConsidering the items as a scale, they have acceptable internal coherence (Cronbach’s \\(\\alpha=\\) 0.73, \\(\\Omega=\\) 0.73). To use concepts from item-response theory, you can see from the plot that a scale composed of these items spans a good range of difficulty23 and all its items have prima facie discrimination, as they reliably order the ideological groups.24 Even with decently cromulent scale in hand, I fail to detect any statistically significant difference between the positive and negative feedback groups.23 Briefly, an item’s difficulty indicates how ‘extreme’ you have to be on a trait in order to answer the item in the affirmative. I say this scale has a good range of difficulty because you have items ranging from <25% agreement to >75% agreement among the moderates.24 An item’s discrimination indicates the extent to which it distinguishes between those with higher and lower values of the trait.\n\n\n\n\n\nYou’ll notice that a) none of these difference are significant and b) even when we might be nearing the ignis fatuus of “marginal” significance, the results are inconsistent. When asked about the government taking over a larger share of the economy (the control item), those who received negative feedback were more likely to assent. However, on an item that really should be tapping the same ideas (‘Do free-market economies keep the poor down or give people a fair chance’), the pattern is reversed.\n\n\nSocialist/Laissez Faire Items\nAnother tranche of items came from Heath, Evans, and Martin (1994). As with the previous scale, we culled out those whose language seemed a throwback to an earlier era using a sophisticated NLP technique called “mid-Millennial/elder Gen Z savoire faire.”2525 Apparently we also added at least a couple of items from Leslie McCall’s research, but I can no longer find their exact source.\nUnlike the previous sentence-completion items, these were presented in a grid format:2626 In case the image is too small to read the text, click here to read the questions.\n\n\n\n\n\nOne thing I always worry about when I (or anyone else) use a grid is that respondents will click in a straight-line.27 I checked and only two respondents did this.27 Occasionally I myself do this when I’m at a doctor’s office and I’m given a bunch of possible pre-existing conditions I don’t have.\nAgain, the scale ‘worked’ in that it was internally consistent (Cronbach’s \\(\\alpha=\\) 0.73, \\(\\Omega=\\) 0.83), traversed a wide range of assent, and differentiated liberals and conservatives.\n\n\n\n\n\nAlso again, no consistent differences between the two treatment groups.\n\n\n\n\n\n\n\nAnti-Hierarchical Aggression\nWe used the 13 items from the left-wing authoritarianism (LWA) scale developed in Costello et al. (2022) that tap the anti-hierarchical aggression facet of LWA. These were:\n\nIf I could remake society, I would put people who currently have the most privilege at the very bottom.\nWhen the tables are turned on the oppressors at the top of society, I will enjoy watching them suffer the violence that they have inflicted on so many others.\nMost rich Wall Street executives deserve to be thrown in prison.\nConstitutions and laws are just another way for the powerful to destroy our dignity and individuality.\nThe current system is beyond repair.\nWe need to replace the established order by any means necessary.\nCertain elements in our society must be made to pay for the violence of their ancestors.\nThe rich should be stripped of their belongings and status.\nRich people should be forced to give up virtually all of their wealth.\nAmerica would be much better off if all of the rich people were at the bottom of the social ladder.\nPolitical violence can be constructive when it serves the cause of social justice.\nIf a few of the worst Republican politicians were assassinated, it wouldn’t be the end of the world.\nI would prefer a far-left leader with absolute authority over a right-wing leader with limited power.\n\nEach item had seven response options arranged vertically below it, ranging from “Strongly disagree” to “Strongly agree”. If you allow a humble researcher (me) to turn those qualitative responses into a 1 - 7 scale, the distribution of responses looks as follows:\n\n\n\n\n\nLike the other scales, this LWA’s “AHA” has high internal coherence (Cronbach’s \\(\\alpha=\\) 0.9, \\(\\Omega=\\) 0.93). Unlike the previous two, this scale is a bit wonkier as far as ‘correctly’ ordering liberals, moderates, and conservatives. The keen eye will notice that it’s often moderates who are higher in anti-hierarchical aggression than liberals. It’s also a more ‘difficult’ scale: as a group, moderates are never at or above the scale’s midpoint (4). Again, no sigificant differences between treatment and control (not pictured).\nOne very interesting thing, though. You’ll notice that some of the items along the y-axis of the plot look like this. These items correspond to the statements I italicized in the list above and are the four items that explicitly mention “the rich.” They exquisitely differentiate among different ideologies. There are only two other items in the scale that differentiate like this, the “prefer a far-left leader” item (unsurprising that liberals would be more sympathetic to a left leader) and the “assassinate Republicans” item (and we now know there’s a taste for that). I think it’s really interesting that “the rich” is such a strong ideological trigger."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#biotsff",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#biotsff",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Bonus Information on the Sample (For Free)",
    "text": "Bonus Information on the Sample (For Free)\nAs I mentioned a moment a go, this leaves me with a relatively small sample. It also skews the sample more ideologically moderate and less politically engaged/interested than the original sample.2828 If you want to know more about who ended up in this arm of the study, I would encourage you to go to the Strategic SDO post where I go through how those who identify with a side in the Israel-Palestine conflict (and consequently end up in the SDO arm) are different from those who do not (and are consequently included in the study discussed here).\n\n\n\n\nRaw NumbersProportions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe idea here isn’t to take away specific numbers, but to determine if you think the selection into this sample would change your interpretation of it. The upper left member of the triptych, political interest, breaks down respondents’ sample by their level of self-averred political interest.29 Only in the “Not at all interested” group is there rough parity between the two groups. “Not at all interested” is, though, by far the smallest group in the entire sample.29 Political interested was measured with the question “How interested are you in politics?” to which people could choose among “{Not at all, Slightly, Somewhat, Very} Interested”.\nIn the upper right, the sample is broken down by ‘folded’ ideology, whereby moderates form one group, slight liberals/conservatives are counted together as one group, same for four-square liberals and conservatives, and way out on the end, all people who attach “extreme” to their ideology are grouped together. Folded ideology shows a similiar pattaren to interest. The soi-disant moderates are roughly equally split between the two samples, as were the not-at-all interested. For anyone who self-describes as slightly liberal/conservative, liberal/conservative, extremely liberal/conservative, they are much more likely to choose one of the two sides and send up in the Strategic SDO sample.\nLest you think that extreme ideologues abound in the one arm while moderates populate the other, you can imagine the following thought experiment. If this entire study had been done in person and each arm had 100 people in it, what would the ideological breakdown be in the respective arms? If the 100 people in a given arm were in a room, how many of them would be slightly liberal, extremely conservative, etc.?\n\n\n\n\n\nThe distributions are different. If effects were observed, the effect in the Strategic SDO arm would be more driven by liberals whereas the effect in this study would be more driven by moderates. How does that affect our findings? First, any differences between those in the positive versus negative treatment groups would still be a causal. There would be no “correlation does not equal causation”-ing this study. One could argue that the disproportionate amount of moderates in this study might make finding an effect easier because their economic ideology (such as it is) is less crystalized. Instead of being well-learned and bound up with their identity, it is more malleable and pragmatic. Or, alternatively, perhaps moderates are less ready to connect economic information with political views, making it less likely that an economic treatment will move political outcomes. Personally, I think ratiocination about potential heterogeneous treatment effects is still premature.30 ## Materials Appendix {#ma}30 Premature at least with regard to political ideology and interest. As I will remark later, age (which proxies as ’amount of time exposed to new dispensation), wealth, and skill level should be relevant moderators.\nAll participants saw the following block of text\n\nAI’s Upending of the Economy\n\n\nThe World Economic Forum estimated that Artificial Intelligence (AI) will replace 85 million jobs by 2025. In advanced economies such as the United States, about 60% of jobs will likely be impacted by AI. While some jobs will benefit from AI integration via enhanced productivity, the lion’s share will be made partially or completely redundant. This will likely lower labor demand, leading to lower wages and reduced hiring. In the most extreme cases, some of these jobs may disappear altogether.\n\n\nThough we know that AI will reshape wealth and income distribution, the ultimate winners and losers of the coming AI revolution are far from obvious. Counterintuitively, those with traditionally “low-prestige” jobs involving manual labor or skills learned at community colleges could find themselves in higher demand and making more money than those holding prestigious degrees, as AI’s first targets are those with “the most highly compensated, highly creative, and highly educated work” (Mollick, 2024).\n\nAfter reading that, they saw a screen with\n\nNow, we’d like to ask you a few questions about your job and field of occupation that will allow us to estimate how you and people with your work profile will fare in the post-AI economy.\n\n\nWe already have your age, level of highest education, and income, so we just need to ask four questions in order to calculate the most likely effects of AI on your future employment.\n\nAnd the four questions, with response options, were\n\nWhat is your current occupation? [Open-ended responses]\nWhat is your current level of flexibility regarding remote work? (Fully remote, hybrid, on-site)\nIn what field is your highest degree? [Open-ended responses]\nIn your current role, does your work primarily involve interacting with people or analyzing data and information?\n\nMostly processing information and analyzing data\nMostly interacting with people\n\n\nI’m not sure I’ll do this again, but it was a learning experience."
  },
  {
    "objectID": "posts_assorted/2025-01-09-ai-econ/index.html#slf-wording",
    "href": "posts_assorted/2025-01-09-ai-econ/index.html#slf-wording",
    "title": "AI Dislocation and Economic Ideology",
    "section": "Socialist/Laissez Faire Text",
    "text": "Socialist/Laissez Faire Text\nHere are the items (with color indicating those that are reverse coded)\n\nMany people who get welfare don’t really deserve any help\nThe government should spend more on unemployment benefits, even at the expense of other government programs\nMost unemployed people could find a job if they really wanted one\nLuck explains people’s misfortune as much as anything else\nManagement will always try to get the better of employees if it gets the chance\nThere is one law for the rich and one for the poor.\nWorking people do not get their fair share of the nation’s wealth\nInequality continues to exist because it benefits the rich and powerful.\nDifferences in income in the US are too large.\nIt is the responsibility of the government to reduce the differences in income.]\n\nReturn to item results."
  },
  {
    "objectID": "posts_assorted/2025-01-12-sdo-ispa/index.html",
    "href": "posts_assorted/2025-01-12-sdo-ispa/index.html",
    "title": "The Gaza Conflict",
    "section": "",
    "text": "This piece is currently up as a placeholder so that links in other pieces don’t produce errors. The post will be finalized and posted the week of January 6th, 2025."
  },
  {
    "objectID": "posts_assorted/2025-01-12-sdo-ispa/index.html#sample-selection",
    "href": "posts_assorted/2025-01-12-sdo-ispa/index.html#sample-selection",
    "title": "The Gaza Conflict",
    "section": "Sample Selection",
    "text": "Sample Selection\nUnlike most studies, the method by which our sample entered provides information on the matter itself. As mentioned in the introduction, our design was to show partisans of the two sides either favorable or unfavorable information about the prospects of the side they identified with in post 10/7 conflict. Obviously, not everyone is a partisan of one of the two sides. These people can’t substantively participate in this experiment, so we wanted to detect them and have them participate in a different experiment (one where they could potentially contribute useful information). The very detection of these people provides the first interesting analysis.\nAfter asking a series of demographic questions, we asked the following question:\n\n    \n        \n            Recently there has been a lot of news about the conflict between Israelis and Palestinians in the Middle East. Which of the following statements best describes your feelings about the conflict?\n        \n        \n            \n                \n                When I think about the conflict, I tend to identify more with the Israeli side\n            \n            \n                \n                When I think about the conflict, I tend to identify more with the Palestinian side\n            \n            \n                \n                When I think about the conflict, I tend not to identify with either side\n            \n        \n    \n\n\n\n\nWe started with 712 participants, 65 of whom were excluded from all analyses as they sped through the survey faster than one could have read and understood the materials. under our consideration saw the first\nOf the 647 non-speeders, 162 (25%) identified with the Israeli side, 183 (28%) identified with the Palestinian side, and the remaining 302 (47%) chose the third option. Those who chose the last option then saw:\n\n    \n        \n            Even if you do not tend to identify with either side, would you say that you lean towards one side or the other?\n        \n        \n            \n                \n                I tend to lean toward the Israeli side\n            \n            \n                \n                I tend to lean toward the Palestinian side\n            \n            \n                \n                I do not lean toward either side\n            \n        \n    \n\nOf these, 60 (20%) indicated that they lean toward the Israeli side, while 64 (21%) leaned toward the Palestinian side. That leaves 178 (59%) again choosing neither side.\nAll in all, the flow of participants looks as follows:\n\n\n\n\n\nSome may see this and think, so what? Some people identify with one side or the other and some identify with neither. It’s life! But this should be strange. First, leaving aside the particulars of the conflict, Americans are (in)famous for not caring about foreign affairs. The fact that Neither was the least frequent choice (by far) would rattle some cocos. Second, it would be very surprising to a time-traveller from just a few years ago that the plurality of a representative-ish sample identified with Palestine. Lastly and related to the previous points, the Gaza conflict has proven to be a subject that evokes passions.1 Among those who lean towards one of the two sides, there are likely those who were driven to change vote because of it, people protested, others counterprotested, etc. The causes and effects of this identification is worth studying.2.1 Upon reading a draft of this, a colleague remarked that, however stongly one feels about the two sides of the conflict over Gaza, the most odious group is, past any doubt, the survey speedsters. This colleague is a ne’er-do-well.2 Let’s hope even the associations are worth studying haha lol\n\nBackground Variables\nTo understand who identifies with which side (or no side at all), I ran a random forest model predicting respondents’ ideological categorization into one of five categories (no identification, plus ‘direct’ and ‘lean’ identification with the two sides).33 For those unfamiliar with random forests, there is an explanation forthcoming!\nWhich demographic variables were important in determining whether people identified with Israel or Palestine?\n\n\n\n\n\n\n\n\nIn the plot below we can see a result of political ideology’s importance is such an important variable in determining which side (if any) a person identifies with in the conflict. The plurality of all types of liberals identifies with Palestine, the plurality of self-described moderates identifies with neither sid, and the plurality of conservatives identify with Israel.\n\n\n\n\n\nSimilarly, we can see age’s importance:\n\n\n\n\n\nYou can think of this plot as telling you, “If you meet a random person from this generation, what is the probability that they fall into one of these buckets?” And you see extremely strong generational effects. Whereas the plurality of Baby Boomers and Gen Xers identify with Israel, Israel identification is the least chosen option among both millennials and gen z. In fact, in both of those groups, identification with the Palestinian side of the conflict is about twice as frequent as identification with Israel.\n\n\nPolitical Interest\nInstead of asking which side people identify with, we can look at how quickly they identify as a function of their self-reported political interest.4 As one would expect, the likelihood of identifying with one of the sides more than doubled going from those who said that they’re not at all interested in politics to those who report being very interested in politics.4 Political interested was measured with the question “How interested are you in politics?” to which people could choose among “{Not at all, Slightly, Somewhat, Very} Interested”.I should mention that it’s very important to refer to political interest measured this way “self-reported political interest” because political interest is like news consumption and voting in that all three conform to House MD’s maxim, “Everyone lies.”"
  }
]