[
  {
    "objectID": "section_text.html",
    "href": "section_text.html",
    "title": "Text Analysis",
    "section": "",
    "text": "Many moons ago, I read Julia Silge and David Robinson’s Text Mining with R and then Emil Hvitfeldt and Silge’s and Supervised Machine Learning for Text Analysis in R. Though I have never done anything professional with text analysis, I’ve done a few things here and there. Some of them are here, below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Sentiment Dictionaries\n\n\nFor Reference Purposes\n\n\n\nJustin Dollman\n\n\nJan 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ezra Klein Show\n\n\nWhat can we learn through text?\n\n\n\nJustin Dollman\n\n\nJan 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "section_ur.html",
    "href": "section_ur.html",
    "title": "Urban-Rural Polarization",
    "section": "",
    "text": "Forthcoming section.\nOne of my dissertation projects examines the urban-rural ideological divide globally. Most academic studies and journalistic commentary alike analyze the American urban-rural divide, but insofar as urbanity and rurality per se either cause or reflect dispositions and preferences linked to political ideology, this should be apparent in ideological self-placement, values, and voting the world over. Is it?\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "section_about.html",
    "href": "section_about.html",
    "title": "About Me",
    "section": "",
    "text": "Since this site’s raison d’être is professional, I should say a word or two about me qua wage laborer.\nI’m a PhD candidate in political science at SUNY Stony Brook where I’m working on my dissertation under the advisement of Michael Peress. During my time here I’ve had the pleasure of taking courses with such luminaries as Stanley Feldman, Andy Delton, Yanna Krupnikov, and Vittorio Mérola.\nAt Stony Brook I’ve been the ‘instructor of record’ for Introduction to Statistics (undergraduate and graduate) and Experiments in Political Science (undergraduate). In addition to those bureaucratically sanctioned courses, I also taught an Introductory Math Camp to incoming PhD students and, along with my esteemed former colleague Pei-Hsun Hsieh, I co-taught a weekly introduction to R during the Fall 2019 semester to graduate students from sundry social sciences (the infamous flyer). Simultaneously with the courses here at Stony Brook, I’ve been teaching statistics and data analysis to students from other graduate programs as well as MBA students.\nBefore landing on the Ye Longe Isle of Bagels & Hockey Fanatics, I taught English for a few years in Ecuador and Peru. And before that, I triple majored in philosophy, psychology, and German at the University of Arkansas (with a year spent at Karl-Franzens Universität in Graz, Austria)."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html",
    "href": "posts_text/2024-01-24-klein-text/index.html",
    "title": "The Ezra Klein Show",
    "section": "",
    "text": "For reasons lost to oblivion, I decided to analyze transcripts of Ezra Klein’s podcasts using elementary text analysis. The following was the result."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#interviewing-style",
    "href": "posts_text/2024-01-24-klein-text/index.html#interviewing-style",
    "title": "The Ezra Klein Show",
    "section": "Interviewing Style",
    "text": "Interviewing Style\nBefore even looking at the content of the episodes, before looking at what words are said, sentiments are expressed and topics are covered, we can look at volume of speaking.\n  \nWe can also compare this distribution of how much Ezra speaks as interviewer to how much other interviewers speak. After all, it’s hard to really feel what the numbers in the graph above mean without comparing them to the relative talkativeness of other interviewers. Below is a comparison of Ezra with two other interviewers, both of whom have had Ezra on as a guest on their programs: Rob Wiblin (80,000 Hours) and Tyler Cowen (Conversations with Tyler).\n\nThis is a little more interesting, I think.\nMy hypothesis before crunching these data was that Tyler was speak the least (and it wouldn’t be close), then Rob, then Ezra. And that turned out to be right.1 After all, Ezra speaks quiet a bit because people tune into Ezra’s show in order to listen to Ezra. Though Tyler’s show does have “Tyler” in the name, he frequently announces that his interviews are the interviews that he wants, not what the listeners might want. And his listener-be-damned approach often results in quick questions and abrupt topic changes. The 80,000 Hours podcast has this intermediate position where Rob often positions himself as a listener might and restates what the speaker said, gives the conventional wisdom against which the interviewee can react, and ask follow-up questions."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#ezras-words",
    "href": "posts_text/2024-01-24-klein-text/index.html#ezras-words",
    "title": "The Ezra Klein Show",
    "section": "Ezra’s Words",
    "text": "Ezra’s Words\nAlright, so what does this ingenious interviewer from Irvine actually talk about? Among the ways of answering that question, one is to just look at the words he uses. The simplest way of operationalizing ‘words used’ is to look at “unigrams,” which result from splitting the unstructured text at every whitespace2, making each whitespace-separated unit into a row in a dataframe, and counting how many rows each token (word) appears in that resulting dataframe. Ezra’s result is below.3\n\n \nBut you can also get slighly more sophisticated and look at contiguous words. So, instead of splitting at every whitespace, you split at every second or third whitespace. The nice thing about that is you start to see phrases, turns of speech. For example, if you were curious why “people,” “lot,” and” bit” appear in the unigrams, the bottom-most panel will give you your answer.\n  \nAmong the bigrams and trigrams, an Ezra Klein Show fan will also be pleased to see Ezra’s pat phrases for parts of his podcast: “final question”, “books you’d recommend,” and “email [is] ezrakleinshow@nytimes.com.” I’m not sure if seeing these phrases is interesting, but they are nice proofs of concept for the method. You also see that the bigrams are dominated by pairs of words that, yes, do have a space between them, but they are lexically one thing. I’d argue that “past couple” and “minute ago” are only instances of more-or-less independent words coming together.\nNow let’s try something potentially more interesting. We can also look at how he uses words differentially. Why would that be useful? Well, just looking at the words he uses might tell us that he’s a guy with a show that covers topics X, Y, and Z that use the words associated with those topics.4 One way we can “control for” topic is to contrast Ezra’s words with those of his guest. Abstractly, the idea is that if person A and B are talking about subject S, the words that differentiate A and B are not going to be those that generically describe subject S. Rather, they will be the words idiosyncratically associated with each interlocutor.\n\nThis surprised me. Sure, one would expect most words to be along the diagonal line which indicates that the word is used in equal proportions by both Ezra and his guests, but I really expected this to show more unique usages in the off-diagonal space.\nWe can also compare the words of the Ezra Klein Show to the words used in introductory political science textbooks. The idea of such an analysis would be to ask, “When Ezra talks politics, which topics (proxied by words) does he dedicate space to than an introduction to politics? Which topics does he spend less time one?” From these results one could hope to find out something about which aspects of politics Ezra finds interesting and important, and which less so. Unfortunately, I carried out the analysis and the result is below:\n \n I call this ‘analysis’ “unfortunate” because it doesn’t generate any insight. I mean, what are you really supposed to get from this? One takeaway for the practitioner of text analysis is that you might have to do a decent amount of preprocessing before you can make impactful deliverables. Two useful ways you can preprocess raw text here are lemmatizing (or stemming) and using a parts-of-speech tagger to extract only nouns. What both of those steps share is the winnowing out (or consolidating) of noise/chaff. In a subsequent post I’ll try these feature-space-shrinking techniques out."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#sentiment-analysis",
    "href": "posts_text/2024-01-24-klein-text/index.html#sentiment-analysis",
    "title": "The Ezra Klein Show",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nI think most listeners of The Ezra Klein Show have noticed that Ezra is unique among interviewers in his emotionality. Not emotionality in a histrionic sense, but more like “general emotional urgency.”5 So, in theory it would be interesting to use text analysis to get at this aspect of The Ezra Klein Show that sets the show out from others and uniquely characterizes Ezra as an interviewer. My hunch, however, is that the basic methods of sentiment analysis are not up to the task. In a forthcoming post I’ll dive deeper into why (and what methods would yield more insightful results), but for now let’s very briefly look at the results of these dictionary-based methods.\nAs Tolstoy famously noted, if there are bad ways of doing something, they’ll bad in different ways. Let’s put that to the test with sentiment dictionaries. There are various ‘sentiment lexicons’ that will limpet onto your text to give you the raw materials for calculating sentiment scores at whatever level aggregation you choose. Here I’ve chosen three supposedly general-purpose dictionaries and calculated the sentiment score for each episode of TEKS. As you can see in the table below, their episode-level correlations are very high.\n\n\n\n\nBING\nAFINN\nNRC\n\n\n\n\nBING\n1\n\n\n\n\nAFINN\n0.91\n1\n\n\n\nNRC\n0.83\n0.85\n1\n\n\n\nNice. Convergent validity✅. And now I don’t have to read Anna Karenina.\nIf we want to be a little more aesthetic (💅🏻), we can look at sentiment scores with a plot over time.\n\nSure enough, it looks like the three dictionaries’ scores also (unsurprisingly) strongly correlate at the week level as well. We also see that, by and large, the overall sentiment of episodes is positive.\nI’ve also highlighted which episode each dictionary picks out as the most extreme, both positive and negative While they disagreed on most positive (hence the three positive episodes), there was three-way agreement on the most negative: his interview Rachel Zoffness’s author of The Pain Management Workbook. So it’s hardly surprising that these out-of-the-box algorithms would rank it as the most negative. It is disappointing, though. I went back and listened to the episode after finding out it was universally declared the most negative and … isn’t actually that negative. The post October 7th episodes on the situation in Israel and Gaza are much much more negative. Lesson: convergent validity does not imply construct validity.\nRight, so the simple valence continuum (positive/negative) was mostly a flop. How about using a sentiment dictionary that maps words to eight different emotions and seeing which emotions predominate in The Ezra Klein Show? Eight is four times bigger than two, so that sounds promising. Below is what that looks like over time.\n\nOk, this could be interesting. We see that trust is consistently the great contributor to the show’s positivity and fear to its negativity. Ideally we’d compare these numbers to those of another show. But one way we can kick the tires on this analysis is by seeing what words drive trust and fear’s respective high-rankings.\n\nI’m just … not sure about this. You see why dictionary-based isn’t a particularly valid way of inferring emotion from word usage. Just looking at the Trust side of things, “kind” appears in collocations like “kind of bad,” (which would have the valence wrong) and, “kind of [entity],” which is not usually valenced. It’s certainly not always (probably not even usually) an indicator of trust. You can go down the list and find similar (and other!) problems with every other word. I’m honestly not sure how people take this seriously."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#concluding-thoughts",
    "href": "posts_text/2024-01-24-klein-text/index.html#concluding-thoughts",
    "title": "The Ezra Klein Show",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nI’m not sure if I would have predicted this beforehand, but by far the most interesting plots here are the two looking at simple volume of speaking. Why is this? I think it has to be because it’s the only case where the method (counting, in this case) adequately represents the thing it purports to measure. I suppose word counts do that as well, but there’s no spice when there’s no implicit contrast (as is the case simple word counts) and then when there is contrast (in those graphs that plot proportions against each other), nothing pops out. Ex ante it might have be unknowable that was going to happen."
  },
  {
    "objectID": "posts_text/2024-01-24-klein-text/index.html#acknowledgements-and-arigatos",
    "href": "posts_text/2024-01-24-klein-text/index.html#acknowledgements-and-arigatos",
    "title": "The Ezra Klein Show",
    "section": "Acknowledgements and Arigatos",
    "text": "Acknowledgements and Arigatos\nAndrew Heiss’s data visualization course inspired (and enabled) me to have nice(ish) looking text on my graphs. If you know some ggplot2 and are looking to become high-intermediate in your ggplot2 skills, I highly recommend his course.\nThe vast majority of the analyses you see here I learned back in the day from Julia Silge and Dave Robinson’s Tidy Text Mining with R book. I recommend it to anyone as a jumping off point for doing text analysis. Most of the analyses here don’t go (too far) beyond what you can do with the tools you pick up in that book"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#afinn",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#afinn",
    "title": "Notes on Sentiment Dictionaries",
    "section": "AFINN",
    "text": "AFINN\nThe subtitle of the original publication says it all, “Evaluation of a word list for sentiment analysis in microblogs.” In the paper’s abstract, the great Dane and presumably the namesake of the lexicon, Finn Arup Nielsen, lays out more explicitly why he created a new sentiment lexicon, “There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs.” There’s AFINN’s origin story.3\nOne of the unique features of the AFINN lexicon is that words are mapped to integers instead of merely {positive, negative}. Here you can see a few words at each value:\n\nset.seed(1)\n\nget_sentiments('afinn') %>%\n  group_by(value) %>% \n  add_count(name = 'count') %>% \n  slice_sample(n = 4) %>%\n  summarise(\n    `no. words` = mean(count),\n    words = glue_collapse(word, sep = ', ')) %>% \n  arrange(value) %>% filter(value != 0)\n\n# A tibble: 10 × 3\n   value `no. words` words                                      \n   <dbl>       <dbl> <glue>                                     \n 1    -5          16 motherfucker, bitches, cocksuckers, bastard\n 2    -4          43 scumbag, fucking, fraudsters, fucked       \n 3    -3         264 moron, destroy, despair, scandals          \n 4    -2         966 animosity, censors, robs, touts            \n 5    -1         309 imposing, unclear, demonstration, uncertain\n 6     1         208 share, extend, feeling, commit             \n 7     2         448 tranquil, consent, supportive, sympathetic \n 8     3         172 audacious, classy, luck, gracious          \n 9     4          45 exuberant, wonderful, rejoicing, wowww     \n10     5           5 hurrah, outstanding, superb, thrilled      \n\n\nAnd yes, I did set the seed above to avoid randomly showing you certain words.\nYou might be wondering\n\nwhy -5 to 5 and\nhow he assigned words to those numbers\n\nQuoting him on the former, “As SentiStrength it uses a scoring range from −5 (very negative) to +5 (very positive).” Convention is powerful. As for the latter, the question of how numbers were assigned to words, “Most of the positive words were labeled with +2 and most of the negative words with –2 […]. I typically rated strong obscene words […] with either –4 or –5.” So he was basically winging it.\nAnother unique feature: the dictionary has 15 bigrams, 10 of which are below.4\n\nset.seed(12)\nget_sentiments('afinn') %>%\n  filter(str_detect(word, ' ')) %>% \n  slice_sample(n = 10)\n\n# A tibble: 10 × 2\n   word          value\n   <chr>         <dbl>\n 1 cashing in       -2\n 2 no fun           -3\n 3 green wash       -3\n 4 not good         -2\n 5 dont like        -2\n 6 not working      -3\n 7 some kind         0\n 8 green washing    -3\n 9 cool stuff        3\n10 messing up       -2\n\n\n\nSize: 2477 entries\nCoverage: In addition to the bog-standard sentiment words, it has words all the cool kids were saying in the late 2000, early 2010s.\nR Packages: tidytext, lexicon, textdata\nPublication: Here\nBottom Line: It’s been superseded by VADER (below)"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#bing-aka-hu-and-liu",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#bing-aka-hu-and-liu",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Bing (aka Hu and Liu)5",
    "text": "Bing (aka Hu and Liu)5\nAccording to the man himself, “This list [i.e., the lexicon] was compiled over many years starting from our first paper (Hu and Liu, KDD-2004).” I’m not sure what the post-publication compilation process was, but the original process is well-described in the original publication. Essentially, they started with adjectives6 with obvious polarity (e.g., great, fantastic, nice, cool, bad, dull) as ‘seed words’ and collected synonyms (and antonyms) of those words, then synonyms and antonyms of those words, and so on, iteratively. To do this, they used WordNet, a chill-ass semantic network. One thing that’s nice about the resulting lexicon is that it’s topic general. That is, though they developed this lexicon for the specific purpose of determining people’s opinions about product features in product reviews, it has a generality beyond that.\nActually looking at the words, you’ll notice there’s some weirdness.\n\nhead(get_sentiments('bing'), 10)\n\n# A tibble: 10 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n\n\nFirst, I’m not sure what “2-faces” is. If you say that it’s a solecistic rendering of “two-faced,” I’d say probably. In their appropriate alphabetic order, both “two-faced” and “two-faces” appear later in the dictionary. Anyway, you’ll notice as well that a lot of the words would reduce to a single lemma if we lemmatized the dictionary. You can think of that as a positive feature of the BING dictionary. It means you don’t have to have lemmatized (or stemmed) text. But its inclusion of derived words seems a bit haphazard. The abort-aborted pair is there, but abolish is hanging out along without its past tense.\n\nSize: In the tidytext package the BING dictionary has 6786 terms (matching what his website says, “around 6800 words”)\nR Packages: tidytext, lexicon, textdata\nBottom Line: It’s a classic and got wide coverage, but not as good as VADER or NRC-EIL."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#loughran-mcdonald-master-dictionary-w-sentiment-word-lists",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#loughran-mcdonald-master-dictionary-w-sentiment-word-lists",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Loughran-McDonald Master Dictionary w/ Sentiment Word Lists",
    "text": "Loughran-McDonald Master Dictionary w/ Sentiment Word Lists\nI’m honestly why this dictionary is included in packages – not because it’s bad7, but because it’s so (so so so) niche. If you’re doing text analysis on financial text (or aware of cool research doing this), please drop me a line and tell me about it.\nIf you want to learn about it, here’s the page.\n\nR Packages: tidytext, lexicon, textdata"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-original",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-original",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC (original)",
    "text": "NRC (original)\nSaif Mohammad and friends developed a few National Research Council (NRC) of Canada-sponsored sentiment dictionaries. The first of them assigns words not only polarity labels (positive, negative), but emotion labels as well:\n\ntidytext::get_sentiments('nrc') %>% \n  count(sentiment, sort = TRUE)\n\n# A tibble: 10 × 2\n   sentiment        n\n   <chr>        <int>\n 1 negative      3318\n 2 positive      2308\n 3 fear          1474\n 4 anger         1246\n 5 trust         1230\n 6 sadness       1187\n 7 disgust       1056\n 8 anticipation   837\n 9 joy            687\n10 surprise       532\n\n\nThese eight emotions below negative and positive were theorized by Bob Plutchik to be fundamental, or something.8 I’m going to ignore those emotions in this subsection. I’m also not going to talk too much about this dictionary, because it’s superseded by the real-valued its successors, the NRC-EIL and NRC-VAD (below).\nBefore I leave this lexicon, though, one bizarre thing about it: 81 words are both positive and negative (???)\n\nget_sentiments('nrc') %>% \n  filter(sentiment %in% c('negative', 'positive')) %>% \n  add_count(word) %>% \n  filter(n > 1) %>% select(-n)\n\n# A tibble: 162 × 2\n   word       sentiment\n   <chr>      <chr>    \n 1 abundance  negative \n 2 abundance  positive \n 3 armed      negative \n 4 armed      positive \n 5 balm       negative \n 6 balm       positive \n 7 boast      negative \n 8 boast      positive \n 9 boisterous negative \n10 boisterous positive \n# ℹ 152 more rows\n\n\nAs a matter of semantics, I get it for some of these (balm I do not get, though). Practically, if you’re doing an analysis with this dictionary, you’re probably going to want to remove all these terms before calculating sentiment scores.\n\nR Packages: tidytext, lexicon, textdata\nSize: 5464 words (positive and negative, not words with ambiguous polarity)\nBottom Line: Superseded by Saif Mohammed’s subsequent efforts."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-eil",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-eil",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC-EIL",
    "text": "NRC-EIL\nThis thing’s value-added, as my economist friend likes to say, is that instead of a simple ‘positive’ or ‘negative’ value for each sentiment entry, there’s a number between -1 and 1. “Real-valued,” as measurement-heads say. This is actually extremely important if you’re aggregating word-level sentiment into something bigger (which … honestly, email me if you’re doing anything other than that.) How they got these real-valued polarity scores is actually a pretty interesting methods story if you’re into that kind of thing, but I won’t go into ‘MaxDiff scaling’ here. One very important thing to note about this dataset, though, is that valence is that polarity isn’t in this dataset. This vexed me for a minute before I realized that it’s in the real-valued NRC-VAD (below). So, on the off chance you’re looking for the best measurement of Plutchik’s eight basic emotions (and that’s a very off chance), this is the best place to look.\n\nR Packages: textdata\nExamples: You can see an example of an analysis of Ezra Klein’s podcasts here.\nBottom Line: I’m not sure why it exists, but it’s the only lexicon doing what it’s doing."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-vad",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#nrc-vad",
    "title": "Notes on Sentiment Dictionaries",
    "section": "NRC-VAD",
    "text": "NRC-VAD\nOnce you have a hammer, everything starts looking like a nail. That’s how I explain the existence of this dictionary to myself. Saif Mohammed & Co. found these cool scaling technique and were like, “On what dimensions can we scale more words?” They seem to have stumbled on this idea the three most fundamental dimensions in concept space are valence, arousal, and dominance. Maybe it’s an indictment of my memory, perhaps an indictment of the psychology department at the University of Arkansas, but I managed to graduate summa cum laude with a degree in psychology without ever hearing of this. Regardless of supposed fundamental dimensions in concept space, valence is fundamental and is just another word for polarity which is the main thing people are doing with sentiment dictionaries.\nThis also led to my favorite table in I’ve ever seen in an academic article\n\nWhenever I need to express that something is pure bad valence, I now reach for the phrase ‘toxic nightmare shit.’\n\nR Packages: textdata\nBottom Line: Hell yeah. This is a good one."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#socal",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#socal",
    "title": "Notes on Sentiment Dictionaries",
    "section": "SOCAL",
    "text": "SOCAL\nIt’s the Semantic Orientation CALculator! Like VADER (below), SOCAL is both a sentiment dictionary and rules for modifying words’ sentiment given the context in which they appear. Below, I’ll only briefly consider the dictionary part. I recommend reading the publication both for more details on SOCAL itself as well as sentiment analysis generally. In a sea of sentiment articles that are slapdash publications from some “Proceedings of blah blah blah” or “Miniconference on yak yak yak” this one really stands out for its professionalism and thoroughness.9 As for whether or not you should actually use this dictionary, uh, you should keep reading.\nOne fun thing to note about SOCAL’s dictionary: it has more entries with spaces than any other dictionary I’ve seen.\n\nhash_sentiment_socal_google %>% \n  group_by(n_gram = str_count(x, ' ') + 1) %>% \n  count(n_gram) %>% \n  ungroup()\n\n# A tibble: 8 × 2\n  n_gram     n\n   <dbl> <int>\n1      1  2071\n2      2  1097\n3      3    97\n4      4    17\n5      5     3\n6      6     3\n7      7     1\n8      8     1\n\n\nThis dictionary has not only a huge bigrams:unigrams ratio, but it has sextagrams, a septagram, and even an octogram! This never happens! Let’s look at the n-grams where n > 4\n\nhash_sentiment_socal_google %>% \n  filter(str_count(x, ' ') > 3)\n\nKey: <x>\n                                                           x         y\n                                                      <char>     <num>\n1:                          darker and funnier than expected -4.092375\n2:                                     every other word is f -1.955614\n3:                          in your face everywhere you turn -5.357512\n4: lowbump bump bump bump bump bump bumpbumpbumpbump lowbump  5.622471\n5:                     throw your peanut shells on the floor -5.186874\n6:                                      trying to get on top -2.573428\n7:                               type stype suh huh uh huh's -4.281949\n8:                            write an awesome story to sell -6.262021\n\n\nI, uh, don’t really know what to make of these. There’s actually a restaurant named “Lambert’s Cafe” in Ozark, Missouri where you get peanuts in tin buckets and “throw your peanut shells on the floor” and, unless I’m remembering it wrong, it’s something people like about the place.\nSpeaking of things that are starting to be concerning, the distribution of scores:\n\nlibrary(ggtext)\nggplot(hash_sentiment_socal_google, aes(y)) +\n  geom_density() +\n  labs(\n    y = '',\n    x = 'Valence Score',\n    title = '**What tale tell ye**, ye two thick tails?'\n  ) +\n  theme(plot.title = element_markdown(face = 'italic'))\n\n\n\n\nNo, that density plot isn’t glitching. There really are words out there in the extremes:\n\nhash_sentiment_socal_google %>% \n  filter(abs(y) > 15)\n\nKey: <x>\n                            x         y\n                       <char>     <num>\n 1:    almost mafiosio styled  15.61625\n 2: automatically bestselling  17.60440\n 3:        coming of age isms  17.62518\n 4:           cushion handled  23.45929\n 5:                 hop ified -30.16008\n 6:          keyboard crafted  30.73891\n 7:      more than palateable  25.61696\n 8:          oven to stovetop  19.49040\n 9:             piano blessed  30.40257\n10:   rustic yet contemporary  16.31055\n11:           slotted spooned  18.19201\n12:              thick spoked  20.43281\n\n\nAt this point, you might be wondering … what scale is this? And what are values for our vanilla valence-indicators “good” and “bad”?\n\nhash_sentiment_socal_google %>% \n  filter(x %in% c('good', 'bad'))\n\nKey: <x>\n        x         y\n   <char>     <num>\n1:    bad -1.636520\n2:   good  1.872093\n\n\nOk, that’s fine. Maybe. But here’s something that probably isn’t fine:\n\nhash_sentiment_socal_google %>% \n  filter(str_detect(x, 'good|bad'))\n\nKey: <x>\n                  x          y\n             <char>      <num>\n1:     average good  0.4205258\n2:              bad -1.6365198\n3:        feel good  1.2984294\n4:             good  1.8720931\n5: good intentioned -4.1537399\n6:     good natured -1.5298719\n7:         half bad -3.9237520\n\n\nHere is where I lost all faith. “Good intentioned” and “good natured” are negative?!\nAt this point I’m going to call it a day with SOCAL. At some point I might write to the SOCAL authors or Tyler Rinker to see if something has gone wrong.\n\nPublication: Again, I truly recommend it\nSize: 3290 entries\nR Packages: lexicon"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#syuzhet",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#syuzhet",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Syuzhet",
    "text": "Syuzhet\n“The default method,”syuzhet” is a custom sentiment dictionary developed in the Nebraska Literary Lab. The default dictionary should be better tuned to fiction as the terms were extracted from a collection of 165,000 human coded sentences taken from a small corpus of contemporary novels.”\nNow, it does include a lot of words I find to be neutral (e.g., “yes”, “true”)\nWe can check out the distribution of the terms in my experimental sideways histogram below:\n\n# ggplot(key_sentiment_jockers, aes(value)) +\n#   geom_histogram(breaks = seq(-1, 1, by = 0.1), color = 'white') +\n#   theme(\n#     panel.grid.major.x = element_blank(),\n#     panel.grid.minor.x = element_blank(),\n#     axis.text.x = element_text(margin = margin(t = -10, b = 5)),\n#     plot.title = element_text(face = 'bold', size = rel(1.3))) +\n#   scale_y_continuous(breaks = c(500, 1000, 1500)) +\n#   labs(\n#     x = 'Jockers/Syuzhet Value',\n#     y = 'Terms in Dictionary',\n#     title = 'Distribution of Sentiment Values in Syuzhet Dictionary'\n#   )\n\n\ndistinct_values <- pull(distinct(key_sentiment_jockers, value))\n\nggplot(key_sentiment_jockers, aes(value)) +\n  geom_bar(width = .07, alpha = 0.8, fill = c(viridis::magma(8, direction = -1), viridis::mako(8))) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), \n            hjust = -0.1,\n            position = position_stack(),\n            family = 'IBM Plex Sans',\n            face = 'bold') +\n  scale_x_continuous(breaks = distinct_values) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    plot.title = element_text(face = 'bold'),\n    axis.text.y = element_text(margin = margin(r = -20))) +\n  coord_flip() +\n  labs(x = 'Valence Value in Syuzhet',\n       y = 'Counts',\n       title = 'Distribution of Sentiment Values in Syuzhet Dictionary') +\n  geom_vline(xintercept = 0)\n\n\n\n\n\nR Packages: syuzhet, lexicon\nSize: 10,748 words\nBottom Line: Pending."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#vader",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#vader",
    "title": "Notes on Sentiment Dictionaries",
    "section": "VADER",
    "text": "VADER\nIs VADER evil? Maybe. But it also stands for Valence Aware Dictionary and sEntiment Reasoner.10 Impressively, the found “that VADER outperforms individual human raters” when classifying tweets as positive, accurate, or neutral.11 Part (most?) of that impressiveness is due to the ‘ER’ of VADER. Nevertheless, here I’m only considering the VAD part. If you want to check out its ruled-based sentiment reasoning, check out its github page or publication.\nThe lexicon has an impressive 7,500 entries, each with its associated polarity and intensity (-4 to 4). Did they get those intensities just winging it like Finn? Nope. Each potential entry was placed on the -4 to 4 scale by 10 Amazon Mechanical Turk workers.12 The score you do see in the dictionary means that a) raters’ scores had a standard deviation of less than 2.513 and b) that the mean rating among the 10 raters was not 0.14\nOne interesting feature of the lexicon is its inclusion of emoticons.\n\nvader <- read_delim('https://raw.githubusercontent.com/cjhutto/vaderSentiment/master/vaderSentiment/vader_lexicon.txt',\n                    delim = '\\t', col_names = FALSE) %>% magrittr::set_names(c('token', 'score', 'sd', 'scores'))\n\nvader %>% \n  filter(str_detect(token, '[A-Za-z1-9]', negate = TRUE)) %>% \n  group_by(bin = score %/% 1) %>% \n  mutate(y = row_number()) %>% \n  ungroup() %>% \n  ggplot(aes(bin, y, label = token)) +\n  geom_text(check_overlap = TRUE, fontface = 'bold', family = 'IBM Plex Sans') +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(\n    x = '',\n    caption = 'To reduce the real-valued chaos, I rounded down emoticons\\' scores to the nearest integer'\n  ) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    axis.text = element_text(face = 'bold'),\n    panel.grid.minor.x = element_line(linetype = 2, color = 'grey35')\n  ) +\n  guides(color = 'none')\n\n\n\n\nI don’t want to make this dictionary seem trivial. Its creators also validated it using sentences from New York Times editorials, as well. It’s just not every day that you can make a histogram of emoticons.\n\nR Packages: As far as I know, it’s not in any. You can get it directly from its github repository.\nSide Benefit: This is probably the one that your “pythonista” friends are familiar with, since it’s in the nltk library."
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#coverage-comparison-for-afinn-bing-and-nrc",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#coverage-comparison-for-afinn-bing-and-nrc",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Coverage Comparison for AFINN, BING, and NRC",
    "text": "Coverage Comparison for AFINN, BING, and NRC\n\nafinn <- get_sentiments(lexicon = 'afinn')\nbing <- get_sentiments(lexicon = 'bing')\nnrc_emotions <- filter(get_sentiments(lexicon = 'nrc'), \n                    !(sentiment %in% c('positive', 'negative')))\nnrc_polar <- filter(get_sentiments(lexicon = 'nrc'), \n                    sentiment %in% c('positive', 'negative'))\n\n\nbind_rows(\n  select(mutate(afinn, lexicon = 'AFINN'), word, lexicon),\n  select(mutate(bing, lexicon = 'BING'), word, lexicon),\n  select(mutate(nrc_polar, lexicon = 'NRC'), word, lexicon)) %>% \n  summarise(Lexica = list(lexicon), .by = word) %>% \n  ggplot(aes(Lexica)) +\n  geom_bar() +\n  scale_x_upset(n_intersections = 7) +\n  theme_minimal() +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    title = element_text(family = \"IBM Plex Sans\"),\n    plot.title = element_text(face = 'bold'),\n    plot.subtitle = element_markdown(),\n    axis.text = element_text(family = \"IBM Plex Sans\")\n  ) +\n  labs(\n    y = 'Set Size',\n    x = 'Set',\n    title = 'Are Bigger Dictionaries Mostly Supersets of Smaller Dictionaries?',\n    subtitle = \"*No*, and that's weird\"\n  )\n\n\n\n\nWe can see how many entries each dictionary has:\n\nc(nrow(afinn), nrow(bing), nrow(nrc_polar))\n\n[1] 2477 6786 5626\n\n\nHere we see a surprising amount of non-overlap. Of Bing’s 6786 terms, almost 4,000 do not appear in either of the other two dictionaries. Almost 3,000 of NRC’s polarity entries aren’t in either of the other two, as well. The third bar indicates that BING and NRC share just over 1,500 words. The short and long of this is that it might be important which dictionary we choose. They have very different coverages.15"
  },
  {
    "objectID": "posts_text/2024-01-31-sentiments-dictionaries/index.html#still-to-do",
    "href": "posts_text/2024-01-31-sentiments-dictionaries/index.html#still-to-do",
    "title": "Notes on Sentiment Dictionaries",
    "section": "Still to do",
    "text": "Still to do\nThis page, like the rest of my life, is a work in progress.\n\nI still have a few dictionaries to add. In Tyler Rinker’s lexicon package there are: jockers (and jockers_rinker), emojis (and emojis_sentiment), senticnet, sentiword, slangsd. I’ve already covered all dictionaries in tidytext and textdata.\nFurther comparison of dictionary overlap. 2a. Right now I look at three dictionaries’ overlap. These three aren’t the best, they’re just the first ones I checked out. That’s not a principled criterion for selecting dictionaries. I’ll redo that section programmatically. 2b. I’m going to lemmatize/stem the dictionaries covered and get their size and overlaps as I did with AFINN, BING, and NRC above. It’s possible that sizes are much more similar once you remove a bunch of morphological tangle.\nInspired by this sentence from Nielsen, “The word list have a bias towards negative words (1598, corresponding to 65%) compared to positive words (878)” I’m also going to see what the respective positive/negative balances are of these dictionaries."
  },
  {
    "objectID": "section_immigration.html",
    "href": "section_immigration.html",
    "title": "Immigration Policy",
    "section": "",
    "text": "One of my (three) dissertation chapters is about immigration opinion and policy. Here you’ll (soon) find assorted posts related to that project. For an overview of half of the project, you can check out my AAPOR presentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImmigration Opinion and Policymaking\n\n\n\n\n\n\nJustin Dollman\n\n\nMay 27, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_immigration/2024-05-20-dependent-variable/index.html",
    "href": "posts_immigration/2024-05-20-dependent-variable/index.html",
    "title": "Immigration Opinion and Policymaking",
    "section": "",
    "text": "[Insert graph of opinion change]\n[Moving graph of California’s immigration policymaking]"
  }
]